{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f4f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.testing._internal import common_dtype\n",
    "from torch.cuda import amp\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "import esm\n",
    "\n",
    "import os\n",
    "\n",
    "import linecache\n",
    "\n",
    "from typing import List, Tuple, Union, Sequence\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "torch.set_num_threads(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97455399",
   "metadata": {},
   "source": [
    "During training, sequences longer than 1023 tokens (without CLS) are randomly cropped to a length of 1023.\n",
    "\n",
    "The details of the masking procedure for each sequence follow Devlin et al. 2019:\n",
    "\n",
    "15% of the amino acids are masked.\n",
    "In 80% of the cases, the masked amino acids are replaced by `<mask>`. <br>\n",
    "In 10% of the cases, the masked amino acids are replaced by a random amino acid (different) from the one they replace. <br>\n",
    "In the 10% remaining cases, the masked amino acids are left as is. \n",
    "\n",
    "https://huggingface.co/facebook/esm-1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13470775",
   "metadata": {},
   "source": [
    "Pre-training task: The masked language modeling pre-training task follows Devlin et al. (14). Specifically, we select as supervision 15% of tokens randomly sampled from the sequence. For those 15% of tokens, we change the input token to a special “masking” token with 80% probability, a randomly-chosen alternate amino acid token with 10% probability, and the original input token (i.e. no change) with 10% probability. We take the loss to be the whole batch average cross entropy loss between the model’s predictions and the true token for these 15% of amino acid tokens. In contrast to Devlin et al. (14), we do not use any additional auxiliary prediction losses.\n",
    "\n",
    "Our model was pre-trained using a context size of 1024 tokens. As most Uniparc sequences (96.7%) contain fewer than 1024 amino acids, the Transformer is able to model the entire context in a single model pass. For those sequences that are longer than 1024 to- kens, we sampled a random crop of 1024 tokens during each training epoch. The model was optimized using Adam (β1 = 0.9, β2 = 0.999) with learning rate 10−4. We trained with 131,072 tokens per batch (128 gpus x 1024 tokens). The models follow a warm-up period of 16000 updates, during which the learning rate increases linearly. Afterwards, the learning rate follows an inverse square root decay schedule. All models were trained using the fairseq toolkit (Ott et al., 2019) on 128 NVIDIA V100 GPUs.\n",
    "\n",
    "https://www.pnas.org/doi/full/10.1073/pnas.2016239118\n",
    "\n",
    "\n",
    "#### Alex notes:\n",
    "    - https://fairseq.readthedocs.io/en/latest/_modules/fairseq/optim/lr_scheduler/inverse_square_root_schedule.html likely used to implement learning rate decay after warmup\n",
    "    - betas are defaults for Adam\n",
    "    - only one \"sequence\" per GPU, sequence-wise batch size of 128\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4de8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '<cls>')\n",
      "(1, '<pad>')\n",
      "(2, '<eos>')\n",
      "(3, '<unk>')\n",
      "(4, 'L')\n",
      "(5, 'A')\n",
      "(6, 'G')\n",
      "(7, 'V')\n",
      "(8, 'S')\n",
      "(9, 'E')\n",
      "(10, 'R')\n",
      "(11, 'T')\n",
      "(12, 'I')\n",
      "(13, 'D')\n",
      "(14, 'P')\n",
      "(15, 'K')\n",
      "(16, 'Q')\n",
      "(17, 'N')\n",
      "(18, 'F')\n",
      "(19, 'Y')\n",
      "(20, 'M')\n",
      "(21, 'H')\n",
      "(22, 'W')\n",
      "(23, 'C')\n",
      "(24, 'X')\n",
      "(25, 'B')\n",
      "(26, 'U')\n",
      "(27, 'Z')\n",
      "(28, 'O')\n",
      "(29, '.')\n",
      "(30, '-')\n",
      "(31, '<null_1>')\n",
      "(32, '<mask>')\n"
     ]
    }
   ],
   "source": [
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "#batch_converter = alphabet.get_batch_converter()\n",
    "#del model\n",
    "print(*enumerate(alphabet.all_toks), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab364ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = esm.model.esm1.ProteinBertModel(model.args, alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbfa5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/work/ucsf/ntranos/variant-gsp1/first-1M-tokens.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c5fd960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_index(  # see https://github.com/pytorch/pytorch/issues/35674; left in codebase unused because it will probably be useful\n",
    "    indices: torch.Tensor,\n",
    "    shape: Union[int, Sequence, torch.Tensor],\n",
    "    *,\n",
    "    as_tuple: bool = True,\n",
    ") -> Union[Tuple[torch.Tensor, ...], torch.Tensor]:\n",
    "    \n",
    "    r\"\"\"Converts a `Tensor` of flat indices into a `Tensor` of coordinates for the given target shape.\n",
    "    Args:\n",
    "        indices: An integral `Tensor` containing flattened indices of a `Tensor` of dimension `shape`.\n",
    "        shape: The shape (can be an `int`, a `Sequence` or a `Tensor`) of the `Tensor` for which\n",
    "               the flattened `indices` are unraveled.\n",
    "    Keyword Args:\n",
    "        as_tuple: A boolean value, which if `True` will return the result as tuple of Tensors,\n",
    "                  else a `Tensor` will be returned. Default: `True`\n",
    "    Returns:\n",
    "        unraveled coordinates from the given `indices` and `shape`. See description of `as_tuple` for\n",
    "        returning a `tuple`.\n",
    "    .. note:: The default behaviour of this function is analogous to\n",
    "              `numpy.unravel_index <https://numpy.org/doc/stable/reference/generated/numpy.unravel_index.html>`_.\n",
    "    Example::\n",
    "        >>> indices = torch.tensor([22, 41, 37])\n",
    "        >>> shape = (7, 6)\n",
    "        >>> torch.unravel_index(indices, shape)\n",
    "        tensor([[3, 4],\n",
    "                [6, 5],\n",
    "                [6, 1]])\n",
    "        >>> torch.unravel_index(indices, shape)\n",
    "        (tensor([3, 6, 6]), tensor([4, 5, 1]))\n",
    "        >>> indices = torch.tensor([3, 10, 12])\n",
    "        >>> shape_ = (4, 2, 3)\n",
    "        >>> torch.unravel_index(indices, shape_, as_tuple=False)\n",
    "        tensor([[0, 1, 0],\n",
    "                [1, 1, 1],\n",
    "                [2, 0, 0]])\n",
    "    \"\"\"\n",
    "\n",
    "    def _helper_type_check(inp: Union[int, Sequence, torch.Tensor], name: str):\n",
    "        # `indices` is expected to be a tensor, while `shape` can be a sequence/int/tensor\n",
    "        if name == \"shape\" and isinstance(inp, Sequence):\n",
    "            for dim in inp:\n",
    "                if not isinstance(dim, int):\n",
    "                    raise TypeError(\"Expected shape to have only integral elements.\")\n",
    "                if dim < 0:\n",
    "                    raise ValueError(\"Negative values in shape are not allowed.\")\n",
    "        elif name == \"shape\" and isinstance(inp, int):\n",
    "            if inp < 0:\n",
    "                raise ValueError(\"Negative values in shape are not allowed.\")\n",
    "        elif isinstance(inp, torch.Tensor):\n",
    "            if inp.dtype not in common_dtype.integral_types():\n",
    "                raise TypeError(\n",
    "                    f\"Expected {name} to be an integral tensor, but found dtype: {inp.dtype}\"\n",
    "                )\n",
    "            if torch.any(inp < 0):\n",
    "                raise ValueError(f\"Negative values in {name} are not allowed.\")\n",
    "        else:\n",
    "            allowed_types = (\n",
    "                \"Sequence/Scalar (int)/Tensor\" if name == \"shape\" else \"Sequence/Tensor\"\n",
    "            )\n",
    "            msg = f\"{name} should either be a {allowed_types}, but found {type(inp)}\"\n",
    "            raise TypeError(msg)\n",
    "\n",
    "    _helper_type_check(indices, \"indices\")\n",
    "    _helper_type_check(shape, \"shape\")\n",
    "\n",
    "    # Convert to a tensor, with the same properties as that of indices\n",
    "    if isinstance(shape, Sequence):\n",
    "        shape_tensor: Tensor = indices.new_tensor(shape)\n",
    "    elif isinstance(shape, int) or (isinstance(shape, Tensor) and shape.ndim == 0):\n",
    "        shape_tensor = indices.new_tensor((shape,))\n",
    "    else:\n",
    "        shape_tensor = shape\n",
    "\n",
    "    # By this time, shape tensor will have dim = 1 if it was passed as scalar (see if-elif above)\n",
    "    assert shape_tensor.ndim == 1, \"Expected dimension of shape tensor to be <= 1, \"\n",
    "    f\"but got the tensor with dim: {shape_tensor.ndim}.\"\n",
    "\n",
    "    # In case no indices passed, return an empty tensor with number of elements = shape.numel()\n",
    "    if indices.numel() == 0:\n",
    "        # If both indices.numel() == 0 and shape.numel() == 0, short-circuit to return itself\n",
    "        shape_numel = shape_tensor.numel()\n",
    "        if shape_numel == 0:\n",
    "            raise ValueError(\n",
    "                \"Got indices and shape as empty tensors, expected non-empty tensors.\"\n",
    "            )\n",
    "        else:\n",
    "            output = [indices.new_tensor([]) for _ in range(shape_numel)]\n",
    "            return tuple(output) if as_tuple else torch.stack(output, dim=1)\n",
    "\n",
    "    if torch.max(indices) >= torch.prod(shape_tensor):\n",
    "        raise ValueError(\"Target shape should cover all source indices.\")\n",
    "\n",
    "    coefs = shape_tensor[1:].flipud().cumprod(dim=0).flipud()\n",
    "    coefs = torch.cat((coefs, coefs.new_tensor((1,))), dim=0)\n",
    "    coords = torch.div(indices[..., None], coefs, rounding_mode=\"trunc\") % shape_tensor\n",
    "\n",
    "    if as_tuple:\n",
    "        return tuple(coords[..., i] for i in range(coords.size(-1)))\n",
    "    \n",
    "    return coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d905408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alex code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c597eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop_sequence(\n",
    "        tokens: Union[torch.Tensor, npt.NDArray[int]], max_length: int, return_start: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Randomly crop a sequence of length `max_length` from tokens.\n",
    "        \"\"\"\n",
    "        if isinstance(tokens, (torch.Tensor, np.ndarray)) is False:\n",
    "            raise TypeError('Argument tokens must be of type torch.Tensor or np.ndarray.')\n",
    "        \n",
    "        start_index = np.random.randint(\n",
    "            0, high=len(tokens) - max_length, size=1, dtype=int\n",
    "        ).item()\n",
    "        \n",
    "        if return_start:\n",
    "            return start_index, tokens[start_index : start_index + max_length]\n",
    "        else:\n",
    "            return tokens[start_index : start_index + max_length]\n",
    "\n",
    "class AASequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: Union[str, os.PathLike],\n",
    "        aa_to_replace_candidates: Sequence[int],\n",
    "        n_lines: Union[None, int] = None,\n",
    "        max_seq_len: Union[None, int] = None,\n",
    "        pct_aa_mask: float = 0.15,\n",
    "        pct_aa_replace: float = 0.10,\n",
    "        pct_aa_noaction: float = 0.10,\n",
    "        padding_token: int = 1,\n",
    "        mask_token: int = 32,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Class to hold a large sequence token file with one sequence token list (space separated) per line.\n",
    "        Will implement the ESM1b strategy of token preprocessing in the `__getitem__` method where:\n",
    "            - sequences are randomly clipped to a given length using `padding_token`\n",
    "            - 15% of AA are masked with `mask_token`; of these 10% get replaced with another AA and the remaining 10% get nothing happened\n",
    "\n",
    "        Inputs:\n",
    "            file_path: input file as fairseq sequence file; one line per sequence and each sequence is a space separated list of int \n",
    "            aa_to_replace_candidates: a list of int that will be randomly selected from to perform the BERT style masking (ie `pct_aa_replace` of the sequence will get replaced with one of these)\n",
    "            n_lines [optional]: total amount of lines, if known before don't have to parse file\n",
    "            max_seq_len [optional]: max size of any one seq--we will pad all seqs to this length\n",
    "            pct_aa_mask [optional]: % of aa to mask for any given sequence with `mask_token`\n",
    "            pct_aa_replace [optional]: % of aa to replace with one of the `aa_to_replace_candidates`\n",
    "            pct_aa_noaction [optional]: % of aa to do nothing with after the selection\n",
    "            padding_token [optional]: token to use for padding (see `max_seq_len`)\n",
    "            mask_token [optional]: token to use for masking (see `pct_aa_mask`)\n",
    "        \n",
    "        TODO:\n",
    "        - 2022-07-26:\n",
    "            - implement on-the-fly decoding from FASTA into tokens using esm.alphabet dictionary\n",
    "            - write tests\n",
    "                - test1: make sure initialization occurs with invalid args\n",
    "                - test2: make sure yields sequences of correct length if given longer than total length\n",
    "                - test3: make sure pct aa mask is roughly respected\n",
    "                - test4: make sure yields correctly from test file\n",
    "                - test5: make sure cropping with `start_index` manually yields same result as returned fn\n",
    "        \"\"\"\n",
    "\n",
    "        # file path check exist\n",
    "        if os.path.exists(file_path) is False:\n",
    "            raise FileExistsError(f\"File path: {file_path} does not exist.\")\n",
    "        self.file_path = file_path\n",
    "\n",
    "        # max seq length check\n",
    "        if max_seq_len is not None:\n",
    "            if isinstance(max_seq_len, int) is False:\n",
    "                raise TypeError(\n",
    "                    f\"Argument `max_seq_len` must be of type int. Got type: {type(max_seq_len)}\"\n",
    "                )\n",
    "\n",
    "        # check validity of % arguments for masking/replacement etc. by looping through them and making sure\n",
    "        # they are all float and on [0, 1]\n",
    "        \n",
    "        for pct, descript in zip(\n",
    "            (pct_aa_mask, pct_aa_replace, pct_aa_noaction),\n",
    "            (\"pct_aa_mask\", \"pct_aa_replace\", \"pct_aa_noaction\"),\n",
    "        ):\n",
    "            if isinstance(pct, float) is False or not (1.0 > pct >= 0):\n",
    "                raise TypeError(\n",
    "                    f\"Argument {descript} must be of type float and on [0, 1).\"\n",
    "                )\n",
    "            else:\n",
    "                setattr(self, descript, pct)\n",
    "\n",
    "        if (\n",
    "            isinstance(mask_token, int) is False\n",
    "            or isinstance(padding_token, int) is False\n",
    "        ):\n",
    "            raise TypeError(\n",
    "                \"Args `mask_token` and `padding_token` must be of type int.\"\n",
    "            )\n",
    "\n",
    "        self.padding_token = padding_token\n",
    "        self.mask_token = mask_token\n",
    "\n",
    "        \n",
    "        # if `n_lines` or `max_seq_len` not provided, loop through and get both number lines and max seq len\n",
    "        if (n_lines is None) and (max_seq_len is None):\n",
    "            n_lines, max_seq_len = self.__parse_file(\n",
    "                file_path, line_no=True, max_seq_len=True\n",
    "            )\n",
    "            \n",
    "        elif n_lines is None: # just get num lines\n",
    "            n_lines = self.__parse_file(file_path, line_no=True, max_seq_len=False)\n",
    "            \n",
    "        elif max_seq_len is None: # just get max seq len\n",
    "            max_seq_len = self.__parse_file(file_path, line_no=False, max_seq_len=True)\n",
    "\n",
    "        self.n_lines = n_lines\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        if len(aa_to_replace_candidates) < 1:\n",
    "            raise ValueError(\n",
    "                \"Arg `aa_to_replace_candidates` must have length greater than 0.\"\n",
    "            )\n",
    "\n",
    "        # TODO: think of way to detect this as all int that is less messy\n",
    "        try:\n",
    "            self.aa_to_replace = self.__try_map_int(aa_to_replace_candidates) \n",
    "        except:\n",
    "            raise TypeError(\n",
    "                \"All elements of arg `aa_to_replace_candidates` must be of type int.\"\n",
    "            )\n",
    "\n",
    "    def __parse_file(\n",
    "        self,\n",
    "        file_path: Union[str, os.PathLike],\n",
    "        line_no: bool = False,\n",
    "        max_seq_len: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parses input file to get the max number of lines or maximum sequence length.\n",
    "        Files are assumed to be one sequence per line, tokenized, with tokens separated by spaces.\n",
    "        \"\"\"\n",
    "\n",
    "        max_len = 0\n",
    "\n",
    "        if isinstance(line_no, bool) is False or isinstance(max_seq_len, bool) is False:\n",
    "            print(line_no, max_seq_len)\n",
    "            raise TypeError(\n",
    "                \"Arguments `line_no` and `max_seq_len` must be of type bool.\"\n",
    "            )\n",
    "\n",
    "        if not (line_no or max_seq_len):\n",
    "            raise ValueError(\"One of `line_no` or `max_seq_len must be True.\")\n",
    "\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line_idx, line in enumerate(f):\n",
    "                length = len(line)\n",
    "                if length > max_len:\n",
    "                    max_len = length\n",
    "\n",
    "        if line_no and max_seq_len:\n",
    "            return line_idx, max_len\n",
    "        elif line_no and (not max_seq_len):\n",
    "            return line_idx\n",
    "        elif (not line_no) and max_seq_len:\n",
    "            return max_len\n",
    "\n",
    "    def random_crop_sequence(\n",
    "        self, tokens: npt.NDArray[int], max_length: int, return_start: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Randomly crop a sequence of length `max_length` from tokens.\n",
    "        \"\"\"\n",
    "        start_index = np.random.randint(\n",
    "            0, high=len(tokens) - max_length, size=1, dtype=int\n",
    "        ).item()\n",
    "        if return_start:\n",
    "            return start_index, tokens[start_index : start_index + max_length]\n",
    "        else:\n",
    "            return tokens[start_index : start_index + max_length]\n",
    "\n",
    "    def __mask_indices(self, indices: npt.NDArray[int], mask: npt.NDArray[bool]):\n",
    "        # return from `indices` elements that are and are not in bool array mask `mask`\n",
    "        return indices[mask], indices[~mask]\n",
    "\n",
    "    def __try_map_int(self, sequence: Sequence[str]):\n",
    "        # try to return a sequence of type int; if not return None for later handling\n",
    "\n",
    "        try:\n",
    "            return list(map(int, sequence))\n",
    "        except ValueError as e:\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_lines\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # TODO: replace operations in this fn with a function call to a fn that can operate on one sequence alone\n",
    "        # f(tokens, pct_aa_mask, pct_aa_replace, pct_aa_leave, maxlen, padding_token, mask_token)    \n",
    "        # instead of having it all written out like this\n",
    "\n",
    "        # need to add index +1 because linecache expects the first line to be indexed with \"1\"\n",
    "        tokens_raw = (\n",
    "            linecache.getline(self.file_path, index + 1).strip().split(\" \")\n",
    "        )  # list of strings; must strip newline\n",
    "\n",
    "        # get raw tokens\n",
    "        tokens_raw = self.__try_map_int(tokens_raw)  # list of int or None\n",
    "        if tokens_raw is None:\n",
    "            print(tokens_raw)\n",
    "            raise ValueError(\n",
    "                f\"Could not convert all elements to int for dataset item number: {index}\"\n",
    "            )\n",
    "\n",
    "        tokens_raw = np.array(tokens_raw)\n",
    "        sequence_length_raw = len(tokens_raw)\n",
    "\n",
    "        # optionally crop sequence\n",
    "        if sequence_length_raw > self.max_seq_len:\n",
    "            sequence_length_raw = self.max_seq_len\n",
    "            start_index, tokens_raw = random_crop_sequence(\n",
    "                tokens_raw, self.max_seq_len, return_start=True\n",
    "            )\n",
    "            start_index = [start_index]\n",
    "        else:\n",
    "            start_index = [0]\n",
    "\n",
    "        # pad tokens to max length if necessary\n",
    "        tokens_padded = np.pad(\n",
    "            tokens_raw,\n",
    "            (0, int(self.max_seq_len - sequence_length_raw)),\n",
    "            \"constant\",\n",
    "            constant_values=self.padding_token,\n",
    "        )  # pad to desired length with constants\n",
    "\n",
    "        # masking:\n",
    "        num_tokens_to_mask = np.rint(sequence_length_raw * self.pct_aa_mask).astype(int)\n",
    "        indices_to_mask = np.random.choice(\n",
    "            np.arange(sequence_length_raw), size=num_tokens_to_mask, replace=False\n",
    "        )  # random pick which tokens indices to mask\n",
    "\n",
    "        # pick the mask indices to replace with another AA\n",
    "        replace_indices_mask = (\n",
    "            np.random.random(size=num_tokens_to_mask) <= self.pct_aa_replace\n",
    "        )\n",
    "        indices_to_mask, replace_indices = self.__mask_indices(\n",
    "            indices_to_mask, replace_indices_mask\n",
    "        )\n",
    "\n",
    "        # pick the mask indices to just ignore\n",
    "        noaction_indices_mask = (\n",
    "            np.random.random(size=len(indices_to_mask)) <= self.pct_aa_noaction\n",
    "        )\n",
    "        indices_to_mask, noaction_indices = self.__mask_indices(\n",
    "            indices_to_mask, noaction_indices_mask\n",
    "        )  # right now we are including this mask in the loss computation, not sure if this is right\n",
    "\n",
    "        # corrupted / masked tokens variable\n",
    "        tokens_processed = tokens_padded.copy()\n",
    "        tokens_processed[indices_to_mask] = self.mask_token\n",
    "\n",
    "        # randomly pick new AA for the ones to corrupt\n",
    "        for index in replace_indices:\n",
    "            random_aa = tokens_processed[index]\n",
    "            while random_aa != tokens_processed[index]:\n",
    "                random_aa = np.random.choice(self.aa_to_replace, size=1)\n",
    "            tokens_processed[index] = random_aa\n",
    "\n",
    "        tokens_tensor = torch.Tensor(\n",
    "            tokens_processed\n",
    "        ).long()  # should be 1 x `max_seq_len` and type torch.int64\n",
    "\n",
    "        all_masked_indices = np.array(\n",
    "            sorted(\n",
    "                indices_to_mask.tolist()\n",
    "                + replace_indices.tolist()\n",
    "                + noaction_indices.tolist()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #         NOTE 2022-07-26: decided not to pad the indices this way and to simply return an unraveled index in our custom `collate_fn`\n",
    "        #         we need to padd all_mask_indices to length constant because collate_fn doesnt like unequal length sequences; leaving in for now\n",
    "        #         TODO: refactor so that we can assign attribute self.max_mask_index_length and use instead of doing this calc every time\n",
    "\n",
    "        #         all_masked_indices = np.pad(\n",
    "        #             np.array(all_masked_indices),\n",
    "        #             (0, int(self.max_seq_len * self.pct_aa_mask - len(all_masked_indices))),\n",
    "        #             \"constant\",\n",
    "        #             constant_values=-1,\n",
    "        #         )\n",
    "\n",
    "        output_dict = dict(\n",
    "            tokens=tokens_tensor,\n",
    "            mask_indices=all_masked_indices,\n",
    "            start_index=torch.Tensor(start_index)[None, :].long(),\n",
    "            labels=torch.Tensor(tokens_padded).long(),\n",
    "        )\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c42398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collater(elements):\n",
    "    \"\"\"\n",
    "    Stacks single elements into dict batch. Needed because some inputs are not equal length elements across batch or input dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    collated_batch = {}\n",
    "    max_size = len(elements[0]['tokens'])\n",
    "    \n",
    "    for key in elements[0].keys():\n",
    "        # for most things we can just stack them into a bigger tensor\n",
    "        if key != 'mask_indices':\n",
    "            collated_batch[key] = torch.stack([single_el[key] for single_el in elements]) # will be nbatch x ntokens\n",
    "            \n",
    "        else:\n",
    "            all_indices = [] # flat indices of the corrupted AA\n",
    "            index_offsets = [] # indicator which is == k if that index was in the k-th sequence\n",
    "            \n",
    "            for batch_idx, single_el in enumerate(elements):\n",
    "                # the strategy here is to compute the flat index of ALL of the AA indices that we are going to mask\n",
    "                # then later we can collect this and use this to collect all of the same positions later for the loss computation\n",
    "                # for ex, if we have ten sequences of length 10 and the first three AA are masked everytime, we will \n",
    "                # obtain flat indices of [[0, 1, 2], [10, 11, 12], ..., [90, 91, 92]]\n",
    "                # these aren't going to be normally concatenated without the custom collate fn\n",
    "                \n",
    "                index_offset = batch_idx * max_size\n",
    "                # need to compute the offset so that flat index within a data pt is equivalent to the one across batches\n",
    "                unraveled = np.unravel_index(single_el['mask_indices'], max_size)[0] + index_offset \n",
    "                all_indices += unraveled.tolist()\n",
    "                index_offsets += [batch_idx] * len(unraveled)\n",
    "                \n",
    "            collated_batch['masked_indices'] = all_indices\n",
    "            collated_batch['index_offsets']  = np.array(index_offsets)\n",
    "    \n",
    "    # do the unraveling using the full token shape to get the indices that we can use directly on the n-dim tensor\n",
    "    collated_batch['masked_indices'] = np.unravel_index(collated_batch['masked_indices'], collated_batch['tokens'].shape) \n",
    "    \n",
    "    return collated_batch\n",
    "\n",
    "class CrossEntropyLoss(nn.modules.loss._Loss):\n",
    "    \"\"\"\n",
    "    Performs token-wise cross entropy computation on flattened tokens. \n",
    "    During forward pass, can optinally pass a tensor of sequence ids that indicate the index of a given token within a batch. \n",
    "    If provided this will compute an equal weighting factor for the loss computation so that sequences of long length do not dominate the loss. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, avg_sequences: bool = True, reduction: str = 'sum'):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            reduction = reduction.lower()\n",
    "        except:\n",
    "            raise TypeError('Arg reduction must be of type str.')\n",
    "            \n",
    "        if isinstance(avg_sequences, bool) is False:\n",
    "            raise TypeError(f'Arg {avg_sequences} must be of type bool. Got type:  {type(avg_sequences)}')\n",
    "            \n",
    "        if reduction not in ['sum', 'mean']:\n",
    "            raise ValueError('Reduction method must be one of `sum`, `mean` or `average`.')\n",
    "            \n",
    "        reduction_fn = torch.__dict__.get(reduction, None) \n",
    "        assert reduction_fn is not None, 'Somehow failed to retrieve `torch.mean` or `torch.sum` from torch.__dict__'\n",
    "        \n",
    "        self.reduction_fn = reduction_fn\n",
    "        self.loss_func = F.cross_entropy\n",
    "        \n",
    "    def forward(self, logits: torch.Tensor, target: torch.Tensor, sequence_ids: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            logits: long vector with all tokens' logits\n",
    "            target: long vector with all tokens' target class integer\n",
    "            sequence_ids: long vector that indicates the sequence index membership; for ex:\n",
    "                3 sequences with length 2 each, pass [0, 0, 0, 1, 1, 1, 2, 2]\n",
    "        \n",
    "        If `sequence_ids` is not provided, we'll just return the loss with all tokens being equally weighted using \"normal\" CE.\n",
    "        If `sequence_ids` is provided, we expect it will be a long vector with sequence membership within class.\n",
    "        \n",
    "        The purpose of `sequence_ids` is to allow for averaging of the CE across sequences, so that if one sequence's CE is very low or very high it \n",
    "        is not weighted equally as other sequences; or to allow for long/short sequences to have the same approximate loss scale.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert logits.device == target.device, 'Logits and target tensors are not on same device.'\n",
    "        \n",
    "        if sequence_ids is not None:\n",
    "            condition = (len(logits) == len(target) == len(sequence_ids))\n",
    "        else:\n",
    "            condition = (len(logits) == len(target))\n",
    "            \n",
    "        if not condition:\n",
    "            raise ValueError('Lengths of tensors for `logits`, `target` and `sequence_ids` (if used) does not match.')\n",
    "        \n",
    "        loss_per_token = self.loss_func(logits, target, reduction='none') \n",
    "        # for now we just want to get the loss across all positions unaggregated\n",
    "\n",
    "        if sequence_ids is not None:\n",
    "            bs = len(logits)\n",
    "            weighting_tensor = torch.ones(bs) # we'll modify this tensor as we go so that it contains a per-token weight across that token's sequence\n",
    "            weighting_tensor.to(logits.device)\n",
    "            \n",
    "            for batch_idx in range(bs):\n",
    "                single_seq_mask = sequence_ids == batch_idx # get the indices for sequence # batch_idx\n",
    "                num_tokens_in_seq = single_seq_mask.sum() # get total loss\n",
    "                weighting_tensor[single_seq_mask] = weighting_tensor[single_seq_mask] / num_tokens_in_seq # average by num tokens, note that this can be different per sequence\n",
    "                #weighting_tensor[mask].div_(num_tokens) # TODO: for reason this doesn't seem to work; fix?\n",
    "            weighting_tensor.div_(weighting_tensor.sum()) # normalization factor\n",
    "            \n",
    "            loss_per_token.mul_(weighting_tensor)\n",
    "        \n",
    "        loss_reduced = self.reduction_fn(loss_per_token)\n",
    "        return loss_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d975b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_token_ids_to_substitute = np.arange(4, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1977c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AASequenceDataset(file_path, aa_token_ids_to_substitute, n_lines=999999, max_seq_len=1024) \n",
    "# dont need to provide n_lines; but if max_seq_len not provided will use the longest seq in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efdb3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(dataset.n_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c61783",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(all_indices, train_size=0.99999) # two data pts in test = 9999985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ec34aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = data.sampler.SubsetRandomSampler(train)\n",
    "test_sampler = data.sampler.SubsetRandomSampler(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53244e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30989f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(dataset, batch_size=2, sampler=train_sampler, collate_fn=collater)\n",
    "test_loader = data.DataLoader(dataset, batch_size=1, sampler=test_sampler, collate_fn=collater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cef7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1c338b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65d6befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "013e52c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(79.3120, device='cuda:0')\n",
      "tensor(79.3156, device='cuda:0')\n",
      "tensor(70.9565, device='cuda:0')\n",
      "tensor(76.5819, device='cuda:0')\n",
      "tensor(66.6982, device='cuda:0')\n",
      "tensor(69.3131, device='cuda:0')\n",
      "tensor(88.9941, device='cuda:0')\n",
      "tensor(79.3052, device='cuda:0')\n",
      "tensor(52.5119, device='cuda:0')\n",
      "tensor(80.5768, device='cuda:0')\n",
      "0 tensor(81.6781, device='cuda:0')\n",
      "0 tensor(84.2323, device='cuda:0')\n",
      "0 tensor(79.4887, device='cuda:0')\n",
      "0 tensor(50.6653, device='cuda:0')\n",
      "0 tensor(73.9508, device='cuda:0')\n",
      "0 tensor(66.9356, device='cuda:0')\n",
      "0 tensor(73.9293, device='cuda:0')\n",
      "0 tensor(68.5769, device='cuda:0')\n",
      "0 tensor(87.8692, device='cuda:0')\n",
      "0 tensor(75.7789, device='cuda:0')\n",
      "1 tensor(74.8772, device='cuda:0')\n",
      "1 tensor(68.9311, device='cuda:0')\n",
      "1 tensor(56.2176, device='cuda:0')\n",
      "1 tensor(77.0048, device='cuda:0')\n",
      "1 tensor(85.6193, device='cuda:0')\n",
      "1 tensor(78.4757, device='cuda:0')\n",
      "1 tensor(68.2610, device='cuda:0')\n",
      "1 tensor(72.0725, device='cuda:0')\n",
      "1 tensor(84.3657, device='cuda:0')\n",
      "1 tensor(63.3375, device='cuda:0')\n",
      "2 tensor(68.2676, device='cuda:0')\n",
      "2 tensor(82.9787, device='cuda:0')\n",
      "2 tensor(69.6260, device='cuda:0')\n",
      "2 tensor(87.4003, device='cuda:0')\n",
      "2 tensor(79.6718, device='cuda:0')\n",
      "2 tensor(78.2152, device='cuda:0')\n",
      "2 tensor(72.4865, device='cuda:0')\n",
      "2 tensor(66.9448, device='cuda:0')\n",
      "2 tensor(79.2321, device='cuda:0')\n",
      "2 tensor(56.2113, device='cuda:0')\n",
      "3 tensor(70.7715, device='cuda:0')\n",
      "3 tensor(82.0965, device='cuda:0')\n",
      "3 tensor(68.1230, device='cuda:0')\n",
      "3 tensor(87.7583, device='cuda:0')\n",
      "3 tensor(72.7010, device='cuda:0')\n",
      "3 tensor(70.5911, device='cuda:0')\n",
      "3 tensor(52.5638, device='cuda:0')\n",
      "3 tensor(80.5303, device='cuda:0')\n",
      "3 tensor(84.6059, device='cuda:0')\n",
      "3 tensor(73.2061, device='cuda:0')\n",
      "4 tensor(70.4510, device='cuda:0')\n",
      "4 tensor(64.5340, device='cuda:0')\n",
      "4 tensor(76.3165, device='cuda:0')\n",
      "4 tensor(81.7712, device='cuda:0')\n",
      "4 tensor(82.6683, device='cuda:0')\n",
      "4 tensor(80.2837, device='cuda:0')\n",
      "4 tensor(71.5167, device='cuda:0')\n",
      "4 tensor(68.1139, device='cuda:0')\n",
      "4 tensor(80.2825, device='cuda:0')\n",
      "4 tensor(62.4165, device='cuda:0')\n",
      "5 tensor(70.9148, device='cuda:0')\n",
      "5 tensor(63.1282, device='cuda:0')\n",
      "5 tensor(68.5316, device='cuda:0')\n",
      "5 tensor(80.3729, device='cuda:0')\n",
      "5 tensor(72.4042, device='cuda:0')\n",
      "5 tensor(87.9595, device='cuda:0')\n",
      "5 tensor(66.6799, device='cuda:0')\n",
      "5 tensor(75.4530, device='cuda:0')\n",
      "5 tensor(59.2405, device='cuda:0')\n",
      "5 tensor(66.8736, device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 23.69 GiB total capacity; 22.15 GiB already allocated; 50.06 MiB free; 22.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_134974/3434906736.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mfwd_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# only logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlogits_unmasked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfwd_pass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'masked_indices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/mambaforge/envs/dev/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/mambaforge/envs/dev/lib/python3.8/site-packages/esm/model/esm1.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             x, attn = layer(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attn_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_head_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_head_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             )\n",
      "\u001b[0;32m~/software/mambaforge/envs/dev/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/mambaforge/envs/dev/lib/python3.8/site-packages/esm/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         x, attn = self.self_attn(\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/mambaforge/envs/dev/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/mambaforge/envs/dev/lib/python3.8/site-packages/esm/multihead_attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[0m\n\u001b[1;32m    206\u001b[0m         ):\n\u001b[1;32m    207\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             return F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/mambaforge/envs/dev/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5177\u001b[0m     \u001b[0;31m# (deep breath) calculate attention and out projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m     \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_scaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/mambaforge/envs/dev/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_scaled_dot_product_attention\u001b[0;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[1;32m   4852\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaddbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4853\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4854\u001b[0;31m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4856\u001b[0m     \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 23.69 GiB total capacity; 22.15 GiB already allocated; 50.06 MiB free; 22.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# gpu loop; see https://pytorch.org/docs/stable/notes/amp_examples.html\n",
    "losses_batch = []\n",
    "halfway = len(train_loader) // 2\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        fwd_pass = model(batch['tokens'].cuda()) # only logits\n",
    "\n",
    "        logits_unmasked = fwd_pass['logits'][batch['masked_indices']]\n",
    "        labels_unmasked = batch['labels'][batch['masked_indices']]\n",
    "        \n",
    "        loss = ce_loss(logits_unmasked, labels_unmasked.cuda())\n",
    "        print(loss)\n",
    "        \n",
    "model.train()        \n",
    "\n",
    "for batch_index, batch in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with amp.autocast():\n",
    "        fwd_pass = model(batch['tokens'].cuda()) # only logits\n",
    "\n",
    "        logits_unmasked = fwd_pass['logits'][batch['masked_indices']]\n",
    "        labels_unmasked = batch['labels'][batch['masked_indices']]\n",
    "\n",
    "        loss = ce_loss(logits_unmasked, labels_unmasked.cuda())\n",
    "    \n",
    "    del batch\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            fwd_pass = model(batch['tokens'].cuda()) # only logits\n",
    "\n",
    "            logits_unmasked = fwd_pass['logits'][batch['masked_indices']]\n",
    "            labels_unmasked = batch['labels'][batch['masked_indices']].cuda()\n",
    "\n",
    "            loss = ce_loss(logits_unmasked, labels_unmasked)\n",
    "            print(batch_index, loss)\n",
    "            \n",
    "            del batch\n",
    "\n",
    "    model.train()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5df0639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(65.7486)\n",
      "0 tensor(62.7873)\n",
      "0 tensor(61.4920)\n",
      "0 tensor(58.4905)\n",
      "0 tensor(56.6521)\n",
      "0 tensor(55.2771)\n",
      "0 tensor(54.0909)\n",
      "0 tensor(50.6875)\n",
      "0 tensor(49.9440)\n",
      "0 tensor(48.7508)\n",
      "0 tensor(49.0366)\n",
      "0 tensor(45.6912)\n",
      "0 tensor(44.9796)\n",
      "0 tensor(44.1981)\n",
      "0 tensor(42.2238)\n",
      "0 tensor(41.2584)\n",
      "0 tensor(39.7795)\n",
      "0 tensor(39.1225)\n",
      "0 tensor(36.7280)\n",
      "0 tensor(36.3127)\n",
      "0 tensor(34.4518)\n",
      "0 tensor(35.5149)\n",
      "0 tensor(34.2678)\n",
      "0 tensor(33.0167)\n",
      "0 tensor(31.8986)\n",
      "0 tensor(31.8731)\n",
      "0 tensor(30.8363)\n",
      "0 tensor(29.8315)\n",
      "0 tensor(29.0559)\n",
      "0 tensor(28.4236)\n",
      "0 tensor(28.8121)\n",
      "0 tensor(28.4787)\n",
      "0 tensor(26.4458)\n",
      "0 tensor(27.1549)\n",
      "0 tensor(25.9423)\n",
      "0 tensor(25.8570)\n",
      "0 tensor(25.0082)\n",
      "0 tensor(25.2830)\n",
      "0 tensor(24.2531)\n",
      "0 tensor(23.8890)\n",
      "0 tensor(24.0143)\n",
      "0 tensor(23.0027)\n",
      "0 tensor(23.6564)\n",
      "0 tensor(22.3643)\n",
      "0 tensor(22.0054)\n",
      "0 tensor(21.7818)\n",
      "0 tensor(22.3664)\n",
      "0 tensor(22.2214)\n",
      "0 tensor(22.1919)\n",
      "0 tensor(21.3890)\n",
      "0 tensor(21.6411)\n",
      "0 tensor(20.8921)\n",
      "0 tensor(20.4594)\n",
      "0 tensor(20.7399)\n",
      "0 tensor(19.7299)\n",
      "0 tensor(20.9234)\n",
      "0 tensor(20.1359)\n",
      "0 tensor(19.1607)\n",
      "0 tensor(18.5770)\n",
      "0 tensor(18.8490)\n",
      "0 tensor(18.7957)\n",
      "0 tensor(18.3729)\n",
      "0 tensor(17.5775)\n",
      "0 tensor(17.9202)\n",
      "0 tensor(17.5374)\n",
      "0 tensor(16.7284)\n",
      "0 tensor(17.3752)\n",
      "0 tensor(17.2874)\n",
      "0 tensor(17.4525)\n",
      "0 tensor(16.2492)\n",
      "0 tensor(16.3944)\n",
      "0 tensor(16.2279)\n",
      "0 tensor(16.3034)\n",
      "0 tensor(15.7949)\n",
      "0 tensor(15.5133)\n",
      "0 tensor(15.1075)\n",
      "0 tensor(15.2623)\n",
      "0 tensor(15.0426)\n",
      "0 tensor(14.8753)\n",
      "0 tensor(15.0869)\n",
      "0 tensor(15.1444)\n",
      "0 tensor(14.4211)\n",
      "0 tensor(14.4156)\n",
      "0 tensor(14.5930)\n",
      "0 tensor(14.5098)\n",
      "0 tensor(14.0037)\n",
      "0 tensor(14.2922)\n",
      "0 tensor(13.5836)\n",
      "0 tensor(13.6501)\n",
      "0 tensor(13.6256)\n",
      "0 tensor(13.4663)\n",
      "0 tensor(12.9784)\n",
      "0 tensor(12.8737)\n",
      "0 tensor(12.8937)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m batch\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 34\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/software/computation/miniconda3/envs/ml/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/software/computation/miniconda3/envs/ml/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/software/computation/miniconda3/envs/ml/lib/python3.9/site-packages/torch/optim/adam.py:133\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/software/computation/miniconda3/envs/ml/lib/python3.9/site-packages/torch/optim/_functional.py:87\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m     86\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m---> 87\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001b[38;5;241m=\u001b[39mmax_exp_avg_sqs[i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# cpu loop\n",
    "losses_batch = []\n",
    "halfway = len(train_loader) // 2\n",
    "\n",
    "#scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        fwd_pass = model(batch['tokens']) # only logits\n",
    "\n",
    "        logits_unmasked = fwd_pass['logits'][batch['masked_indices']]\n",
    "        labels_unmasked = batch['labels'][batch['masked_indices']]\n",
    "        \n",
    "        loss = ce_loss(logits_unmasked, labels_unmasked.cpu())\n",
    "        print(loss)\n",
    "        \n",
    "model.train()        \n",
    "\n",
    "for _ in range(100):\n",
    "    for batch_index, batch in enumerate(test_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        fwd_pass = model(batch['tokens']) # only logits\n",
    "\n",
    "        logits_unmasked = fwd_pass['logits'][batch['masked_indices']]\n",
    "        labels_unmasked = batch['labels'][batch['masked_indices']]\n",
    "\n",
    "        loss = ce_loss(logits_unmasked, labels_unmasked)\n",
    "\n",
    "        del batch\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                fwd_pass = model(batch['tokens']) # only logits\n",
    "\n",
    "                logits_unmasked = fwd_pass['logits'][batch['masked_indices']]\n",
    "                labels_unmasked = batch['labels'][batch['masked_indices']]\n",
    "\n",
    "                loss = ce_loss(logits_unmasked, labels_unmasked)\n",
    "                print(batch_index, loss)\n",
    "\n",
    "                del batch\n",
    "\n",
    "        model.train()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3333aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "756bf0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff1acd9a160>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# copy pasted this from another notebook instead of capturing because lazy\n",
    "\n",
    "losses = [65.7486, 62.7873, 61.4920, 58.4905, 56.6521, 55.2771, 54.0909, 50.6875, 49.9440, 48.7508, 49.0366, 45.6912, 44.9796, 44.1981,\n",
    "          42.2238, 41.2584, 39.7795, 39.1225, 36.728, 36.3127, 34.4518, 35.5149, 34.2678, 33.0167, 31.8986, 31.8731, 30.8363, 29.8315,\n",
    "          29.0559, 28.4236, 28.8121, 28.4787, 26.4458, 27.1549, 25.9423, 25.8570, 25.0082, 25.2830, 25.2531, 23.8890, 24.0143, 23.0027, \n",
    "          23.6564, 22.3643, 22.0054, 21.7818, 22.3664, 22.0054, 21.7818, 22.3664, 22.2214, 22.1919, 21.3890, 21.6411, 20.8921, 20.4594, \n",
    "          20.7399, 19.7299, 20.9234, 20.1359, 19.1607, 18.5770, 18.8490, 18.7957, 18.3729, 17.5775, 17.9202, 17.5374, 16.7284, 17.3752,\n",
    "         17.2874, 17.4525, 16.2492, 16.3944, 16.2279, 16.3034, 15.7949, 15.5133, 15.1075, 15.2623, 15.0426, 14.8753, 15.0869, 15.1444, 14.421,\n",
    "         14.421, 14.4156, 14.59, 14.51, 14.000, 14.2922, 13.5836, 13.4663, 12.9784]\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76737764",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './state-dict.pth',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdb6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3079268f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f87c783ff10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# losses = [.5, .4568, .3538, .3786, .4046, .4503, .3055, .5221, .347, .4936, .3991, .4556, .3950, .4341, .4401, .4141,\\\n",
    "#           .511, .4030, .3859, .4714, .4197, .3512, .4796, .3916, .3598, .3606, .4395, .3865, .3565, .5050, .4287, .2985, .4247, \\\n",
    "#               .4434, .4578, .4469, .4875, .4336, .4957, .3987, .3791, .3195, .4968, .3250, .3648, .4496, .4175, \\\n",
    "#           .3703, .5425, .4998, .4605, .3880, .3407, .4117, .3808, .4145, .3636, .4256, .4222, .4466, .5085, .5007, \\\n",
    "#           .3795, .5068, .4244, .4176, .3885, .5385, .4360, .4877, .4208, .5163         \n",
    "#          ]\n",
    "\n",
    "# from scipy import ndimage\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# smooth = ndimage.gaussian_filter1d(losses, 3)\n",
    "# plt.plot(smooth, label='sigma 1.5')\n",
    "# plt.plot(losses, label='sigma 0.')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51afc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b02d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a022b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here below, testing functions to allow for per layer freezing or differential LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48468dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_layers = [\n",
    "    'lm_head',\n",
    "    'embed_positions',\n",
    "    'contact_head',\n",
    "    'emb_layer_norm_before',\n",
    "    'emb_layer_norm_after',\n",
    "    'embed_tokens',\n",
    "] # then 32 transformer layers\n",
    "\n",
    "named_layers += [f'transformer-{i}' for i in range(len(model.layers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_params(model, layer_description: Union[int, None], head: bool = False, pos_embed: bool = True, aa_embed: bool = True):\n",
    "            \n",
    "    \"\"\"\n",
    "    Returns parameters `layer_description` for the transformer layers or flags the freezing of the head and two embeddings with named args.\n",
    "    I wanted this function to either just freeze a layer from its name or return it so that I could provide it to an optimizer. \n",
    "    Never finished.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(layer_description, int) is None and layer_description != None:\n",
    "        raise TypeError('Arg `layer_description` must be of type int.')\n",
    "        \n",
    "    if mode == 'freeze':\n",
    "        in_group \n",
    "    \n",
    "    if isinstance(layer_description, int):\n",
    "        if layer_description > 33 or layer_description < 0:\n",
    "            raise ValueError('Arg `layer_description` must be between 0 and 34.')\n",
    "    \n",
    "        for idx in range(len(model.layers)):\n",
    "            if idx < layer_description:\n",
    "                if mode == 'freeze':\n",
    "                    model.layers[idx].requires_grad(False)\n",
    "                elif mode == ''\n",
    "    \n",
    "    is_bool_checker = lambda el: isinstance(el, bool)\n",
    "    for arg_name, var in zip(('head', 'pos_embed', 'aa_embed'), (head, pos_embed, aa_embed)):\n",
    "        if isinstance(var, bool) is False:\n",
    "            raise TypeError(f'Arg {ar_name} be of type bool.')\n",
    "    \n",
    "    if head:\n",
    "        getattr(model, 'lm_head').requires_grad_(False)\n",
    "    \n",
    "    if pos_embed:\n",
    "        getattr(model, 'embed_positions').requires_grad_(False)\n",
    "    \n",
    "    if aa_embed:\n",
    "        getattr(model, 'embed_tokens').requires_grad_(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7e4c0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('embed_tokens.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0400, -0.0787, -0.1237,  ..., -0.0205,  0.0952,  0.1434],\n",
       "          [-0.0255,  0.0207,  0.0418,  ..., -0.0434,  0.0045,  0.0602],\n",
       "          [ 0.0125, -0.1013,  0.0591,  ...,  0.0138,  0.0934,  0.0736],\n",
       "          ...,\n",
       "          [-0.0197,  0.0026,  0.0323,  ..., -0.0438, -0.0165, -0.0336],\n",
       "          [-0.0140, -0.0051, -0.0207,  ..., -0.0202, -0.0003, -0.0101],\n",
       "          [-0.0214,  0.0134,  0.0395,  ..., -0.0057,  0.0037,  0.0643]],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0006,  0.0447, -0.0345,  ..., -0.0340,  0.0024, -0.0409],\n",
       "          [-0.0117,  0.0563,  0.0225,  ...,  0.0378, -0.0242, -0.0344],\n",
       "          [ 0.0146,  0.0299,  0.0226,  ..., -0.0519,  0.0083, -0.0126],\n",
       "          ...,\n",
       "          [ 0.2439, -0.0351,  0.0753,  ..., -0.2883,  0.1647,  0.4050],\n",
       "          [-0.1716,  0.4006, -0.3696,  ..., -0.1650, -0.2449, -0.1517],\n",
       "          [ 0.0258,  0.3433, -0.1777,  ...,  0.1685, -0.2267, -0.2412]],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0074, -0.0008, -0.0040,  ..., -0.0350, -0.0301,  0.0205],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0481,  0.0329,  0.0422,  ..., -0.0465,  0.0184,  0.0418],\n",
       "          [-0.0197,  0.0275,  0.0676,  ...,  0.0546, -0.0305, -0.0197],\n",
       "          [ 0.0126, -0.0287,  0.0012,  ..., -0.0114, -0.0181,  0.0357],\n",
       "          ...,\n",
       "          [-0.0303, -0.0382, -0.0097,  ...,  0.0768,  0.0084,  0.0105],\n",
       "          [ 0.0234, -0.0082, -0.0203,  ..., -0.0067, -0.0130,  0.0238],\n",
       "          [ 0.0128, -0.0129,  0.0273,  ...,  0.0149, -0.0164,  0.0227]],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0157, -0.0184,  0.0114,  ...,  0.0055, -0.0208,  0.0226],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0214, -0.0990,  0.0352,  ..., -0.0083,  0.0383,  0.1033],\n",
       "          [-0.0011, -0.0529, -0.0396,  ..., -0.0812,  0.0268, -0.0058],\n",
       "          [-0.0596, -0.0786, -0.0333,  ...,  0.0397, -0.0009, -0.0576],\n",
       "          ...,\n",
       "          [-0.0282, -0.1205,  0.2126,  ...,  0.1362, -0.0869,  0.0173],\n",
       "          [-0.0571, -0.0289,  0.1323,  ..., -0.0290,  0.2205, -0.0226],\n",
       "          [-0.1532,  0.3086, -0.4536,  ..., -0.2888, -0.0250,  0.3250]],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0681,  0.0992,  0.0687,  ..., -0.0253, -0.0351,  0.0309],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0582,  0.0250,  0.0159,  ..., -0.0090,  0.0027,  0.0267],\n",
       "          [ 0.0139, -0.0189,  0.0180,  ...,  0.0048, -0.0013, -0.0226],\n",
       "          [ 0.0479,  0.0442, -0.0002,  ...,  0.0142, -0.0260, -0.0679],\n",
       "          ...,\n",
       "          [-0.0007,  0.0185, -0.0093,  ..., -0.0084, -0.0171, -0.0274],\n",
       "          [ 0.0125,  0.0191,  0.0262,  ...,  0.0393,  0.0137,  0.1000],\n",
       "          [ 0.0160,  0.0102, -0.0224,  ..., -0.0077, -0.0182, -0.0663]],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0341,  0.0020,  0.0192,  ..., -0.0028,  0.0316,  0.0032],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.0789, 0.4265, 0.0479,  ..., 0.3352, 0.0688, 0.0961],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0381, -0.0034, -0.0017,  ...,  0.0033, -0.0029, -0.0194],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1017,  0.2089, -0.3096,  ..., -0.2086, -0.1613,  0.2330],\n",
       "          [ 0.0113, -0.0032,  0.0205,  ..., -0.0260,  0.0011,  0.0218],\n",
       "          [ 0.0281,  0.0931,  0.1136,  ..., -0.0401, -0.1422, -0.0814],\n",
       "          ...,\n",
       "          [ 0.0909,  0.2681, -0.0031,  ...,  0.0994, -0.1164, -0.0803],\n",
       "          [-0.0315, -0.0235, -0.0392,  ...,  0.0293, -0.0476, -0.0255],\n",
       "          [-0.0526,  0.0751, -0.0475,  ...,  0.1360,  0.0865, -0.0186]],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0320, -0.0447, -0.0221,  ..., -0.0258, -0.0641, -0.0215],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1566,  0.0378, -0.0803,  ...,  0.0199,  0.0218, -0.0089],\n",
       "          [ 0.1092, -0.0066, -0.0249,  ...,  0.0547,  0.0437,  0.0354],\n",
       "          [-0.0115, -0.0044,  0.0432,  ...,  0.0687, -0.0614, -0.0357],\n",
       "          ...,\n",
       "          [ 0.0083,  0.0175,  0.0348,  ..., -0.0816, -0.0076,  0.0796],\n",
       "          [ 0.0135, -0.0223,  0.0034,  ..., -0.0176, -0.0021,  0.0566],\n",
       "          [ 0.0518, -0.0012,  0.0663,  ..., -0.0826,  0.0070,  0.1152]],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 5.7251e-02, -2.3193e-02, -2.0523e-02,  ...,  2.0325e-05,\n",
       "           1.5572e-02, -2.4536e-02], requires_grad=True)),\n",
       " ('layers.0.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.7065, 0.8696, 0.9531,  ..., 0.6392, 0.9531, 0.6133],\n",
       "         requires_grad=True)),\n",
       " ('layers.0.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0562,  0.0134,  0.0111,  ...,  0.0158,  0.0040,  0.0182],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 3.1471e-04,  1.0339e-01, -2.0740e-01,  ..., -1.8036e-02,\n",
       "            5.2551e-02, -1.4014e-01],\n",
       "          [ 1.5576e-01, -3.0441e-02,  4.5898e-02,  ..., -5.0934e-02,\n",
       "           -1.1902e-01, -2.5806e-01],\n",
       "          [-2.9221e-02,  2.1082e-01,  9.4421e-02,  ...,  2.3544e-02,\n",
       "            6.2622e-02, -7.6233e-02],\n",
       "          ...,\n",
       "          [ 8.0872e-02, -1.6797e-01, -1.2769e-01,  ..., -7.9880e-03,\n",
       "           -9.4849e-02,  1.3940e-01],\n",
       "          [-3.2837e-01,  1.4539e-01,  3.7158e-01,  ...,  6.5186e-02,\n",
       "            2.8735e-01,  3.1464e-02],\n",
       "          [ 2.0984e-01, -2.7100e-01,  8.6731e-02,  ..., -1.1096e-01,\n",
       "           -5.8301e-01,  3.0713e-01]], requires_grad=True)),\n",
       " ('layers.1.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0135, -0.0215, -0.0076,  ...,  0.0486, -0.0381, -0.0363],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1558, -0.0792,  0.0549,  ..., -0.0776, -0.0201, -0.0459],\n",
       "          [-0.0287,  0.0625, -0.0436,  ...,  0.0671, -0.0118,  0.0976],\n",
       "          [ 0.0612, -0.0738, -0.0205,  ..., -0.0249,  0.0284,  0.0161],\n",
       "          ...,\n",
       "          [-0.0919, -0.0386,  0.0380,  ..., -0.0953, -0.0444,  0.1030],\n",
       "          [ 0.0088,  0.0544,  0.0196,  ..., -0.0859,  0.0668,  0.0446],\n",
       "          [-0.0384,  0.1105, -0.0343,  ...,  0.0138,  0.0546,  0.0158]],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0104,  0.0117,  0.0031,  ..., -0.0190,  0.0139, -0.0395],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.2019,  0.0995,  0.0612,  ..., -0.1246,  0.0329,  0.0327],\n",
       "          [ 0.1882,  0.0797,  0.2717,  ..., -0.0726, -0.0329, -0.1989],\n",
       "          [ 0.0657, -0.0174, -0.1027,  ...,  0.0787, -0.0862, -0.0130],\n",
       "          ...,\n",
       "          [-0.3865,  0.2642, -0.1288,  ..., -0.1528,  0.0788,  0.0581],\n",
       "          [ 0.3525, -0.1528,  0.0561,  ...,  0.1626, -0.0497,  0.1406],\n",
       "          [-0.0508,  0.0253, -0.2014,  ..., -0.2949, -0.1100, -0.1042]],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0289, -0.0332,  0.1141,  ...,  0.1002, -0.1271,  0.2397],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1107,  0.0478, -0.0636,  ..., -0.0527, -0.0173,  0.1194],\n",
       "          [-0.0001, -0.0519,  0.0826,  ...,  0.0295, -0.0010, -0.1015],\n",
       "          [ 0.0215,  0.1041, -0.0213,  ..., -0.0350,  0.0506,  0.0269],\n",
       "          ...,\n",
       "          [ 0.0097, -0.0250,  0.0227,  ...,  0.0414, -0.0184,  0.0798],\n",
       "          [ 0.0145,  0.0003,  0.0466,  ..., -0.0290, -0.0338, -0.0148],\n",
       "          [-0.0026,  0.0039, -0.0641,  ..., -0.0092, -0.0591, -0.1268]],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1005, -0.0020, -0.0137,  ..., -0.0402,  0.0027, -0.0447],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9321, 0.5151, 0.4121,  ..., 0.4263, 0.5732, 0.5405],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0233, -0.0145, -0.0089,  ...,  0.0292, -0.0078,  0.0248],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0210, -0.0024,  0.0323,  ...,  0.1394, -0.0711, -0.2715],\n",
       "          [ 0.0589, -0.0152,  0.0096,  ...,  0.1320,  0.0037, -0.1423],\n",
       "          [-0.0025, -0.0882,  0.0060,  ..., -0.0579,  0.0324,  0.0630],\n",
       "          ...,\n",
       "          [ 0.0479, -0.0051, -0.1306,  ..., -0.0070, -0.0970, -0.0248],\n",
       "          [ 0.1191,  0.1045, -0.1188,  ...,  0.0170, -0.0264,  0.0359],\n",
       "          [-0.0655,  0.0327,  0.0206,  ...,  0.0560, -0.0959,  0.0393]],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0158, -0.0098, -0.0263,  ..., -0.0229, -0.0040, -0.0200],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0361, -0.1167,  0.0213,  ...,  0.0546, -0.0909,  0.0750],\n",
       "          [-0.0969,  0.0295,  0.0143,  ..., -0.0416, -0.1185,  0.0560],\n",
       "          [-0.0052,  0.0192, -0.0384,  ..., -0.0398,  0.0074, -0.0453],\n",
       "          ...,\n",
       "          [-0.0051, -0.0864,  0.0208,  ..., -0.0951, -0.0471,  0.0508],\n",
       "          [-0.0021, -0.0015,  0.0048,  ..., -0.1292, -0.0459, -0.0586],\n",
       "          [ 0.0114, -0.0925, -0.0511,  ...,  0.0481, -0.0567, -0.0145]],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0401,  0.0018,  0.0162,  ..., -0.0230,  0.0099, -0.0391],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.6055, 0.5386, 0.7363,  ..., 0.7168, 0.4443, 0.6890],\n",
       "         requires_grad=True)),\n",
       " ('layers.1.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1162, -0.0111, -0.0721,  ..., -0.0435, -0.0157, -0.0260],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0221, -0.0083,  0.0174,  ..., -0.0210,  0.1202, -0.0453],\n",
       "          [ 0.0624, -0.0747,  0.1068,  ...,  0.1167,  0.1109,  0.0040],\n",
       "          [-0.0973,  0.1284,  0.0175,  ..., -0.0607, -0.0687,  0.1664],\n",
       "          ...,\n",
       "          [ 0.0275, -0.1361, -0.0330,  ...,  0.0108, -0.0889,  0.0369],\n",
       "          [-0.0634,  0.0393, -0.0704,  ...,  0.0701, -0.0015,  0.0202],\n",
       "          [-0.0731, -0.0275,  0.0333,  ...,  0.1763,  0.0231,  0.0536]],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0094,  0.0110, -0.0043,  ...,  0.0062, -0.0032, -0.0082],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0352, -0.0440,  0.0654,  ..., -0.0037, -0.0107, -0.0038],\n",
       "          [ 0.1136,  0.1070,  0.0785,  ..., -0.0290, -0.0102, -0.0242],\n",
       "          [ 0.0235,  0.0475, -0.0369,  ..., -0.0446, -0.0220, -0.0063],\n",
       "          ...,\n",
       "          [ 0.0754, -0.1433, -0.0572,  ..., -0.0077, -0.1019, -0.0585],\n",
       "          [ 0.1110,  0.0870,  0.1733,  ..., -0.0333, -0.0243,  0.0163],\n",
       "          [ 0.0217,  0.0438,  0.0587,  ..., -0.0848, -0.0345, -0.0099]],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0080, -0.0087, -0.0147,  ..., -0.0010, -0.0303,  0.0011],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0198, -0.0540,  0.0936,  ...,  0.0212,  0.0812,  0.0127],\n",
       "          [-0.0311, -0.0632,  0.0390,  ...,  0.0068,  0.1289,  0.0913],\n",
       "          [-0.0983,  0.0184, -0.0480,  ...,  0.0931, -0.0310,  0.0095],\n",
       "          ...,\n",
       "          [-0.0498, -0.0269, -0.0331,  ...,  0.0305, -0.0414, -0.0678],\n",
       "          [-0.0315, -0.0930,  0.0077,  ...,  0.0909,  0.0292, -0.1012],\n",
       "          [-0.1425,  0.0420,  0.0531,  ..., -0.0239, -0.1530, -0.1093]],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0643, -0.0829, -0.0457,  ..., -0.1652,  0.0047, -0.0387],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0910, -0.0590, -0.0247,  ..., -0.1160, -0.0471, -0.0841],\n",
       "          [-0.0050, -0.0331, -0.0239,  ...,  0.0664, -0.0932,  0.0109],\n",
       "          [-0.1100,  0.0436,  0.0208,  ...,  0.0406, -0.0571,  0.0093],\n",
       "          ...,\n",
       "          [-0.1099,  0.0911,  0.1488,  ..., -0.0064,  0.0017,  0.0558],\n",
       "          [-0.1915, -0.0122,  0.0952,  ...,  0.1710,  0.0551,  0.1180],\n",
       "          [-0.0121,  0.0227, -0.0106,  ...,  0.0977, -0.0008,  0.0920]],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0254,  0.0029,  0.0275,  ..., -0.0251, -0.0186,  0.0094],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9575, 0.8936, 0.8999,  ..., 1.1016, 1.0410, 0.9854],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0045,  0.0039,  0.0060,  ...,  0.0236,  0.0226, -0.0568],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-7.7576e-02,  5.0316e-03,  2.4829e-01,  ..., -6.1218e-02,\n",
       "            6.8787e-02,  6.3904e-02],\n",
       "          [ 5.6549e-02, -1.4404e-01,  7.5867e-02,  ...,  2.0862e-01,\n",
       "            6.0516e-02, -1.8420e-01],\n",
       "          [-1.6479e-01,  3.6240e-04, -2.0172e-02,  ...,  3.3203e-02,\n",
       "            1.0938e-01,  8.1543e-02],\n",
       "          ...,\n",
       "          [-2.0081e-01, -5.9509e-02, -7.5012e-02,  ..., -5.2124e-02,\n",
       "            6.4026e-02,  6.3848e-04],\n",
       "          [ 1.7834e-01, -3.8910e-02, -4.3152e-02,  ...,  7.3792e-02,\n",
       "            4.8309e-02, -1.1847e-01],\n",
       "          [-1.0376e-01, -1.2169e-02, -6.6162e-02,  ..., -8.5938e-02,\n",
       "            6.7261e-02, -4.1626e-01]], requires_grad=True)),\n",
       " ('layers.2.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0077, -0.0205, -0.0300,  ...,  0.0198, -0.0142,  0.0006],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0286,  0.2227,  0.0207,  ..., -0.0232,  0.0584, -0.0543],\n",
       "          [ 0.2585, -0.1548,  0.0406,  ..., -0.1202,  0.1153,  0.0548],\n",
       "          [-0.0900, -0.0415, -0.0771,  ...,  0.1589, -0.0375,  0.0508],\n",
       "          ...,\n",
       "          [-0.1987,  0.2118, -0.0653,  ..., -0.0289, -0.0008,  0.1006],\n",
       "          [ 0.0054,  0.1678,  0.0709,  ..., -0.0279,  0.0164, -0.0397],\n",
       "          [ 0.1164,  0.2169,  0.0161,  ...,  0.0042, -0.1088,  0.0761]],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0233, -0.0051,  0.0147,  ..., -0.0149, -0.0174,  0.0136],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.5303, 0.6738, 0.8799,  ..., 0.9595, 0.6035, 0.9829],\n",
       "         requires_grad=True)),\n",
       " ('layers.2.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0091,  0.0515,  0.0255,  ...,  0.0080,  0.0568,  0.0350],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-4.7882e-02, -2.2869e-03,  6.0089e-02,  ...,  8.5938e-02,\n",
       "           -2.1716e-01,  2.0801e-01],\n",
       "          [-1.3708e-01, -6.9031e-02,  7.0496e-02,  ..., -3.4142e-03,\n",
       "            1.3818e-01,  1.2573e-01],\n",
       "          [-2.4084e-01, -1.0480e-01,  2.4976e-01,  ...,  1.6418e-01,\n",
       "           -1.9058e-02,  4.1779e-02],\n",
       "          ...,\n",
       "          [-5.0934e-02,  2.5330e-02, -1.1066e-01,  ...,  7.1045e-02,\n",
       "            1.6187e-01, -1.3149e-04],\n",
       "          [-5.0110e-02,  1.2299e-02, -2.5803e-02,  ...,  7.7148e-02,\n",
       "           -1.4807e-01, -1.4368e-01],\n",
       "          [-6.6162e-02, -1.2994e-05,  9.0942e-02,  ...,  6.8970e-02,\n",
       "            1.6101e-01, -4.7943e-02]], requires_grad=True)),\n",
       " ('layers.3.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0078, -0.0123,  0.0142,  ..., -0.0144,  0.0219, -0.0140],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1906,  0.1008,  0.0781,  ...,  0.1011,  0.0790, -0.0405],\n",
       "          [-0.0657,  0.0765, -0.1212,  ...,  0.0753,  0.0440,  0.0156],\n",
       "          [ 0.0189, -0.0707,  0.0680,  ...,  0.0207, -0.0336,  0.0726],\n",
       "          ...,\n",
       "          [ 0.0218, -0.0136, -0.0349,  ...,  0.0923,  0.0781,  0.0363],\n",
       "          [-0.0588,  0.0435,  0.0529,  ..., -0.0848,  0.0091, -0.1165],\n",
       "          [ 0.0081, -0.0333, -0.0561,  ...,  0.0310,  0.0145, -0.0638]],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0168,  0.0034, -0.0168,  ..., -0.0022, -0.0471,  0.0041],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1191,  0.0305, -0.0531,  ...,  0.0114,  0.0182, -0.0389],\n",
       "          [-0.0490, -0.0605,  0.0137,  ...,  0.0330,  0.0620, -0.0059],\n",
       "          [-0.0890,  0.0576, -0.0625,  ..., -0.1299, -0.1165,  0.2001],\n",
       "          ...,\n",
       "          [ 0.0482,  0.0976,  0.0136,  ...,  0.0614,  0.1837, -0.0898],\n",
       "          [ 0.1197,  0.0390,  0.0649,  ..., -0.1010, -0.1854, -0.0723],\n",
       "          [-0.0604, -0.0154, -0.1938,  ...,  0.0282, -0.0210,  0.0768]],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0119, -0.1304, -0.0884,  ..., -0.0423, -0.0515,  0.0379],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1799,  0.0482, -0.0794,  ..., -0.0616,  0.0813,  0.0491],\n",
       "          [ 0.0943, -0.0124,  0.0815,  ...,  0.0693, -0.0079, -0.0677],\n",
       "          [-0.0780, -0.0669,  0.0487,  ..., -0.0029,  0.0087,  0.0973],\n",
       "          ...,\n",
       "          [ 0.1043, -0.0605,  0.0702,  ..., -0.0446, -0.0090,  0.0522],\n",
       "          [-0.0358, -0.0673,  0.0383,  ..., -0.0192,  0.0574,  0.0452],\n",
       "          [ 0.0123,  0.0237, -0.0669,  ...,  0.0519, -0.0569,  0.0842]],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0153,  0.0108,  0.0314,  ..., -0.0399, -0.0101,  0.0566],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.7417, 0.6138, 0.7808,  ..., 0.7671, 0.6411, 0.7139],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0524, -0.0158, -0.0315,  ...,  0.0239, -0.0113, -0.0894],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-5.2673e-02, -7.4524e-02, -2.5970e-02,  ..., -1.3733e-01,\n",
       "            3.6792e-01,  6.6833e-02],\n",
       "          [-4.3457e-02, -1.0693e-01, -2.5146e-01,  ...,  5.4840e-02,\n",
       "            7.2876e-02, -5.6000e-02],\n",
       "          [ 4.1107e-02, -3.6041e-02, -1.2140e-01,  ...,  4.6005e-03,\n",
       "           -6.8726e-02, -1.2264e-03],\n",
       "          ...,\n",
       "          [ 3.7781e-02, -9.0271e-02,  1.2585e-01,  ...,  7.3425e-02,\n",
       "           -7.1655e-02, -6.6467e-02],\n",
       "          [-3.4380e-04, -1.4355e-01, -1.7773e-01,  ...,  4.0512e-03,\n",
       "            9.0820e-02,  6.3049e-02],\n",
       "          [ 3.7842e-02, -4.5471e-03, -2.4268e-01,  ...,  2.2827e-01,\n",
       "           -1.6617e-02,  2.3956e-02]], requires_grad=True)),\n",
       " ('layers.3.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0026, -0.0289,  0.0035,  ..., -0.0529,  0.0099, -0.0530],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0146,  0.2037,  0.1808,  ...,  0.1980, -0.0731,  0.0629],\n",
       "          [ 0.0749,  0.0340,  0.1316,  ...,  0.0163, -0.0272,  0.1924],\n",
       "          [-0.0334, -0.1849, -0.0873,  ...,  0.0892, -0.0306, -0.0967],\n",
       "          ...,\n",
       "          [-0.0609,  0.0338, -0.0981,  ...,  0.1866,  0.0196,  0.1157],\n",
       "          [ 0.0517, -0.0199,  0.1638,  ...,  0.0240, -0.0851,  0.0519],\n",
       "          [-0.0341,  0.1359, -0.2156,  ..., -0.0471,  0.0172,  0.0430]],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0593,  0.0005,  0.0012,  ..., -0.0210,  0.0059, -0.0500],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9771, 0.8916, 1.1299,  ..., 1.1895, 0.6494, 1.3477],\n",
       "         requires_grad=True)),\n",
       " ('layers.3.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.2346,  0.0584,  0.0634,  ..., -0.0109, -0.0478,  0.3467],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0026,  0.0537, -0.0359,  ...,  0.0899,  0.0397,  0.0185],\n",
       "          [ 0.0196,  0.0596,  0.1191,  ..., -0.1085,  0.1069, -0.1851],\n",
       "          [ 0.0569,  0.0670,  0.1188,  ..., -0.0880,  0.1698,  0.0400],\n",
       "          ...,\n",
       "          [ 0.0494,  0.0191,  0.1504,  ..., -0.1768, -0.0428,  0.1512],\n",
       "          [-0.1385,  0.0500,  0.1001,  ..., -0.0459, -0.0282, -0.0864],\n",
       "          [ 0.1494, -0.0065,  0.0354,  ...,  0.0790,  0.0299,  0.0572]],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0311, -0.0150, -0.0175,  ...,  0.0061,  0.0078,  0.0051],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0814,  0.0137,  0.0279,  ..., -0.1757,  0.1736, -0.0489],\n",
       "          [-0.0287, -0.2113,  0.1266,  ...,  0.1130, -0.0083, -0.0294],\n",
       "          [-0.0014,  0.0037, -0.0962,  ..., -0.0542,  0.0632,  0.0771],\n",
       "          ...,\n",
       "          [-0.0741,  0.0361,  0.0765,  ...,  0.1282, -0.0132,  0.0784],\n",
       "          [ 0.0639, -0.0146, -0.1155,  ...,  0.0267,  0.0046,  0.0040],\n",
       "          [-0.0151,  0.1002,  0.0179,  ..., -0.0355,  0.0016, -0.0257]],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 2.7130e-02, -8.9417e-03,  9.5844e-05,  ..., -9.3002e-03,\n",
       "          -1.9333e-02,  6.2790e-03], requires_grad=True)),\n",
       " ('layers.4.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0157, -0.1265,  0.0015,  ...,  0.0512,  0.0603,  0.0334],\n",
       "          [-0.0718,  0.0704,  0.0411,  ..., -0.0912, -0.0420,  0.0319],\n",
       "          [-0.0424, -0.0160,  0.1461,  ..., -0.1487,  0.3032,  0.0232],\n",
       "          ...,\n",
       "          [ 0.0712,  0.0269,  0.2512,  ...,  0.1180, -0.1161, -0.0641],\n",
       "          [ 0.0289, -0.0250, -0.0353,  ..., -0.0299, -0.1711, -0.0035],\n",
       "          [ 0.0412, -0.0808,  0.0417,  ..., -0.0592,  0.1653,  0.0227]],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.2500, -0.0305, -0.0397,  ...,  0.0028,  0.1315,  0.1721],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-1.1621e-01,  3.3630e-02,  3.3508e-02,  ..., -2.7756e-02,\n",
       "           -6.1893e-04,  1.1035e-01],\n",
       "          [ 7.4816e-04,  1.9547e-02,  7.1777e-02,  ...,  1.8677e-02,\n",
       "           -5.9753e-02, -6.2752e-03],\n",
       "          [-4.4159e-02, -1.6870e-01, -1.1145e-01,  ...,  4.9744e-02,\n",
       "            2.1347e-02,  1.0144e-01],\n",
       "          ...,\n",
       "          [ 1.2032e-02, -3.8239e-02, -5.6213e-02,  ..., -3.1242e-03,\n",
       "            4.7852e-02,  1.0352e-01],\n",
       "          [-1.2384e-01, -7.1831e-03, -3.3173e-02,  ...,  1.0986e-01,\n",
       "           -5.1422e-02, -1.2671e-01],\n",
       "          [ 8.3374e-02,  2.7313e-03, -6.3956e-05,  ...,  3.2928e-02,\n",
       "           -2.1000e-03,  7.7332e-02]], requires_grad=True)),\n",
       " ('layers.4.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0437, -0.0082,  0.0197,  ..., -0.0255,  0.0128, -0.0379],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.6323, 0.6001, 0.7578,  ..., 0.7715, 0.6421, 0.6729],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0859, -0.0210, -0.0300,  ...,  0.0102, -0.0169, -0.0457],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0008,  0.0093,  0.1523,  ..., -0.0953,  0.2024, -0.0387],\n",
       "          [-0.0512,  0.1066, -0.1575,  ..., -0.0688,  0.0691, -0.0467],\n",
       "          [ 0.1110,  0.0191, -0.0222,  ...,  0.0481,  0.0175,  0.0634],\n",
       "          ...,\n",
       "          [ 0.0917, -0.0699, -0.0582,  ..., -0.0470,  0.0400,  0.0979],\n",
       "          [ 0.0096,  0.0497,  0.0014,  ...,  0.0275,  0.0426, -0.0629],\n",
       "          [-0.0634,  0.0574,  0.0412,  ..., -0.1433,  0.1276,  0.1077]],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0398,  0.0194, -0.0478,  ..., -0.0493, -0.0262, -0.0742],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0687, -0.0830,  0.0613,  ...,  0.0522, -0.0020,  0.0061],\n",
       "          [ 0.1760, -0.1714,  0.0213,  ..., -0.1893,  0.0110, -0.1626],\n",
       "          [-0.0852, -0.0764,  0.1229,  ..., -0.1600,  0.0885, -0.0674],\n",
       "          ...,\n",
       "          [ 0.1527, -0.0861,  0.1091,  ..., -0.0452,  0.0135, -0.1442],\n",
       "          [-0.1197, -0.0558,  0.2517,  ...,  0.0084, -0.0867,  0.2047],\n",
       "          [ 0.0823,  0.0173, -0.0887,  ..., -0.0055, -0.0121,  0.1353]],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0718,  0.0078,  0.0270,  ..., -0.0417,  0.0173, -0.0140],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.1494, 1.0312, 1.1211,  ..., 1.1182, 0.7925, 1.1406],\n",
       "         requires_grad=True)),\n",
       " ('layers.4.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.2072, -0.0260,  0.0010,  ...,  0.0466, -0.0445,  0.0320],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.2292, -0.0933, -0.0555,  ...,  0.2404, -0.0170, -0.0455],\n",
       "          [-0.1707, -0.0412, -0.1931,  ...,  0.0977, -0.0286, -0.0989],\n",
       "          [ 0.1429,  0.0102, -0.0725,  ...,  0.0169,  0.1759, -0.1881],\n",
       "          ...,\n",
       "          [-0.0476,  0.0947, -0.0423,  ..., -0.1093,  0.0978, -0.1002],\n",
       "          [-0.0530, -0.2261, -0.0798,  ...,  0.0808,  0.0097,  0.0455],\n",
       "          [ 0.0319,  0.1318,  0.0566,  ...,  0.0111, -0.0380, -0.0900]],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0023,  0.0067, -0.0133,  ...,  0.0093, -0.0017, -0.0035],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0839,  0.0159,  0.0445,  ..., -0.0421,  0.0542,  0.0999],\n",
       "          [ 0.1471, -0.0206,  0.0168,  ...,  0.1179, -0.0462, -0.0373],\n",
       "          [-0.0624,  0.0950, -0.0350,  ...,  0.0467,  0.0077,  0.0658],\n",
       "          ...,\n",
       "          [ 0.0355, -0.1019,  0.0138,  ..., -0.0190, -0.1807,  0.1412],\n",
       "          [-0.0019, -0.0191, -0.1989,  ..., -0.2290,  0.0402,  0.1659],\n",
       "          [-0.0436, -0.0574,  0.1089,  ...,  0.0389,  0.0793, -0.1133]],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0014, -0.0035, -0.0086,  ...,  0.0058, -0.0044,  0.0112],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0856,  0.0422,  0.1335,  ...,  0.0242,  0.0260,  0.0388],\n",
       "          [ 0.0630, -0.0794, -0.2203,  ...,  0.0261, -0.1666, -0.0297],\n",
       "          [-0.0500, -0.0131,  0.1306,  ...,  0.0770,  0.2368, -0.0970],\n",
       "          ...,\n",
       "          [ 0.0188,  0.1207, -0.0757,  ..., -0.0655,  0.0785,  0.0384],\n",
       "          [-0.0433, -0.1771,  0.0827,  ..., -0.0891,  0.2092, -0.0014],\n",
       "          [-0.1918,  0.0713, -0.1326,  ...,  0.1283,  0.1331,  0.0866]],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1052, -0.2037, -0.0453,  ...,  0.0220,  0.0450, -0.0509],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0812,  0.0469,  0.1120,  ..., -0.0008,  0.0612,  0.0530],\n",
       "          [-0.0515, -0.0146, -0.0211,  ..., -0.0528, -0.0092, -0.0070],\n",
       "          [ 0.0319, -0.0423,  0.0030,  ..., -0.1589,  0.0725, -0.1508],\n",
       "          ...,\n",
       "          [-0.0072, -0.0183, -0.0611,  ...,  0.0263,  0.1478, -0.0592],\n",
       "          [-0.0715,  0.0525, -0.0042,  ...,  0.1835, -0.0710, -0.0638],\n",
       "          [-0.0795,  0.0978, -0.0321,  ..., -0.3101, -0.0485, -0.0529]],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0754, -0.0140,  0.0231,  ..., -0.0395,  0.0348, -0.0264],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.7349, 0.7173, 0.8052,  ..., 0.7539, 0.6709, 0.8232],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0742, -0.0081, -0.0189,  ..., -0.0048, -0.0023, -0.0332],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0187, -0.1254, -0.1531,  ..., -0.0768,  0.1442,  0.0107],\n",
       "          [ 0.0244,  0.0290, -0.2372,  ...,  0.0198,  0.1141, -0.0589],\n",
       "          [ 0.0116, -0.2595, -0.0380,  ..., -0.2866, -0.1525, -0.0527],\n",
       "          ...,\n",
       "          [ 0.0251,  0.0784,  0.0087,  ...,  0.0775, -0.0269,  0.1029],\n",
       "          [-0.0798, -0.0951,  0.0127,  ...,  0.0358, -0.0367,  0.0570],\n",
       "          [ 0.0656, -0.2014,  0.0029,  ...,  0.2285, -0.0449, -0.2059]],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0524, -0.0113, -0.0166,  ..., -0.0469, -0.0351,  0.0243],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1260,  0.0826, -0.0559,  ...,  0.0391, -0.0417,  0.0010],\n",
       "          [ 0.0043, -0.0069, -0.0806,  ..., -0.0372,  0.0134, -0.0068],\n",
       "          [ 0.0793, -0.0476, -0.0107,  ..., -0.0502,  0.0154,  0.1041],\n",
       "          ...,\n",
       "          [-0.1562,  0.0995, -0.0875,  ...,  0.1262,  0.0739,  0.0735],\n",
       "          [-0.1032, -0.0595, -0.1211,  ..., -0.1478,  0.0421, -0.0361],\n",
       "          [ 0.0532,  0.1121,  0.0322,  ...,  0.0839,  0.1917, -0.1284]],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1115, -0.0007,  0.0286,  ..., -0.0366,  0.0217, -0.0047],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.2139, 1.0117, 1.2256,  ..., 1.2100, 0.8232, 1.1240],\n",
       "         requires_grad=True)),\n",
       " ('layers.5.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.1838,  0.0099,  0.0302,  ...,  0.0372, -0.0014,  0.0206],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0431, -0.1051,  0.0509,  ..., -0.1189,  0.0497,  0.0713],\n",
       "          [ 0.0039, -0.0281,  0.0464,  ...,  0.0603,  0.1310, -0.1566],\n",
       "          [-0.0643, -0.1763,  0.0390,  ..., -0.0988, -0.0132,  0.0145],\n",
       "          ...,\n",
       "          [ 0.0276,  0.1958, -0.0024,  ...,  0.2113, -0.1322, -0.2031],\n",
       "          [ 0.1039, -0.0121,  0.0103,  ..., -0.0212,  0.2411, -0.0769],\n",
       "          [-0.0775, -0.0017, -0.0158,  ...,  0.0815,  0.0555, -0.0540]],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0121, -0.0079,  0.0155,  ..., -0.0016, -0.0082, -0.0061],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0690,  0.0320, -0.0731,  ...,  0.0048,  0.0718,  0.0516],\n",
       "          [-0.0079,  0.0701, -0.0703,  ...,  0.0606, -0.0151, -0.0087],\n",
       "          [ 0.0385, -0.1772, -0.0232,  ...,  0.0099,  0.0764,  0.0909],\n",
       "          ...,\n",
       "          [-0.0685,  0.0070, -0.0535,  ...,  0.0318,  0.0340, -0.0210],\n",
       "          [-0.0126, -0.1177,  0.0146,  ...,  0.0782, -0.0421,  0.0025],\n",
       "          [-0.2196, -0.0583,  0.0948,  ..., -0.0426,  0.0759, -0.1189]],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0120, -0.0214, -0.0009,  ..., -0.0177, -0.0021,  0.0046],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1414,  0.0829,  0.0327,  ..., -0.0278, -0.0830,  0.1208],\n",
       "          [ 0.0176,  0.1002, -0.2212,  ...,  0.0382, -0.2637,  0.0791],\n",
       "          [ 0.2686, -0.0538,  0.0064,  ...,  0.0800, -0.0736,  0.0568],\n",
       "          ...,\n",
       "          [ 0.2014,  0.0228, -0.0473,  ...,  0.0087, -0.2537, -0.3357],\n",
       "          [-0.1344, -0.0411,  0.0161,  ..., -0.1498,  0.1497,  0.0789],\n",
       "          [ 0.0697,  0.0273, -0.1061,  ..., -0.0550,  0.0749, -0.0088]],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1257, -0.0551, -0.0795,  ...,  0.1295,  0.0369,  0.3738],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0015,  0.0397, -0.0316,  ...,  0.0298, -0.0239,  0.1859],\n",
       "          [ 0.0970, -0.0779,  0.1167,  ...,  0.0280,  0.0534,  0.0157],\n",
       "          [ 0.1301,  0.0879,  0.1219,  ...,  0.0125,  0.0367,  0.0401],\n",
       "          ...,\n",
       "          [ 0.0124, -0.1594,  0.0653,  ..., -0.0238,  0.0023,  0.0477],\n",
       "          [-0.1134, -0.0542,  0.0561,  ...,  0.0790,  0.0179, -0.0549],\n",
       "          [ 0.1220,  0.1195, -0.1167,  ...,  0.0598, -0.0046,  0.0348]],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1401, -0.0249, -0.0140,  ..., -0.0283,  0.0201, -0.0064],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.6348, 0.7041, 0.8467,  ..., 0.7969, 0.7168, 0.7715],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0424, -0.0066,  0.0098,  ..., -0.0048,  0.0061, -0.0180],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0417, -0.0516,  0.0240,  ..., -0.0158, -0.1705, -0.0338],\n",
       "          [-0.0762,  0.0060, -0.0858,  ...,  0.1438,  0.0995,  0.0618],\n",
       "          [-0.0636,  0.0729, -0.1146,  ...,  0.0145,  0.1475, -0.0184],\n",
       "          ...,\n",
       "          [-0.0579,  0.0679, -0.0850,  ...,  0.0092,  0.1233, -0.0237],\n",
       "          [ 0.1823, -0.1132,  0.1238,  ..., -0.0759, -0.0235, -0.0733],\n",
       "          [-0.0709,  0.0734,  0.1016,  ...,  0.0651,  0.1522, -0.1743]],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0408, -0.0170, -0.0435,  ..., -0.0283, -0.0464, -0.0706],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1033, -0.1865, -0.0243,  ..., -0.0766,  0.0358, -0.1646],\n",
       "          [-0.1376,  0.0116, -0.0484,  ...,  0.1038,  0.0093, -0.1085],\n",
       "          [ 0.0338, -0.1080, -0.1309,  ...,  0.0340,  0.0486,  0.0383],\n",
       "          ...,\n",
       "          [ 0.0070,  0.2020, -0.1626,  ...,  0.1147,  0.1527,  0.1159],\n",
       "          [-0.1170,  0.0059,  0.0139,  ...,  0.1591,  0.1876, -0.1216],\n",
       "          [-0.0329,  0.0448,  0.0310,  ..., -0.0565, -0.1545,  0.1498]],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1405, -0.0029, -0.0126,  ..., -0.0270, -0.0130,  0.0192],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0957, 1.0703, 1.1436,  ..., 1.1377, 0.8530, 1.0654],\n",
       "         requires_grad=True)),\n",
       " ('layers.6.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0587,  0.0140,  0.0407,  ...,  0.0174,  0.0048, -0.0001],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1327,  0.0453, -0.1660,  ..., -0.0338,  0.0363,  0.0873],\n",
       "          [-0.0294, -0.0674, -0.1013,  ..., -0.0841, -0.0987, -0.0557],\n",
       "          [ 0.0559, -0.1132,  0.0275,  ...,  0.0476, -0.1276, -0.0495],\n",
       "          ...,\n",
       "          [-0.1203, -0.1174, -0.0129,  ..., -0.2323,  0.0726,  0.0182],\n",
       "          [ 0.0236, -0.1242,  0.1024,  ...,  0.1530, -0.1228, -0.0029],\n",
       "          [-0.1138, -0.1455,  0.1495,  ..., -0.1770, -0.1036,  0.0864]],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0101,  0.0158, -0.0137,  ...,  0.0102, -0.0152,  0.0054],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0630, -0.0483,  0.0065,  ...,  0.0986,  0.0635, -0.0176],\n",
       "          [-0.0364, -0.0355, -0.0097,  ...,  0.1935,  0.0696, -0.0503],\n",
       "          [ 0.0652,  0.0193,  0.0729,  ..., -0.0251, -0.0995,  0.0195],\n",
       "          ...,\n",
       "          [ 0.0344, -0.0151, -0.0220,  ..., -0.0740,  0.1139,  0.0679],\n",
       "          [ 0.1102,  0.0345,  0.0740,  ...,  0.0022,  0.0067,  0.0764],\n",
       "          [ 0.0955, -0.1366,  0.0887,  ...,  0.0455,  0.0242,  0.0079]],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0096,  0.0043,  0.0340,  ...,  0.0018, -0.0060,  0.0151],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0578,  0.0350, -0.1071,  ..., -0.0862,  0.1061,  0.0264],\n",
       "          [-0.0397,  0.2101,  0.1104,  ..., -0.0682, -0.0480, -0.2747],\n",
       "          [-0.0850, -0.2252,  0.1501,  ...,  0.0578, -0.0632,  0.1982],\n",
       "          ...,\n",
       "          [-0.0332, -0.0382,  0.0980,  ...,  0.2303,  0.0870, -0.0212],\n",
       "          [-0.0168,  0.0213, -0.1216,  ..., -0.0652, -0.1191,  0.0682],\n",
       "          [-0.0770,  0.0477,  0.0901,  ...,  0.0012, -0.0393, -0.0045]],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.1025,  0.0243, -0.0475,  ..., -0.0336, -0.0563,  0.0969],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0093,  0.0638, -0.0927,  ..., -0.0359, -0.0463,  0.0552],\n",
       "          [-0.0016,  0.0572,  0.1295,  ..., -0.0219, -0.0106,  0.1913],\n",
       "          [-0.0412,  0.0627, -0.1545,  ...,  0.0901, -0.0801, -0.1259],\n",
       "          ...,\n",
       "          [-0.0062, -0.0519,  0.0191,  ..., -0.0140, -0.1008,  0.0342],\n",
       "          [ 0.0144, -0.0166, -0.0582,  ..., -0.0347,  0.0071,  0.0018],\n",
       "          [ 0.0314,  0.0481, -0.0415,  ..., -0.1091, -0.0024, -0.0374]],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1580, -0.0204, -0.0306,  ..., -0.0275, -0.0052,  0.0190],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.6982, 0.8008, 0.8623,  ..., 0.7881, 0.7593, 0.8701],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0665, -0.0316,  0.0142,  ..., -0.0161, -0.0117, -0.0120],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0572, -0.1754,  0.0460,  ...,  0.1821, -0.1929,  0.0251],\n",
       "          [-0.0207,  0.0456, -0.0292,  ...,  0.1665, -0.2144,  0.1099],\n",
       "          [ 0.0567, -0.0396, -0.0720,  ..., -0.0483,  0.0743, -0.1244],\n",
       "          ...,\n",
       "          [-0.0782, -0.0262,  0.0017,  ..., -0.0103,  0.0724,  0.0502],\n",
       "          [ 0.0752,  0.0572, -0.1039,  ...,  0.1437, -0.0372,  0.0333],\n",
       "          [ 0.1232, -0.0045, -0.0018,  ...,  0.0701, -0.2024,  0.2710]],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0034, -0.0484, -0.0381,  ...,  0.0063, -0.0558, -0.0345],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0367,  0.2864, -0.0126,  ...,  0.1304,  0.0012, -0.0235],\n",
       "          [-0.1121,  0.1680, -0.0209,  ...,  0.0805,  0.1190,  0.0471],\n",
       "          [-0.0769,  0.0450,  0.2040,  ..., -0.0378, -0.0460,  0.1204],\n",
       "          ...,\n",
       "          [-0.1653,  0.3745, -0.0566,  ..., -0.0692, -0.0942,  0.0183],\n",
       "          [-0.0439, -0.1992, -0.0067,  ..., -0.1304,  0.0519,  0.0843],\n",
       "          [-0.0293,  0.0181, -0.1323,  ..., -0.0835,  0.0253,  0.1194]],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1218,  0.0007, -0.0398,  ...,  0.0221,  0.0029,  0.0647],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0908, 1.0137, 1.1279,  ..., 1.1523, 0.8584, 1.1309],\n",
       "         requires_grad=True)),\n",
       " ('layers.7.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0193, -0.0105,  0.0124,  ..., -0.0345, -0.0064, -0.0003],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0382,  0.2028, -0.0710,  ..., -0.0757, -0.1240, -0.0726],\n",
       "          [ 0.0188, -0.1050, -0.0235,  ...,  0.0212,  0.0925,  0.0765],\n",
       "          [-0.0892, -0.0693, -0.0360,  ..., -0.1202,  0.0486,  0.0233],\n",
       "          ...,\n",
       "          [-0.1108,  0.1216,  0.1517,  ...,  0.1857, -0.0941, -0.1072],\n",
       "          [-0.2764, -0.2893, -0.0233,  ...,  0.0650, -0.2498,  0.1234],\n",
       "          [ 0.0258,  0.1409,  0.0082,  ..., -0.2089,  0.1375,  0.0371]],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0140,  0.0032,  0.0186,  ..., -0.0524,  0.0313, -0.0407],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0080,  0.0190, -0.0723,  ..., -0.0859,  0.0574, -0.1215],\n",
       "          [-0.0102,  0.0710,  0.0911,  ..., -0.0219, -0.0896,  0.0726],\n",
       "          [ 0.0442,  0.1714,  0.0020,  ...,  0.0021, -0.0676, -0.0422],\n",
       "          ...,\n",
       "          [ 0.0057, -0.1031,  0.0276,  ...,  0.0163, -0.0837, -0.0431],\n",
       "          [ 0.0892, -0.0371,  0.0495,  ...,  0.0850, -0.0059,  0.0376],\n",
       "          [-0.0192,  0.0827,  0.0852,  ..., -0.0989, -0.0641, -0.0515]],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0410,  0.0020, -0.0237,  ..., -0.0029, -0.0168, -0.0022],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0424, -0.1003, -0.1029,  ...,  0.0732,  0.1030,  0.0084],\n",
       "          [-0.0952,  0.1198, -0.0338,  ...,  0.0719,  0.0874, -0.0486],\n",
       "          [ 0.1230,  0.0412, -0.0173,  ...,  0.3027,  0.0886,  0.0690],\n",
       "          ...,\n",
       "          [ 0.0121,  0.0558,  0.0735,  ..., -0.1848, -0.1099,  0.0831],\n",
       "          [ 0.2018, -0.1473,  0.0416,  ...,  0.0988, -0.0574, -0.0938],\n",
       "          [ 0.1699, -0.1117, -0.1287,  ...,  0.0461, -0.0190,  0.0587]],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0111,  0.1243, -0.1554,  ...,  0.1737, -0.0822,  0.2749],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0127, -0.0165,  0.0060,  ..., -0.0059,  0.0112, -0.0029],\n",
       "          [-0.0399, -0.0294, -0.0805,  ...,  0.0700,  0.0063, -0.0444],\n",
       "          [-0.0115, -0.0829, -0.0506,  ..., -0.0243, -0.0166, -0.0948],\n",
       "          ...,\n",
       "          [ 0.0688,  0.0123,  0.0274,  ...,  0.0992, -0.0389,  0.0750],\n",
       "          [-0.0365,  0.0941,  0.0010,  ...,  0.0102,  0.0771, -0.0831],\n",
       "          [ 0.0746, -0.0842,  0.0451,  ...,  0.0150,  0.0321, -0.1396]],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1479, -0.0256, -0.0302,  ..., -0.0005,  0.0095,  0.0518],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.7700, 0.8247, 0.8472,  ..., 0.8789, 0.8057, 0.9048],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0964, -0.0119, -0.0145,  ..., -0.0224, -0.0212,  0.0032],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0459, -0.2075, -0.1212,  ...,  0.0976, -0.0969, -0.0776],\n",
       "          [ 0.0296,  0.1174, -0.3062,  ..., -0.1368,  0.0476,  0.0674],\n",
       "          [-0.0042,  0.1161, -0.0211,  ...,  0.0318,  0.1140,  0.0024],\n",
       "          ...,\n",
       "          [ 0.0923, -0.2059, -0.1622,  ..., -0.0164,  0.0041, -0.0919],\n",
       "          [ 0.0356,  0.2301, -0.0080,  ...,  0.0396,  0.0452, -0.1271],\n",
       "          [ 0.0159,  0.0149,  0.0378,  ..., -0.0892,  0.0023, -0.1189]],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0371, -0.0387, -0.0191,  ..., -0.0316, -0.0704,  0.0017],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0192,  0.0329,  0.0585,  ...,  0.1077,  0.2053, -0.1627],\n",
       "          [-0.0768,  0.1160,  0.0540,  ..., -0.1142,  0.1775,  0.0662],\n",
       "          [-0.0691, -0.2798,  0.0630,  ...,  0.0336, -0.0086, -0.0514],\n",
       "          ...,\n",
       "          [ 0.0035, -0.0525,  0.0392,  ...,  0.0748, -0.2708, -0.0235],\n",
       "          [ 0.1219, -0.1692, -0.0172,  ..., -0.0932, -0.0164, -0.0146],\n",
       "          [ 0.0371,  0.1396, -0.0013,  ..., -0.0442, -0.1604, -0.1660]],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0871, -0.0049, -0.0362,  ...,  0.0377,  0.0081,  0.0898],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0918, 1.0225, 1.0811,  ..., 1.1357, 0.8477, 1.0234],\n",
       "         requires_grad=True)),\n",
       " ('layers.8.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0356,  0.0071,  0.0155,  ...,  0.0168,  0.0084, -0.0048],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-9.9915e-02,  2.0813e-02, -1.1237e-01,  ..., -8.4961e-02,\n",
       "           -9.7466e-04, -1.7749e-01],\n",
       "          [ 1.9434e-01,  1.4404e-01,  1.0890e-04,  ..., -1.4832e-01,\n",
       "           -4.6936e-02,  4.8279e-02],\n",
       "          [-2.7417e-01,  3.9917e-02,  4.8218e-02,  ..., -9.6191e-02,\n",
       "            3.5217e-02,  9.1309e-02],\n",
       "          ...,\n",
       "          [-8.1604e-02, -9.2651e-02,  3.7384e-02,  ..., -1.1298e-01,\n",
       "            1.4221e-01,  9.0942e-02],\n",
       "          [ 4.7882e-02,  8.3313e-03,  5.5389e-03,  ..., -8.2947e-02,\n",
       "            1.7651e-01, -1.5100e-01],\n",
       "          [-2.0248e-02,  2.8591e-03,  4.8752e-03,  ..., -2.2156e-02,\n",
       "           -1.2756e-01,  1.0815e-01]], requires_grad=True)),\n",
       " ('layers.9.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0002, -0.0089,  0.0028,  ...,  0.0124, -0.0200,  0.0187],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0237,  0.0245,  0.0158,  ...,  0.0679, -0.1143, -0.0575],\n",
       "          [-0.0770,  0.0486,  0.0082,  ...,  0.0789,  0.2251, -0.1306],\n",
       "          [ 0.0440,  0.0770, -0.0652,  ...,  0.0532, -0.0529,  0.0219],\n",
       "          ...,\n",
       "          [-0.0820,  0.0033, -0.0143,  ...,  0.1051, -0.0513, -0.0424],\n",
       "          [ 0.1114, -0.0144, -0.0020,  ..., -0.2075,  0.0088, -0.1611],\n",
       "          [ 0.0992,  0.1086, -0.1030,  ...,  0.1312, -0.0045, -0.1230]],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0143, -0.0025, -0.0118,  ..., -0.0108,  0.0070, -0.0156],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1516,  0.1052,  0.0630,  ...,  0.0610,  0.0666, -0.1285],\n",
       "          [ 0.0861,  0.0897,  0.0970,  ..., -0.0252, -0.0015,  0.1128],\n",
       "          [-0.0520,  0.0735, -0.1379,  ..., -0.0297, -0.1349, -0.0319],\n",
       "          ...,\n",
       "          [-0.0024,  0.1122,  0.0043,  ...,  0.0085, -0.0583, -0.1936],\n",
       "          [-0.2698, -0.0058,  0.0831,  ...,  0.0941,  0.1334,  0.0311],\n",
       "          [-0.2013,  0.0980,  0.1230,  ..., -0.0824, -0.0464, -0.0963]],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0094, -0.0396,  0.0490,  ...,  0.0753,  0.0760,  0.1456],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0351, -0.0033, -0.0722,  ...,  0.0497,  0.0827, -0.0299],\n",
       "          [ 0.1009, -0.1587,  0.1455,  ...,  0.0184,  0.0242,  0.0497],\n",
       "          [ 0.0118,  0.1215,  0.1205,  ..., -0.0276, -0.0072, -0.0071],\n",
       "          ...,\n",
       "          [ 0.1343,  0.0789, -0.0244,  ..., -0.1483,  0.2279,  0.0551],\n",
       "          [ 0.0181, -0.0291, -0.0779,  ..., -0.0185, -0.0970, -0.0785],\n",
       "          [ 0.0458, -0.0013, -0.1295,  ...,  0.0225,  0.1857, -0.0213]],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 1.3879e-01, -5.9319e-04, -4.4617e-02,  ...,  1.6998e-02,\n",
       "           1.3053e-04,  6.1951e-02], requires_grad=True)),\n",
       " ('layers.9.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.7344, 0.8833, 0.8853,  ..., 0.9287, 0.8462, 0.9180],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0695, -0.0326, -0.0174,  ..., -0.0039, -0.0065,  0.0178],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1494,  0.2184, -0.1274,  ..., -0.0089, -0.0670,  0.0288],\n",
       "          [-0.0195,  0.0811, -0.0152,  ..., -0.1562,  0.1278,  0.0483],\n",
       "          [ 0.0742,  0.1908, -0.0938,  ...,  0.1465,  0.0020,  0.0642],\n",
       "          ...,\n",
       "          [-0.0398,  0.0798, -0.1230,  ...,  0.0066,  0.1699, -0.0324],\n",
       "          [-0.0174,  0.0157,  0.1769,  ..., -0.0434, -0.0424, -0.0178],\n",
       "          [-0.1004, -0.0065, -0.0662,  ..., -0.1065,  0.0830, -0.0012]],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0365, -0.0481, -0.0241,  ..., -0.0107,  0.0024, -0.0360],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0011, -0.0041, -0.0361,  ..., -0.1934, -0.0867,  0.0265],\n",
       "          [-0.0141,  0.0417, -0.0377,  ..., -0.0032,  0.0084,  0.0470],\n",
       "          [ 0.0258, -0.1180, -0.0090,  ..., -0.1477,  0.1163,  0.0473],\n",
       "          ...,\n",
       "          [-0.0802, -0.1133,  0.2869,  ..., -0.1270,  0.2686,  0.0264],\n",
       "          [-0.1129,  0.0109,  0.2155,  ...,  0.0433, -0.1334, -0.0184],\n",
       "          [ 0.0052, -0.1947,  0.0055,  ...,  0.0449,  0.0687,  0.0760]],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1129,  0.0360, -0.0108,  ...,  0.0181, -0.0070,  0.1047],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0928, 0.9692, 1.0488,  ..., 1.1309, 0.8394, 1.0029],\n",
       "         requires_grad=True)),\n",
       " ('layers.9.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0191, -0.0163,  0.0193,  ...,  0.0657,  0.0433, -0.0227],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1044, -0.1770,  0.0412,  ..., -0.0419, -0.0085,  0.1707],\n",
       "          [-0.1072,  0.1376, -0.1331,  ...,  0.0984, -0.1240,  0.1027],\n",
       "          [-0.0742,  0.0712, -0.0334,  ..., -0.0569,  0.1317,  0.0510],\n",
       "          ...,\n",
       "          [-0.0395, -0.2484,  0.0898,  ...,  0.0740,  0.1815,  0.0616],\n",
       "          [-0.2119, -0.0919, -0.1812,  ...,  0.0454, -0.0553, -0.0262],\n",
       "          [ 0.0836,  0.0771, -0.1053,  ...,  0.0352,  0.1689, -0.0256]],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0134, -0.0260,  0.0086,  ..., -0.0216,  0.0187,  0.0182],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0576, -0.1471,  0.0831,  ...,  0.0745,  0.1932,  0.1023],\n",
       "          [-0.0784, -0.1293,  0.0491,  ...,  0.0302, -0.0811,  0.0063],\n",
       "          [ 0.0687, -0.0366,  0.0753,  ...,  0.1025, -0.0232, -0.0707],\n",
       "          ...,\n",
       "          [-0.0416,  0.1140,  0.0715,  ..., -0.0263, -0.0076,  0.0132],\n",
       "          [-0.1111, -0.0909,  0.0480,  ...,  0.0737,  0.0311,  0.0075],\n",
       "          [-0.0916, -0.0756,  0.1705,  ..., -0.0060,  0.0690,  0.0189]],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0004, -0.0070,  0.0137,  ..., -0.0024,  0.0219,  0.0224],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0591,  0.0825,  0.0769,  ...,  0.0217, -0.0421,  0.0943],\n",
       "          [ 0.0422,  0.0652, -0.0122,  ..., -0.0334, -0.1235, -0.0776],\n",
       "          [-0.0346, -0.0157, -0.0686,  ..., -0.1837,  0.1635,  0.0094],\n",
       "          ...,\n",
       "          [-0.1022, -0.0414, -0.0114,  ..., -0.0399, -0.0267,  0.1609],\n",
       "          [-0.1459,  0.0263, -0.1288,  ...,  0.1566,  0.0781,  0.0865],\n",
       "          [-0.0446,  0.1610, -0.2325,  ..., -0.0306, -0.0414,  0.0778]],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.2233,  0.0590, -0.0137,  ..., -0.2424, -0.0709,  0.0324],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0120,  0.0070, -0.0441,  ...,  0.0677,  0.1448,  0.1282],\n",
       "          [ 0.0153, -0.0401, -0.0302,  ..., -0.1414,  0.0121, -0.0351],\n",
       "          [-0.0264,  0.0536, -0.0230,  ..., -0.1036, -0.0607, -0.0194],\n",
       "          ...,\n",
       "          [ 0.0376,  0.0836, -0.1962,  ...,  0.0231, -0.0026, -0.0156],\n",
       "          [-0.0654, -0.1326,  0.1199,  ..., -0.1324,  0.0355, -0.1785],\n",
       "          [-0.0696,  0.0623,  0.0347,  ..., -0.0706, -0.0131,  0.0908]],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0910,  0.0253, -0.0075,  ...,  0.0035, -0.0191,  0.0717],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.8042, 0.8701, 0.8472,  ..., 0.8950, 0.8379, 0.9028],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1271,  0.0015, -0.0197,  ...,  0.0040, -0.0099,  0.0513],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0707,  0.1027, -0.1976,  ..., -0.1577, -0.0534,  0.2368],\n",
       "          [ 0.0165,  0.0598, -0.0828,  ...,  0.0911,  0.0925,  0.0521],\n",
       "          [ 0.0374, -0.1230,  0.0093,  ..., -0.2490, -0.0497, -0.0695],\n",
       "          ...,\n",
       "          [ 0.2751, -0.1600,  0.0025,  ...,  0.0977,  0.2435,  0.0057],\n",
       "          [ 0.0935, -0.0287, -0.0641,  ..., -0.2457, -0.2725, -0.0407],\n",
       "          [ 0.0686,  0.0191, -0.1373,  ..., -0.2124,  0.0022, -0.0528]],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0193, -0.0225, -0.0300,  ..., -0.0466, -0.0328,  0.0010],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0979, -0.0974,  0.0684,  ...,  0.1610,  0.2825,  0.0346],\n",
       "          [ 0.1022,  0.0020,  0.0828,  ..., -0.1635, -0.1051, -0.0235],\n",
       "          [ 0.1202,  0.1602, -0.0610,  ..., -0.0812,  0.0289, -0.0911],\n",
       "          ...,\n",
       "          [-0.0661,  0.0905, -0.3149,  ...,  0.0937, -0.1118, -0.1647],\n",
       "          [-0.1320,  0.0131,  0.0244,  ...,  0.0875, -0.0524,  0.1649],\n",
       "          [ 0.0117,  0.0033,  0.0538,  ..., -0.0931,  0.1754,  0.0479]],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0432,  0.0164, -0.0166,  ..., -0.0237, -0.0156,  0.0631],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.2734, 1.0391, 1.2393,  ..., 1.1758, 0.7720, 1.0469],\n",
       "         requires_grad=True)),\n",
       " ('layers.10.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.1065,  0.1032,  0.1682,  ...,  0.0986,  0.0154,  0.0381],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1282, -0.1584,  0.0509,  ...,  0.1464, -0.1776,  0.1200],\n",
       "          [-0.0256, -0.0948, -0.1145,  ..., -0.0437, -0.0656,  0.0323],\n",
       "          [ 0.1965, -0.1029,  0.0301,  ...,  0.2515, -0.1246, -0.1038],\n",
       "          ...,\n",
       "          [-0.1021,  0.0222,  0.0570,  ..., -0.0231, -0.1769,  0.1849],\n",
       "          [-0.0295, -0.3420,  0.0826,  ...,  0.2747,  0.0673, -0.0287],\n",
       "          [-0.2732,  0.1327, -0.2686,  ...,  0.2141,  0.0892,  0.0085]],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0027,  0.0053, -0.0188,  ..., -0.0065,  0.0017,  0.0292],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1511, -0.0359, -0.0828,  ..., -0.1937,  0.0193,  0.1415],\n",
       "          [-0.0905, -0.0598,  0.0273,  ...,  0.1630,  0.0094, -0.0222],\n",
       "          [-0.0383, -0.2478, -0.1799,  ...,  0.0827,  0.0532,  0.0212],\n",
       "          ...,\n",
       "          [ 0.1049, -0.0225, -0.1512,  ..., -0.0814,  0.0463,  0.0504],\n",
       "          [ 0.0390, -0.0444, -0.0115,  ...,  0.0898, -0.0136, -0.0608],\n",
       "          [ 0.0406,  0.0008, -0.0456,  ..., -0.0880,  0.0327,  0.0672]],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0283, -0.0062, -0.0177,  ..., -0.0091,  0.0061,  0.0086],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0800, -0.0394, -0.0848,  ...,  0.2590,  0.0250, -0.0355],\n",
       "          [ 0.1249, -0.0826, -0.1775,  ..., -0.0245,  0.0636,  0.1354],\n",
       "          [ 0.2915, -0.1871,  0.0110,  ...,  0.2251, -0.2083, -0.0272],\n",
       "          ...,\n",
       "          [ 0.0601, -0.1279, -0.1705,  ..., -0.1302, -0.0519,  0.0581],\n",
       "          [ 0.1946,  0.1215,  0.0413,  ...,  0.1290, -0.1037,  0.0187],\n",
       "          [-0.0514, -0.1414,  0.0718,  ...,  0.1387, -0.0151, -0.1683]],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0169,  0.2842,  0.0039,  ..., -0.0492, -0.1919,  0.0312],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1700,  0.0676,  0.1016,  ..., -0.0386,  0.0492, -0.1081],\n",
       "          [ 0.0931, -0.0087,  0.1025,  ..., -0.1458,  0.0050,  0.0938],\n",
       "          [ 0.1561, -0.0978, -0.0809,  ...,  0.0257, -0.0349, -0.0205],\n",
       "          ...,\n",
       "          [ 0.1184, -0.0599,  0.0003,  ...,  0.0792,  0.0590, -0.0184],\n",
       "          [ 0.0561,  0.2084,  0.0165,  ...,  0.0645, -0.0416, -0.0337],\n",
       "          [ 0.0509,  0.0242,  0.0930,  ...,  0.1630,  0.0187, -0.0475]],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0581, -0.0141, -0.0365,  ..., -0.0410, -0.0176,  0.0706],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.8242, 0.9131, 0.8701,  ..., 0.8721, 0.7993, 0.8906],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1144,  0.0116, -0.0168,  ...,  0.0232, -0.0234,  0.0241],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1542,  0.0636,  0.0267,  ..., -0.0666,  0.0713, -0.0169],\n",
       "          [ 0.0371,  0.0175, -0.0214,  ...,  0.0237, -0.0158, -0.0086],\n",
       "          [-0.1099,  0.0632, -0.2413,  ..., -0.0270, -0.0981, -0.0396],\n",
       "          ...,\n",
       "          [-0.0178, -0.0847, -0.1246,  ...,  0.0563, -0.1877,  0.1508],\n",
       "          [-0.0221,  0.0976,  0.0499,  ...,  0.0729, -0.1075,  0.0267],\n",
       "          [ 0.0895,  0.1353, -0.0942,  ..., -0.0221, -0.0786, -0.0007]],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0134, -0.0071, -0.0193,  ..., -0.0268, -0.0009, -0.0036],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-3.8745e-01,  4.7699e-02, -1.6675e-01,  ..., -1.4929e-01,\n",
       "            1.9006e-01, -6.3477e-02],\n",
       "          [ 2.1851e-01, -1.6739e-02, -1.5625e-02,  ...,  2.1133e-02,\n",
       "            5.5115e-02, -1.5839e-02],\n",
       "          [-2.0569e-01,  9.8267e-02, -2.8809e-01,  ..., -8.5205e-02,\n",
       "           -2.5487e-04,  1.3466e-02],\n",
       "          ...,\n",
       "          [-1.1035e-01, -6.4575e-02, -1.8201e-01,  ..., -3.4118e-04,\n",
       "            6.8176e-02, -2.9761e-01],\n",
       "          [-2.9724e-02, -1.4368e-01, -2.8915e-02,  ..., -4.8370e-02,\n",
       "            3.9642e-02, -2.4963e-02],\n",
       "          [-8.8562e-02, -1.3672e-01, -2.3315e-02,  ...,  8.3069e-02,\n",
       "           -2.6514e-01, -1.4185e-01]], requires_grad=True)),\n",
       " ('layers.11.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0876, -0.0337, -0.0872,  ..., -0.0435, -0.0138,  0.0361],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.4678, 1.1455, 1.3018,  ..., 1.1768, 0.6846, 1.0156],\n",
       "         requires_grad=True)),\n",
       " ('layers.11.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.3857,  0.1650,  0.3171,  ...,  0.0247,  0.0565,  0.0751],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1664,  0.0533,  0.0342,  ..., -0.1198,  0.3235, -0.1787],\n",
       "          [-0.0270,  0.0748,  0.0161,  ...,  0.1411,  0.0929, -0.1080],\n",
       "          [-0.1107,  0.1149,  0.0826,  ...,  0.1858, -0.0943, -0.1118],\n",
       "          ...,\n",
       "          [ 0.0312, -0.0370, -0.1404,  ..., -0.0906,  0.0315, -0.2209],\n",
       "          [-0.1235, -0.0856, -0.1658,  ...,  0.0084, -0.0815, -0.0766],\n",
       "          [-0.1683, -0.1445,  0.0645,  ...,  0.0517, -0.0137, -0.0544]],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0217,  0.0033, -0.0082,  ...,  0.0166,  0.0315,  0.0184],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0719,  0.0081,  0.0570,  ...,  0.0144,  0.1218,  0.0139],\n",
       "          [-0.0255, -0.0329,  0.0925,  ...,  0.0627,  0.0283, -0.0481],\n",
       "          [-0.0379, -0.0299,  0.0266,  ...,  0.1132,  0.1043,  0.0788],\n",
       "          ...,\n",
       "          [-0.1305, -0.0417, -0.1526,  ..., -0.0384, -0.0831, -0.0621],\n",
       "          [ 0.0816,  0.0609,  0.0115,  ...,  0.0522,  0.0390, -0.1337],\n",
       "          [-0.0139, -0.0840,  0.0752,  ...,  0.0756,  0.0458, -0.0007]],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0011, -0.0142, -0.0109,  ..., -0.0017, -0.0209,  0.0178],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1010,  0.1855,  0.1292,  ...,  0.0605,  0.1759, -0.0620],\n",
       "          [ 0.1375,  0.0420,  0.0080,  ...,  0.0424, -0.1340,  0.0683],\n",
       "          [ 0.0863, -0.0978,  0.0862,  ...,  0.1694, -0.3926,  0.0145],\n",
       "          ...,\n",
       "          [-0.0502, -0.0171,  0.0216,  ..., -0.1185, -0.0372, -0.0511],\n",
       "          [-0.1250,  0.1035, -0.1095,  ..., -0.2417,  0.0233, -0.0847],\n",
       "          [-0.1312,  0.0611, -0.1104,  ..., -0.1151, -0.2240,  0.0222]],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1816,  0.0568,  0.0829,  ..., -0.5791, -0.0795,  0.0750],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1433,  0.0037,  0.0452,  ...,  0.0216, -0.0958,  0.0737],\n",
       "          [ 0.0640,  0.0718, -0.0246,  ...,  0.1399, -0.1473,  0.0715],\n",
       "          [ 0.0210, -0.0695,  0.1625,  ...,  0.1399, -0.0901, -0.0531],\n",
       "          ...,\n",
       "          [ 0.0246, -0.0378,  0.0889,  ...,  0.0123,  0.0209, -0.0973],\n",
       "          [ 0.1505, -0.0113,  0.0820,  ..., -0.0930,  0.0692, -0.0454],\n",
       "          [-0.0348,  0.0379, -0.0091,  ...,  0.0140,  0.0710,  0.1602]],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1096, -0.0388, -0.0801,  ..., -0.0263, -0.0284,  0.0329],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.8149, 0.8691, 0.8862,  ..., 0.8555, 0.6011, 0.8276],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0776,  0.0038, -0.0353,  ...,  0.0043, -0.0254, -0.0001],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0855,  0.0508,  0.0260,  ..., -0.0763, -0.0757, -0.0056],\n",
       "          [ 0.0654, -0.1953,  0.0170,  ..., -0.1070,  0.1659, -0.0077],\n",
       "          [ 0.0870, -0.1075,  0.0734,  ..., -0.2554,  0.0132,  0.0863],\n",
       "          ...,\n",
       "          [-0.1515,  0.0380, -0.2117,  ...,  0.0085,  0.0193, -0.1191],\n",
       "          [-0.1331, -0.0790, -0.0227,  ...,  0.0669, -0.0995, -0.0504],\n",
       "          [-0.0202,  0.0958, -0.1205,  ..., -0.1555,  0.2727,  0.2561]],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0116, -0.0127, -0.0264,  ..., -0.0220, -0.0323, -0.0428],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0482,  0.1669,  0.0693,  ...,  0.0396, -0.1501, -0.0525],\n",
       "          [ 0.0147, -0.1262,  0.0204,  ...,  0.1051,  0.1381,  0.0123],\n",
       "          [ 0.1212,  0.2352, -0.0895,  ...,  0.0066, -0.0193, -0.2164],\n",
       "          ...,\n",
       "          [ 0.1465, -0.0988, -0.1812,  ..., -0.2395,  0.0435,  0.3008],\n",
       "          [-0.0226,  0.0890,  0.0474,  ...,  0.0131, -0.0150, -0.1748],\n",
       "          [-0.0792, -0.1048, -0.1970,  ..., -0.0475, -0.0565,  0.2383]],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0945, -0.0166, -0.0618,  ..., -0.0325, -0.0381, -0.0214],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.2051, 1.0869, 1.2402,  ..., 1.1895, 0.6885, 1.0957],\n",
       "         requires_grad=True)),\n",
       " ('layers.12.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0909, -0.0187,  0.0930,  ...,  0.0039,  0.1318,  0.1459],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0222,  0.0259, -0.0379,  ..., -0.1597, -0.0313, -0.1215],\n",
       "          [ 0.0699, -0.1183,  0.0540,  ..., -0.1807,  0.2015, -0.1164],\n",
       "          [-0.1880,  0.2313, -0.0987,  ...,  0.2266,  0.1508,  0.0100],\n",
       "          ...,\n",
       "          [ 0.0050, -0.1136,  0.0802,  ..., -0.0912,  0.0557, -0.1864],\n",
       "          [ 0.0497, -0.0094,  0.0157,  ...,  0.0887,  0.2094, -0.1201],\n",
       "          [-0.1958,  0.1180, -0.1721,  ...,  0.0234, -0.1642,  0.0567]],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0089,  0.0066, -0.0276,  ..., -0.0190,  0.0203, -0.0025],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0526, -0.0163,  0.0815,  ...,  0.1277, -0.0482, -0.0323],\n",
       "          [-0.1322, -0.0378, -0.0670,  ...,  0.1030, -0.0042,  0.0193],\n",
       "          [ 0.1063, -0.0758, -0.0911,  ..., -0.0373, -0.0041,  0.0512],\n",
       "          ...,\n",
       "          [ 0.0200,  0.0564, -0.1137,  ..., -0.1077, -0.0793,  0.0650],\n",
       "          [-0.1180,  0.1074,  0.0105,  ..., -0.0370, -0.0320, -0.1123],\n",
       "          [-0.0698,  0.1023,  0.0617,  ...,  0.1466,  0.0006,  0.0233]],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 4.5955e-05,  1.0521e-02,  3.5000e-04,  ...,  1.2550e-03,\n",
       "          -1.1311e-03, -1.3313e-03], requires_grad=True)),\n",
       " ('layers.13.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0215,  0.2095, -0.1031,  ..., -0.0544,  0.0627,  0.0190],\n",
       "          [ 0.0643,  0.0417,  0.0212,  ..., -0.0624, -0.2517, -0.1415],\n",
       "          [ 0.0244, -0.1248, -0.0506,  ..., -0.1032, -0.0086, -0.1576],\n",
       "          ...,\n",
       "          [ 0.2039,  0.1182,  0.0415,  ..., -0.0350,  0.0652,  0.0413],\n",
       "          [-0.1466, -0.0759, -0.0476,  ..., -0.1416,  0.0845, -0.0491],\n",
       "          [ 0.2417,  0.0638,  0.2578,  ...,  0.0147, -0.0331,  0.0950]],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0318, -0.0857,  0.0591,  ..., -0.0230, -0.0469,  0.0537],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0385,  0.1068, -0.0601,  ...,  0.1010,  0.0519,  0.0105],\n",
       "          [-0.1372, -0.0134,  0.0930,  ...,  0.0422, -0.0441,  0.0775],\n",
       "          [-0.0120,  0.1100,  0.0347,  ...,  0.0002, -0.0674,  0.1409],\n",
       "          ...,\n",
       "          [-0.0674,  0.0355,  0.0244,  ...,  0.0794, -0.0585, -0.1035],\n",
       "          [ 0.0898,  0.1170,  0.0311,  ...,  0.1138, -0.0231,  0.0317],\n",
       "          [ 0.0784, -0.0696, -0.0762,  ...,  0.0248,  0.1078,  0.0650]],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1018, -0.0374, -0.0309,  ..., -0.0084, -0.0313, -0.0136],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9497, 0.9248, 0.9307,  ..., 0.9429, 0.6533, 0.8950],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0585,  0.0062, -0.0529,  ...,  0.0011, -0.0109, -0.0086],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 8.6304e-02,  2.9907e-02, -1.3721e-01,  ...,  6.8481e-02,\n",
       "            8.8196e-02, -1.9385e-01],\n",
       "          [-3.4332e-02,  1.2866e-01,  1.3086e-01,  ..., -2.6392e-01,\n",
       "            1.1407e-01,  3.4515e-02],\n",
       "          [ 1.1681e-02,  1.8103e-01, -1.3586e-01,  ..., -5.0446e-02,\n",
       "           -4.5715e-02, -3.2617e-01],\n",
       "          ...,\n",
       "          [ 1.1743e-01,  3.0899e-02,  1.3098e-01,  ...,  9.7473e-02,\n",
       "           -2.1576e-02,  4.1626e-02],\n",
       "          [ 3.8683e-05,  1.4984e-02, -6.2225e-02,  ...,  8.5938e-02,\n",
       "           -1.6602e-02, -9.5940e-04],\n",
       "          [-2.0538e-02, -1.7654e-02, -2.0496e-01,  ...,  1.0431e-01,\n",
       "           -4.7340e-03, -1.3940e-01]], requires_grad=True)),\n",
       " ('layers.13.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0762, -0.0376, -0.0273,  ..., -0.0350, -0.0002, -0.0317],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1415,  0.3357, -0.0999,  ..., -0.1381, -0.1368,  0.0228],\n",
       "          [-0.2751,  0.0457,  0.0286,  ...,  0.1846, -0.1267, -0.2136],\n",
       "          [-0.2556,  0.3706, -0.3726,  ...,  0.0401, -0.0519, -0.0695],\n",
       "          ...,\n",
       "          [ 0.0450, -0.0039,  0.0840,  ...,  0.2878, -0.0029,  0.0177],\n",
       "          [ 0.0803,  0.1580,  0.0503,  ..., -0.2532, -0.0270,  0.0916],\n",
       "          [ 0.1110,  0.1707, -0.1587,  ...,  0.0137,  0.0142,  0.0380]],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0674, -0.0214,  0.0039,  ...,  0.0257, -0.0304, -0.0025],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.1250, 1.0664, 1.2510,  ..., 1.1836, 0.5947, 1.0273],\n",
       "         requires_grad=True)),\n",
       " ('layers.13.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0141, -0.1010,  0.0808,  ..., -0.0047,  0.0220,  0.0645],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1353, -0.0231,  0.2198,  ..., -0.0045, -0.2015,  0.4014],\n",
       "          [ 0.1199, -0.0635, -0.2279,  ..., -0.3630, -0.1639,  0.2664],\n",
       "          [-0.3530, -0.0136, -0.0112,  ...,  0.1017,  0.1877,  0.0833],\n",
       "          ...,\n",
       "          [-0.0811,  0.1769, -0.1033,  ..., -0.1219, -0.1891, -0.0659],\n",
       "          [-0.0776,  0.1478,  0.0058,  ..., -0.0771, -0.0945,  0.0819],\n",
       "          [ 0.1655, -0.1830,  0.0274,  ..., -0.0632,  0.2546, -0.0588]],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0121, -0.0017, -0.0085,  ...,  0.0243, -0.0204,  0.0196],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0464, -0.0489, -0.0908,  ...,  0.0510, -0.0617,  0.0086],\n",
       "          [-0.1021, -0.0402,  0.0977,  ...,  0.1323, -0.0104,  0.0206],\n",
       "          [-0.0093,  0.0972,  0.0781,  ..., -0.0523, -0.0027, -0.0183],\n",
       "          ...,\n",
       "          [-0.0112,  0.0316, -0.0312,  ...,  0.0632,  0.0532,  0.1003],\n",
       "          [-0.0253,  0.0327, -0.1035,  ...,  0.0207, -0.1060, -0.0539],\n",
       "          [-0.0610,  0.0624, -0.0587,  ..., -0.0662, -0.0380, -0.0005]],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0007,  0.0003,  0.0185,  ..., -0.0082, -0.0183,  0.0051],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.3816, -0.2727, -0.0709,  ...,  0.0186, -0.3169, -0.2808],\n",
       "          [ 0.0497, -0.2156,  0.0546,  ..., -0.2720,  0.0715, -0.0500],\n",
       "          [ 0.3630, -0.1153,  0.0503,  ..., -0.1379,  0.0760,  0.0986],\n",
       "          ...,\n",
       "          [-0.1259,  0.0865, -0.0072,  ..., -0.0105,  0.1505,  0.1993],\n",
       "          [-0.0597,  0.0162, -0.0825,  ..., -0.0443,  0.2139, -0.0557],\n",
       "          [-0.0632,  0.1492, -0.0469,  ..., -0.1727,  0.0038, -0.1643]],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.2343, -0.0229, -0.2008,  ..., -0.0196,  0.0916, -0.0550],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0902,  0.1503, -0.0126,  ..., -0.0417,  0.0337,  0.0759],\n",
       "          [ 0.0892,  0.1165, -0.1426,  ...,  0.1248, -0.0889,  0.0224],\n",
       "          [ 0.0238, -0.0007, -0.0897,  ..., -0.0590,  0.1433, -0.0757],\n",
       "          ...,\n",
       "          [ 0.1796,  0.0655, -0.2566,  ...,  0.0238, -0.0110,  0.0684],\n",
       "          [ 0.0293, -0.1949, -0.1099,  ..., -0.0562, -0.0901, -0.0421],\n",
       "          [-0.1333,  0.1033, -0.0385,  ...,  0.0302,  0.1160, -0.0706]],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0759, -0.0355, -0.0067,  ...,  0.0150, -0.0417, -0.0034],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9375, 0.9561, 0.9121,  ..., 0.9849, 0.6157, 0.8706],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0317,  0.0283, -0.0194,  ...,  0.0099, -0.0046,  0.0146],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 5.5023e-02,  1.1925e-02, -6.2988e-02,  ..., -1.4275e-02,\n",
       "            1.5167e-02,  6.7261e-02],\n",
       "          [-1.5637e-01, -1.2875e-05,  1.5480e-02,  ...,  3.7323e-02,\n",
       "            5.3192e-02,  4.3518e-02],\n",
       "          [-1.7471e-02,  4.7668e-02, -1.7236e-01,  ...,  2.9694e-02,\n",
       "            2.6245e-02,  2.0007e-01],\n",
       "          ...,\n",
       "          [-5.2948e-02,  8.2520e-02, -7.5745e-02,  ...,  4.6356e-02,\n",
       "            4.8248e-02,  3.5736e-02],\n",
       "          [ 6.4941e-02,  3.9764e-02, -3.8696e-02,  ...,  2.8955e-01,\n",
       "            9.7290e-02,  2.1130e-01],\n",
       "          [ 1.3403e-01,  5.6091e-02, -1.5564e-01,  ..., -2.5406e-02,\n",
       "           -2.1143e-01, -8.8928e-02]], requires_grad=True)),\n",
       " ('layers.14.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0255, -0.0520, -0.0228,  ..., -0.0346, -0.0219, -0.0424],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0395,  0.1969,  0.1114,  ...,  0.0945,  0.0518,  0.3054],\n",
       "          [-0.0455,  0.0444,  0.0569,  ...,  0.1542, -0.1482, -0.0095],\n",
       "          [ 0.0468,  0.0214, -0.0730,  ..., -0.0175, -0.2136, -0.0552],\n",
       "          ...,\n",
       "          [-0.0819,  0.1396, -0.1014,  ..., -0.0312, -0.0123,  0.1074],\n",
       "          [-0.0344, -0.0212,  0.0279,  ...,  0.0747, -0.1460, -0.0598],\n",
       "          [ 0.0657,  0.0041, -0.0253,  ...,  0.1442,  0.1403,  0.0012]],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0363, -0.0449,  0.0317,  ..., -0.0050, -0.0341,  0.0046],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0723, 1.0430, 1.1631,  ..., 1.1846, 0.5742, 1.0664],\n",
       "         requires_grad=True)),\n",
       " ('layers.14.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0016, -0.0286,  0.0724,  ..., -0.0378, -0.0076,  0.0526],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1055, -0.0527, -0.1245,  ..., -0.0985,  0.0437,  0.0749],\n",
       "          [-0.0474, -0.1533, -0.0854,  ...,  0.0254,  0.0051,  0.1616],\n",
       "          [ 0.0198,  0.1375, -0.0172,  ..., -0.0404, -0.0293, -0.0424],\n",
       "          ...,\n",
       "          [-0.0469, -0.0676,  0.0256,  ..., -0.0173,  0.0764, -0.0547],\n",
       "          [-0.0503,  0.0602,  0.2112,  ..., -0.0136, -0.0370, -0.0944],\n",
       "          [-0.1660,  0.0312, -0.0510,  ..., -0.1015,  0.1396, -0.1718]],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0414, -0.0728, -0.0559,  ...,  0.0670, -0.0147,  0.0143],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0016, -0.0356, -0.0246,  ...,  0.0062,  0.0213,  0.0990],\n",
       "          [-0.0193, -0.1061, -0.0863,  ...,  0.0374, -0.0052,  0.0050],\n",
       "          [-0.0318,  0.0742, -0.0101,  ..., -0.0441, -0.0497,  0.0390],\n",
       "          ...,\n",
       "          [ 0.0692, -0.0829, -0.1532,  ...,  0.0939,  0.0661,  0.1537],\n",
       "          [-0.1184,  0.0683, -0.1031,  ...,  0.0491, -0.0884,  0.1068],\n",
       "          [-0.0570,  0.0905, -0.0385,  ...,  0.0371,  0.0185,  0.0061]],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0365,  0.0175, -0.0040,  ...,  0.0176, -0.0164,  0.0067],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1436, -0.0094,  0.0862,  ...,  0.0456,  0.1544, -0.0322],\n",
       "          [-0.0707,  0.0636, -0.1388,  ..., -0.1766,  0.0539, -0.0555],\n",
       "          [-0.0030,  0.2457,  0.0300,  ..., -0.2979, -0.1324, -0.0558],\n",
       "          ...,\n",
       "          [ 0.2180,  0.1115,  0.0485,  ...,  0.0355,  0.1100,  0.0854],\n",
       "          [-0.0604,  0.0893,  0.0737,  ...,  0.0853, -0.0773,  0.0306],\n",
       "          [ 0.0182, -0.2747,  0.0938,  ..., -0.0900,  0.0737, -0.0354]],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.4182, -0.1230, -0.2615,  ...,  0.1327,  0.1191,  0.0396],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0666, -0.1032,  0.0080,  ...,  0.0187,  0.0822,  0.0087],\n",
       "          [-0.0770,  0.0869, -0.0703,  ...,  0.0906, -0.0761, -0.0415],\n",
       "          [ 0.0046,  0.1177,  0.0705,  ...,  0.1816,  0.0435,  0.0700],\n",
       "          ...,\n",
       "          [-0.0738, -0.0987, -0.0519,  ..., -0.0533, -0.0363,  0.0120],\n",
       "          [-0.0539, -0.0040,  0.0839,  ..., -0.1169, -0.0285, -0.0300],\n",
       "          [ 0.0076,  0.0105, -0.0151,  ..., -0.0213, -0.1125, -0.0956]],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0500, -0.0500, -0.0031,  ..., -0.0125, -0.0387,  0.0040],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9106, 0.9614, 0.9673,  ..., 1.0420, 0.6421, 0.9146],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0353,  0.0379, -0.0025,  ...,  0.0153, -0.0071,  0.0071],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1354, -0.0509,  0.0624,  ..., -0.0948, -0.0777, -0.1412],\n",
       "          [-0.1903, -0.0435,  0.0036,  ..., -0.0672, -0.0116,  0.0543],\n",
       "          [ 0.0271, -0.0044, -0.0734,  ..., -0.1105,  0.0462, -0.2185],\n",
       "          ...,\n",
       "          [ 0.1101,  0.0200, -0.0120,  ...,  0.0480, -0.1289,  0.0642],\n",
       "          [-0.0184, -0.0465,  0.1235,  ...,  0.1097, -0.1322,  0.0370],\n",
       "          [ 0.0143,  0.1313, -0.0560,  ..., -0.0575, -0.0237,  0.0292]],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0038, -0.0348, -0.0362,  ..., -0.0282, -0.0785, -0.0547],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1946, -0.0271,  0.0135,  ..., -0.0721,  0.1105, -0.0024],\n",
       "          [-0.0690,  0.0083,  0.0801,  ...,  0.0513,  0.0482, -0.0830],\n",
       "          [ 0.1226,  0.0104, -0.1864,  ..., -0.2803, -0.1765, -0.2013],\n",
       "          ...,\n",
       "          [-0.0536, -0.1003,  0.1829,  ..., -0.0707,  0.0811,  0.0229],\n",
       "          [-0.0010, -0.0570,  0.1456,  ..., -0.0609,  0.0425, -0.0926],\n",
       "          [-0.0562, -0.0867, -0.1721,  ...,  0.2025, -0.0797,  0.1181]],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0284, -0.0757,  0.0124,  ..., -0.0045, -0.0246,  0.0390],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0967, 1.0557, 1.0742,  ..., 1.1611, 0.5688, 0.9995],\n",
       "         requires_grad=True)),\n",
       " ('layers.15.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0611,  0.0198,  0.0115,  ...,  0.0137,  0.0173,  0.0203],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0881, -0.0370,  0.1826,  ..., -0.0421,  0.1031,  0.0862],\n",
       "          [-0.0108,  0.1256, -0.0468,  ...,  0.1023, -0.0974,  0.0516],\n",
       "          [ 0.0709,  0.2109,  0.0691,  ...,  0.1122,  0.1296,  0.0770],\n",
       "          ...,\n",
       "          [-0.0470,  0.0241,  0.1163,  ...,  0.0211,  0.1603, -0.1920],\n",
       "          [-0.0304, -0.0531,  0.0671,  ..., -0.0349, -0.1354, -0.0197],\n",
       "          [ 0.0191,  0.0396,  0.0830,  ..., -0.0461,  0.0306,  0.0684]],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0520, -0.0294, -0.0178,  ..., -0.0236,  0.0130, -0.0144],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0862,  0.0185, -0.0284,  ...,  0.0270,  0.0685,  0.0163],\n",
       "          [-0.0533, -0.0859, -0.0233,  ...,  0.0240,  0.0602, -0.0748],\n",
       "          [-0.0729, -0.0299,  0.0032,  ..., -0.0250, -0.0764,  0.0071],\n",
       "          ...,\n",
       "          [-0.0285,  0.1083,  0.0737,  ..., -0.0539,  0.0377,  0.0220],\n",
       "          [-0.0790,  0.0248,  0.0291,  ..., -0.1705,  0.1804,  0.0067],\n",
       "          [-0.1687, -0.0781,  0.0038,  ..., -0.0956, -0.0153,  0.0513]],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0373,  0.0262, -0.0183,  ...,  0.0017, -0.0486,  0.0213],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1153,  0.1626,  0.0928,  ..., -0.2183, -0.0418,  0.1368],\n",
       "          [-0.1862, -0.0329, -0.2190,  ...,  0.0222, -0.1207, -0.1062],\n",
       "          [-0.0055,  0.1844, -0.1036,  ..., -0.0429,  0.0814, -0.1155],\n",
       "          ...,\n",
       "          [-0.0199,  0.0589, -0.0396,  ..., -0.0169,  0.0319,  0.0261],\n",
       "          [-0.0999, -0.0752, -0.1041,  ..., -0.1604,  0.0486,  0.0709],\n",
       "          [ 0.0132,  0.0392, -0.0094,  ...,  0.0100, -0.1422,  0.0251]],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0341,  0.0677, -0.0521,  ...,  0.2969,  0.1758, -0.1902],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0272,  0.1254, -0.0250,  ..., -0.0591,  0.1874,  0.1534],\n",
       "          [ 0.0180, -0.0795, -0.0182,  ...,  0.1393, -0.0035, -0.0293],\n",
       "          [-0.1043, -0.1188,  0.0905,  ...,  0.1761, -0.0276,  0.0197],\n",
       "          ...,\n",
       "          [-0.0804, -0.0683, -0.0494,  ...,  0.0041,  0.1201,  0.1087],\n",
       "          [ 0.0692,  0.0310,  0.1199,  ..., -0.0581, -0.1527, -0.0003],\n",
       "          [-0.0246,  0.0252,  0.0398,  ...,  0.0963,  0.0306, -0.1639]],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0687, -0.0863,  0.0042,  ..., -0.0159, -0.0535,  0.0477],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9385, 0.9282, 0.9814,  ..., 1.0049, 0.6343, 0.8984],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0140,  0.0470, -0.0013,  ...,  0.0123,  0.0188,  0.0056],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0299, -0.0352, -0.1263,  ...,  0.0292,  0.1141, -0.1302],\n",
       "          [-0.1498,  0.0971, -0.0192,  ..., -0.0623, -0.0663, -0.1273],\n",
       "          [ 0.0920,  0.1011,  0.0257,  ..., -0.0941, -0.0820,  0.0331],\n",
       "          ...,\n",
       "          [-0.0217,  0.0419, -0.0761,  ...,  0.0153, -0.0590,  0.1364],\n",
       "          [-0.0066, -0.0225, -0.1868,  ..., -0.2306,  0.1184,  0.0494],\n",
       "          [ 0.2542, -0.0413,  0.0836,  ..., -0.1339,  0.1641,  0.0513]],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0202, -0.0563, -0.0275,  ..., -0.0602, -0.0259, -0.0263],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1212,  0.0110,  0.0068,  ..., -0.0329, -0.2478, -0.1106],\n",
       "          [ 0.0751,  0.1317, -0.0383,  ...,  0.2625,  0.0906,  0.1044],\n",
       "          [ 0.0237,  0.0189, -0.0529,  ...,  0.2324,  0.0652,  0.0097],\n",
       "          ...,\n",
       "          [ 0.2068,  0.0462,  0.0364,  ...,  0.1366, -0.1665,  0.2639],\n",
       "          [ 0.1183,  0.0429, -0.0246,  ...,  0.0428, -0.0267, -0.2847],\n",
       "          [ 0.0706, -0.0370, -0.0438,  ...,  0.0695,  0.0434, -0.1181]],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0745, -0.1025,  0.0248,  ..., -0.0107, -0.0670,  0.0446],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0195, 0.9834, 1.0488,  ..., 1.1348, 0.5918, 0.9668],\n",
       "         requires_grad=True)),\n",
       " ('layers.16.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0528,  0.0260,  0.0140,  ...,  0.0557,  0.0228,  0.0405],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0401,  0.1720, -0.1201,  ...,  0.0676,  0.0599,  0.0303],\n",
       "          [-0.0388,  0.1014,  0.0781,  ...,  0.1158, -0.0519, -0.1175],\n",
       "          [ 0.1615, -0.0133,  0.0365,  ..., -0.0667, -0.0784,  0.0552],\n",
       "          ...,\n",
       "          [ 0.1399, -0.0488,  0.1166,  ..., -0.0745,  0.0260, -0.0058],\n",
       "          [ 0.0671, -0.1716,  0.0834,  ..., -0.1276,  0.2391, -0.0613],\n",
       "          [-0.0590, -0.0507,  0.0710,  ..., -0.0819, -0.1482,  0.1389]],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0008,  0.0255,  0.0049,  ...,  0.0263, -0.0138,  0.0107],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0247,  0.0830,  0.0599,  ..., -0.0468, -0.0166,  0.0850],\n",
       "          [ 0.0428,  0.0266, -0.1677,  ..., -0.1962,  0.1033,  0.1978],\n",
       "          [ 0.0226, -0.0558,  0.0080,  ...,  0.0476, -0.1327,  0.1749],\n",
       "          ...,\n",
       "          [ 0.0213, -0.0875, -0.1220,  ..., -0.1232,  0.0067, -0.0462],\n",
       "          [-0.1309, -0.0830,  0.1343,  ...,  0.0538,  0.0588,  0.0152],\n",
       "          [ 0.0750,  0.0410,  0.0426,  ...,  0.0393,  0.0155, -0.0266]],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0018,  0.0168, -0.0094,  ...,  0.0236, -0.0231, -0.0509],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1392,  0.1978,  0.1129,  ..., -0.0103,  0.1664, -0.1652],\n",
       "          [ 0.0938,  0.0618, -0.0451,  ...,  0.0656, -0.0315, -0.0396],\n",
       "          [ 0.0265, -0.0517,  0.0898,  ...,  0.0228, -0.0521,  0.0080],\n",
       "          ...,\n",
       "          [-0.1062,  0.1188,  0.0486,  ..., -0.0338,  0.0598, -0.1989],\n",
       "          [-0.0186, -0.2600,  0.0601,  ..., -0.0023, -0.0161, -0.1193],\n",
       "          [ 0.1603, -0.0636,  0.0978,  ...,  0.0341,  0.1323,  0.0070]],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1853,  0.1693, -0.0446,  ..., -0.1184, -0.0088, -0.1216],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1605, -0.0372, -0.0576,  ...,  0.0248, -0.0747, -0.1098],\n",
       "          [-0.0567,  0.0511, -0.0181,  ...,  0.0815, -0.0051,  0.0837],\n",
       "          [-0.1486,  0.1506, -0.0157,  ..., -0.0880, -0.0641, -0.0731],\n",
       "          ...,\n",
       "          [-0.0595,  0.0071, -0.0072,  ...,  0.0324, -0.2228, -0.0502],\n",
       "          [-0.0984, -0.0612,  0.2015,  ..., -0.0729,  0.1050, -0.1825],\n",
       "          [-0.0532, -0.1128, -0.0840,  ..., -0.0009, -0.0319, -0.1018]],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0707, -0.0991, -0.0047,  ..., -0.0082, -0.1058,  0.0373],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9424, 0.9307, 0.9478,  ..., 0.9907, 0.7070, 0.9077],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0353,  0.0344,  0.0135,  ..., -0.0025,  0.0243, -0.0297],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0230,  0.0429,  0.0138,  ..., -0.1169,  0.2527,  0.0331],\n",
       "          [ 0.0651,  0.1389,  0.1517,  ..., -0.0944, -0.0460, -0.0077],\n",
       "          [ 0.0177,  0.1260,  0.0213,  ..., -0.1720, -0.0364, -0.1871],\n",
       "          ...,\n",
       "          [-0.1098,  0.0794, -0.0662,  ..., -0.0989, -0.0853, -0.1114],\n",
       "          [-0.0123,  0.2109,  0.1092,  ...,  0.0435, -0.1843,  0.0047],\n",
       "          [-0.0132, -0.0389,  0.0547,  ...,  0.0643, -0.0616,  0.0568]],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0181, -0.0073, -0.0456,  ..., -0.0453, -0.0564, -0.0736],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0132,  0.0494, -0.0648,  ...,  0.2534, -0.0575,  0.1499],\n",
       "          [ 0.1026, -0.0324, -0.0793,  ..., -0.0889, -0.1410,  0.0660],\n",
       "          [ 0.1099, -0.1404,  0.0583,  ..., -0.1543, -0.1780,  0.0712],\n",
       "          ...,\n",
       "          [ 0.1174,  0.1888,  0.1100,  ..., -0.1399, -0.0156,  0.0144],\n",
       "          [ 0.0407, -0.0383, -0.0201,  ..., -0.0157,  0.0410, -0.0071],\n",
       "          [ 0.1583, -0.0009, -0.1846,  ...,  0.0843,  0.0049,  0.0408]],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0577, -0.1160, -0.0324,  ..., -0.0273, -0.1138,  0.0428],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0166, 1.0186, 1.0430,  ..., 1.0762, 0.6328, 0.9531],\n",
       "         requires_grad=True)),\n",
       " ('layers.17.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.0042, 0.0204, 0.0064,  ..., 0.0557, 0.0350, 0.0113],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0321,  0.1222,  0.0466,  ..., -0.1028, -0.0003, -0.0363],\n",
       "          [ 0.1624, -0.1066,  0.0295,  ..., -0.0470,  0.0419,  0.0998],\n",
       "          [ 0.0104,  0.0403,  0.0455,  ..., -0.0159,  0.0010,  0.1126],\n",
       "          ...,\n",
       "          [ 0.0878,  0.0772,  0.0833,  ..., -0.0386,  0.1003, -0.0647],\n",
       "          [-0.0519, -0.0744, -0.0283,  ...,  0.0824,  0.1113, -0.2206],\n",
       "          [ 0.0278,  0.0181, -0.0104,  ..., -0.0286, -0.1733, -0.0457]],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0058,  0.0064,  0.0353,  ...,  0.0107, -0.0435,  0.0023],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0865,  0.0769, -0.0529,  ...,  0.0278,  0.0647,  0.0217],\n",
       "          [-0.0853, -0.1235,  0.0412,  ..., -0.1348,  0.1126, -0.0234],\n",
       "          [ 0.0687,  0.0401,  0.0768,  ...,  0.0912,  0.0024, -0.2122],\n",
       "          ...,\n",
       "          [-0.0504, -0.0988, -0.0097,  ...,  0.0787, -0.1146,  0.0605],\n",
       "          [-0.1389,  0.0218,  0.1080,  ..., -0.0925, -0.0649, -0.0713],\n",
       "          [-0.1405, -0.0767,  0.1099,  ..., -0.0856, -0.0793,  0.0533]],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0068, -0.0210,  0.0630,  ...,  0.0051, -0.0549,  0.0609],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1665,  0.0194,  0.0214,  ...,  0.1001,  0.1968, -0.0518],\n",
       "          [ 0.1870, -0.1337,  0.0529,  ..., -0.1963,  0.0799,  0.1120],\n",
       "          [-0.0450,  0.0382,  0.0035,  ..., -0.0465,  0.0168,  0.0349],\n",
       "          ...,\n",
       "          [-0.1271,  0.0125, -0.1605,  ..., -0.1094,  0.0183,  0.0404],\n",
       "          [-0.2025, -0.0668, -0.1432,  ...,  0.0828,  0.0134, -0.0015],\n",
       "          [ 0.1071, -0.0478, -0.1114,  ...,  0.1853, -0.2448, -0.1400]],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0527, -0.0154,  0.1234,  ..., -0.0254,  0.0378,  0.2087],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0566,  0.1274,  0.0112,  ...,  0.0532,  0.0381, -0.0211],\n",
       "          [-0.0021, -0.0066, -0.0398,  ...,  0.1020, -0.1167, -0.0255],\n",
       "          [ 0.0173,  0.0420, -0.0344,  ..., -0.0909, -0.1001,  0.0036],\n",
       "          ...,\n",
       "          [-0.0005, -0.0304,  0.0052,  ..., -0.0255,  0.1533,  0.0827],\n",
       "          [ 0.0009,  0.0025,  0.0331,  ...,  0.0576, -0.0441,  0.1533],\n",
       "          [ 0.0052,  0.1113,  0.0558,  ..., -0.0320,  0.0847, -0.1973]],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0703, -0.1049, -0.0396,  ..., -0.0497, -0.1278,  0.0620],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0039, 0.9966, 1.0137,  ..., 1.0225, 0.8203, 0.9624],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 6.5804e-03,  8.6823e-03, -9.1732e-05,  ...,  2.7863e-02,\n",
       "          -9.0361e-04, -2.9816e-02], requires_grad=True)),\n",
       " ('layers.18.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0634, -0.0882,  0.1031,  ..., -0.0335, -0.0280,  0.0670],\n",
       "          [-0.0687,  0.1071, -0.0037,  ..., -0.0257,  0.0188,  0.0322],\n",
       "          [ 0.0684,  0.0159,  0.0347,  ..., -0.2325,  0.1379, -0.0366],\n",
       "          ...,\n",
       "          [ 0.0197,  0.0421, -0.0327,  ..., -0.1066, -0.0677,  0.1216],\n",
       "          [-0.0224,  0.0637, -0.1488,  ...,  0.0657, -0.0429,  0.0542],\n",
       "          [-0.1003,  0.1205,  0.0347,  ..., -0.1011, -0.0484, -0.2469]],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0490, -0.0414, -0.0468,  ..., -0.0586, -0.0518, -0.0374],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1687, -0.1703, -0.0185,  ...,  0.0053, -0.0383,  0.0018],\n",
       "          [ 0.0588,  0.0247, -0.0350,  ...,  0.0769, -0.1827, -0.0209],\n",
       "          [ 0.0520,  0.0891, -0.0773,  ...,  0.0978, -0.1109,  0.2791],\n",
       "          ...,\n",
       "          [-0.0544,  0.0013,  0.0340,  ..., -0.0596,  0.1242, -0.1235],\n",
       "          [ 0.1527,  0.1398,  0.0858,  ...,  0.0543,  0.0406,  0.3979],\n",
       "          [-0.0143, -0.1418,  0.0010,  ..., -0.1324, -0.0822,  0.0623]],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0650, -0.1049, -0.0357,  ..., -0.0801, -0.1271,  0.0605],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9819, 1.0186, 0.9858,  ..., 1.0674, 0.7188, 0.9307],\n",
       "         requires_grad=True)),\n",
       " ('layers.18.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0359,  0.0118, -0.0038,  ...,  0.0891,  0.0058, -0.0016],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0968,  0.0471,  0.0064,  ...,  0.1853,  0.0016,  0.2546],\n",
       "          [-0.0454,  0.0607,  0.2080,  ...,  0.0228,  0.0170,  0.0377],\n",
       "          [-0.0468,  0.1648,  0.0539,  ..., -0.0779, -0.0380, -0.1285],\n",
       "          ...,\n",
       "          [ 0.1442,  0.0857,  0.2036,  ..., -0.2769,  0.4258,  0.0811],\n",
       "          [-0.0403,  0.0904,  0.0056,  ...,  0.0200,  0.0465,  0.1382],\n",
       "          [ 0.0996,  0.0316,  0.2539,  ..., -0.2325, -0.1410,  0.1416]],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0282, -0.0294, -0.0032,  ...,  0.0603, -0.0901,  0.0163],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1466, -0.0361, -0.1084,  ...,  0.0848, -0.0300, -0.0020],\n",
       "          [-0.1179, -0.0682, -0.0547,  ..., -0.0086, -0.0041,  0.0699],\n",
       "          [-0.1846, -0.0632, -0.0892,  ...,  0.0146,  0.0362,  0.1312],\n",
       "          ...,\n",
       "          [ 0.0335,  0.0210, -0.0756,  ...,  0.0384,  0.0765,  0.2026],\n",
       "          [ 0.0095, -0.0098,  0.0118,  ..., -0.0355,  0.0108, -0.0778],\n",
       "          [ 0.0494,  0.1605, -0.0258,  ..., -0.1471, -0.0823,  0.0596]],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0141,  0.0612, -0.0213,  ..., -0.0690,  0.0905,  0.0537],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0948, -0.0338, -0.0410,  ...,  0.0726,  0.0543, -0.0960],\n",
       "          [ 0.0393, -0.1063, -0.0489,  ...,  0.0098,  0.0003, -0.0125],\n",
       "          [ 0.1172, -0.1243, -0.0637,  ..., -0.1110, -0.0346, -0.0551],\n",
       "          ...,\n",
       "          [ 0.1407,  0.2539,  0.1290,  ..., -0.1148,  0.2308,  0.0214],\n",
       "          [ 0.0956, -0.1213, -0.0065,  ..., -0.2402,  0.1624, -0.0634],\n",
       "          [-0.0282, -0.1098, -0.1169,  ...,  0.0508, -0.2191, -0.0500]],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0372,  0.1438, -0.0077,  ...,  0.1924, -0.2374, -0.0507],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0550, -0.0332, -0.0273,  ...,  0.0857, -0.0896,  0.0209],\n",
       "          [ 0.0919, -0.1088,  0.0417,  ..., -0.0579,  0.0583, -0.0815],\n",
       "          [-0.1339,  0.0727,  0.1008,  ..., -0.1108,  0.1163,  0.0024],\n",
       "          ...,\n",
       "          [-0.0387, -0.0369,  0.0380,  ...,  0.0241, -0.1591,  0.0045],\n",
       "          [ 0.0754, -0.1488, -0.1554,  ..., -0.0444,  0.0889,  0.0823],\n",
       "          [ 0.0999,  0.1390, -0.2177,  ...,  0.0247,  0.0661,  0.1011]],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0935, -0.1148, -0.0316,  ..., -0.0657, -0.1241,  0.0749],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9771, 1.0361, 0.9854,  ..., 1.0176, 0.8091, 0.9277],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0104,  0.0061, -0.0095,  ...,  0.0024, -0.0263, -0.0464],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1365,  0.0897, -0.0150,  ...,  0.1558, -0.0064,  0.0707],\n",
       "          [-0.1083, -0.1478,  0.0192,  ...,  0.0822, -0.0990, -0.0218],\n",
       "          [ 0.0158, -0.0907,  0.0760,  ...,  0.0889, -0.0946,  0.0559],\n",
       "          ...,\n",
       "          [ 0.0108,  0.0563,  0.0010,  ...,  0.1076,  0.0348,  0.0657],\n",
       "          [ 0.1583,  0.0005, -0.0080,  ..., -0.0537,  0.1613,  0.0881],\n",
       "          [-0.0476, -0.0098, -0.0837,  ..., -0.1536,  0.0902,  0.0908]],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0280, -0.0696, -0.0225,  ..., -0.0544, -0.0374, -0.0617],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1244, -0.0808,  0.0466,  ...,  0.0279, -0.0820,  0.1775],\n",
       "          [-0.0229, -0.1873,  0.0295,  ..., -0.0716,  0.1159, -0.1320],\n",
       "          [-0.0149,  0.1086, -0.1257,  ..., -0.1515,  0.0399,  0.0693],\n",
       "          ...,\n",
       "          [-0.0475,  0.0884,  0.0466,  ...,  0.0124, -0.0798, -0.1619],\n",
       "          [ 0.0748,  0.0446,  0.0508,  ...,  0.0869, -0.0950,  0.1632],\n",
       "          [ 0.1118, -0.0398, -0.0361,  ...,  0.1736, -0.0275, -0.0045]],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1031, -0.0925, -0.0199,  ..., -0.0688, -0.1072,  0.0794],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9526, 0.9692, 0.9868,  ..., 1.0059, 0.7661, 0.9824],\n",
       "         requires_grad=True)),\n",
       " ('layers.19.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0249, -0.0248, -0.0274,  ...,  0.0471, -0.0253, -0.0203],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0910, -0.0312, -0.0963,  ...,  0.1302, -0.2749, -0.0062],\n",
       "          [ 0.0058, -0.0616, -0.1600,  ...,  0.1058,  0.0564,  0.0945],\n",
       "          [ 0.1307,  0.0496,  0.1862,  ...,  0.2861,  0.0486, -0.2634],\n",
       "          ...,\n",
       "          [-0.0724, -0.0121,  0.0120,  ...,  0.0323, -0.0878,  0.0756],\n",
       "          [-0.0291, -0.0191, -0.1858,  ..., -0.0972,  0.0386, -0.0055],\n",
       "          [ 0.1582,  0.0169,  0.0259,  ..., -0.2030,  0.2366, -0.0952]],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0717, -0.0355,  0.0451,  ..., -0.0172,  0.0157, -0.0305],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1101, -0.1174,  0.0905,  ...,  0.0645,  0.1486, -0.1899],\n",
       "          [ 0.0901,  0.1105, -0.0174,  ...,  0.0313, -0.0690, -0.0681],\n",
       "          [-0.0924,  0.1133, -0.1006,  ...,  0.0507,  0.1744,  0.0905],\n",
       "          ...,\n",
       "          [ 0.2313, -0.0383, -0.0043,  ..., -0.0475,  0.1976,  0.1200],\n",
       "          [ 0.0747, -0.0828, -0.0754,  ..., -0.1231, -0.0423,  0.0573],\n",
       "          [-0.0124,  0.0080, -0.1492,  ..., -0.0459, -0.0848, -0.0744]],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0032,  0.0270, -0.0493,  ...,  0.0586, -0.0320, -0.0100],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.2184,  0.0194, -0.0180,  ...,  0.0521, -0.1466, -0.0541],\n",
       "          [ 0.0193, -0.1277,  0.0947,  ...,  0.0075,  0.1698, -0.0517],\n",
       "          [-0.0667,  0.1293,  0.0607,  ...,  0.3101, -0.0047, -0.1244],\n",
       "          ...,\n",
       "          [ 0.1068, -0.0516,  0.0197,  ...,  0.0533,  0.0880,  0.0370],\n",
       "          [-0.1377, -0.1294, -0.0344,  ..., -0.0623, -0.0518,  0.0220],\n",
       "          [-0.0993, -0.0668, -0.0009,  ..., -0.0210, -0.1846,  0.1126]],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0358, -0.0763,  0.0244,  ..., -0.1270, -0.1455, -0.0297],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1093, -0.0197,  0.1796,  ..., -0.0079, -0.1199, -0.0346],\n",
       "          [ 0.0382,  0.0413, -0.0416,  ...,  0.0093, -0.0946,  0.1897],\n",
       "          [-0.0414, -0.1169, -0.0363,  ..., -0.0265, -0.0411,  0.1812],\n",
       "          ...,\n",
       "          [-0.0933, -0.1161, -0.1304,  ...,  0.0282,  0.1559,  0.0207],\n",
       "          [-0.1362,  0.0322, -0.0611,  ..., -0.1068,  0.0267,  0.1131],\n",
       "          [-0.0297,  0.1224,  0.0546,  ..., -0.0371, -0.1887, -0.1149]],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1036, -0.0906, -0.0144,  ..., -0.0825, -0.1091,  0.1154],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.8418, 0.9111, 0.9229,  ..., 0.8857, 0.8257, 0.8584],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0100, -0.0080, -0.0157,  ..., -0.0011, -0.0434, -0.0359],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-7.6782e-02, -2.9053e-02,  1.1253e-02,  ..., -4.3058e-04,\n",
       "           -5.7617e-02, -6.6040e-02],\n",
       "          [-1.5930e-01, -1.3806e-01, -2.2552e-02,  ..., -3.6041e-02,\n",
       "           -4.0359e-03,  8.7830e-02],\n",
       "          [ 9.1675e-02, -5.2643e-02,  1.4331e-01,  ..., -1.1780e-01,\n",
       "           -2.4780e-02,  5.7526e-02],\n",
       "          ...,\n",
       "          [-4.7089e-02,  1.4746e-01, -2.3096e-01,  ...,  4.7852e-02,\n",
       "            6.0547e-02, -5.5206e-02],\n",
       "          [ 9.8694e-02,  1.0608e-01,  1.5906e-01,  ..., -2.9160e-02,\n",
       "            1.5930e-01, -2.1924e-01],\n",
       "          [ 1.4257e-04,  3.0426e-02, -2.6932e-03,  ..., -2.0544e-01,\n",
       "           -9.1476e-03,  9.8328e-02]], requires_grad=True)),\n",
       " ('layers.20.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0496, -0.0626, -0.0205,  ..., -0.0599, -0.0494, -0.0396],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1131,  0.0406, -0.1045,  ..., -0.1610, -0.1377,  0.0275],\n",
       "          [-0.1423, -0.0613,  0.0919,  ...,  0.0396, -0.0983,  0.2673],\n",
       "          [-0.1404, -0.0061, -0.1295,  ..., -0.0823,  0.0058,  0.1154],\n",
       "          ...,\n",
       "          [-0.1829, -0.1616, -0.0157,  ..., -0.0441, -0.1115,  0.0435],\n",
       "          [-0.0177,  0.0283,  0.0471,  ..., -0.0855,  0.0729,  0.0230],\n",
       "          [-0.0197, -0.0190,  0.1450,  ...,  0.1009,  0.0386,  0.0071]],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1234, -0.0788,  0.0024,  ..., -0.0831, -0.1098,  0.1064],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0469, 1.0283, 1.0332,  ..., 1.0547, 0.8203, 0.9917],\n",
       "         requires_grad=True)),\n",
       " ('layers.20.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0385, -0.0543, -0.0282,  ...,  0.0432, -0.0399,  0.0439],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1361,  0.0039,  0.1199,  ...,  0.1501,  0.1147, -0.1671],\n",
       "          [-0.1936, -0.1001, -0.1079,  ...,  0.0842,  0.2542,  0.0340],\n",
       "          [-0.0286, -0.0595, -0.1290,  ..., -0.1311, -0.0991,  0.1498],\n",
       "          ...,\n",
       "          [-0.1182, -0.0646, -0.1560,  ...,  0.1680, -0.2700,  0.2585],\n",
       "          [-0.0694, -0.1348, -0.0278,  ..., -0.0065,  0.1180, -0.1479],\n",
       "          [ 0.1511,  0.1677,  0.0864,  ...,  0.0403, -0.1299, -0.0485]],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-1.7120e-02,  2.0065e-02,  1.0977e-03,  ...,  1.0277e-02,\n",
       "           5.3740e-04,  1.9968e-05], requires_grad=True)),\n",
       " ('layers.21.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0337, -0.0414,  0.2369,  ...,  0.2354,  0.0623,  0.0817],\n",
       "          [ 0.0297,  0.0113,  0.0421,  ...,  0.1487, -0.0308,  0.1259],\n",
       "          [ 0.0040,  0.1547,  0.0513,  ...,  0.1110, -0.1177,  0.0748],\n",
       "          ...,\n",
       "          [ 0.0151, -0.0569,  0.0360,  ...,  0.1101, -0.0197, -0.0168],\n",
       "          [ 0.3274,  0.0300,  0.0325,  ..., -0.0736, -0.1447, -0.0970],\n",
       "          [-0.0245, -0.0609, -0.0077,  ..., -0.0096, -0.1278,  0.1965]],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0424, -0.0125, -0.0102,  ...,  0.0050,  0.0079, -0.0387],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0345, -0.0660,  0.0157,  ..., -0.0286,  0.0759, -0.0933],\n",
       "          [-0.0048, -0.0975,  0.0209,  ...,  0.0585, -0.0045,  0.0560],\n",
       "          [ 0.0313, -0.1384, -0.1022,  ...,  0.2157, -0.0844, -0.1171],\n",
       "          ...,\n",
       "          [-0.0526, -0.1425,  0.0076,  ..., -0.1104, -0.0743,  0.0285],\n",
       "          [ 0.0754, -0.0486, -0.0790,  ..., -0.1364, -0.0666,  0.0079],\n",
       "          [ 0.0839, -0.1722, -0.0100,  ..., -0.0238,  0.0549,  0.2385]],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.2263,  0.0025, -0.0080,  ...,  0.0718,  0.0583, -0.1808],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0242, -0.0171,  0.0172,  ...,  0.0609, -0.1849,  0.1234],\n",
       "          [ 0.0393,  0.0828, -0.0738,  ..., -0.0458, -0.0084,  0.0561],\n",
       "          [-0.1252,  0.0728, -0.1187,  ...,  0.0227,  0.0501,  0.0161],\n",
       "          ...,\n",
       "          [-0.1034, -0.1343, -0.0980,  ...,  0.0450,  0.0191, -0.0316],\n",
       "          [ 0.1082,  0.1555,  0.0391,  ...,  0.0629, -0.0523, -0.0927],\n",
       "          [-0.1475, -0.1838, -0.0281,  ..., -0.0233,  0.0916, -0.1001]],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1110, -0.0834, -0.0150,  ..., -0.0712, -0.1239,  0.1232],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.8540, 0.8794, 0.9141,  ..., 0.9463, 0.8477, 0.8462],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0061, -0.0199, -0.0139,  ...,  0.0065, -0.0143, -0.0276],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0592, -0.1802,  0.2791,  ..., -0.0332, -0.0668,  0.0929],\n",
       "          [-0.0871,  0.0722, -0.0050,  ...,  0.0526, -0.0008,  0.0139],\n",
       "          [-0.0034,  0.0606,  0.0668,  ..., -0.0593, -0.0492,  0.0459],\n",
       "          ...,\n",
       "          [-0.0045,  0.1004,  0.0130,  ..., -0.0170, -0.0500,  0.1241],\n",
       "          [ 0.2820, -0.0569, -0.0671,  ...,  0.0187,  0.2166, -0.0954],\n",
       "          [ 0.0894,  0.0937, -0.0119,  ...,  0.0284, -0.1049, -0.0806]],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0319, -0.0469, -0.0313,  ..., -0.0519, -0.0406, -0.0654],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0132,  0.0565, -0.0864,  ...,  0.0556,  0.0912, -0.1476],\n",
       "          [-0.2712, -0.0145,  0.0780,  ...,  0.0743, -0.0623, -0.0726],\n",
       "          [ 0.0058,  0.1899, -0.0818,  ..., -0.0825,  0.2157, -0.1952],\n",
       "          ...,\n",
       "          [-0.0295, -0.1650,  0.0445,  ...,  0.1324,  0.0857, -0.1013],\n",
       "          [-0.1110, -0.0913,  0.0910,  ..., -0.0129,  0.1011,  0.0020],\n",
       "          [ 0.1990, -0.1783,  0.0797,  ..., -0.0309,  0.0426,  0.0812]],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1265, -0.0358, -0.0126,  ..., -0.0885, -0.1122,  0.1068],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0342, 1.0420, 1.0527,  ..., 1.0723, 0.9077, 1.0039],\n",
       "         requires_grad=True)),\n",
       " ('layers.21.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0422, -0.0709, -0.0025,  ...,  0.0182, -0.0121,  0.0307],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 9.7107e-02, -1.4758e-01, -4.4403e-02,  ..., -9.0637e-02,\n",
       "           -2.4994e-02,  6.7749e-02],\n",
       "          [-1.4124e-01,  7.2327e-02, -7.0862e-02,  ...,  5.3589e-02,\n",
       "            3.2135e-02, -1.0516e-01],\n",
       "          [-7.3975e-02, -3.4515e-02,  1.8787e-01,  ...,  5.4321e-02,\n",
       "           -2.1680e-01,  6.6650e-02],\n",
       "          ...,\n",
       "          [ 2.7527e-02,  3.5797e-02,  1.0529e-01,  ...,  7.8979e-02,\n",
       "           -1.3077e-02,  4.5441e-02],\n",
       "          [-1.0547e-01, -6.7383e-02,  4.5959e-02,  ..., -3.6407e-02,\n",
       "           -2.0703e-01,  4.0375e-02],\n",
       "          [-1.9727e-01,  1.1578e-01, -2.4445e-02,  ...,  1.7383e-01,\n",
       "            5.8472e-02,  1.4961e-04]], requires_grad=True)),\n",
       " ('layers.22.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1177,  0.0365,  0.0656,  ...,  0.0293, -0.0094, -0.0208],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0415,  0.0776, -0.0076,  ...,  0.1130,  0.0656,  0.1193],\n",
       "          [ 0.0196,  0.1245,  0.1103,  ...,  0.0749, -0.0826,  0.0415],\n",
       "          [ 0.0692, -0.0647,  0.1697,  ..., -0.0704,  0.0553,  0.1373],\n",
       "          ...,\n",
       "          [-0.1083,  0.0853, -0.1949,  ...,  0.0430,  0.0279, -0.0511],\n",
       "          [-0.0459, -0.0094,  0.0724,  ...,  0.0075,  0.0402, -0.0819],\n",
       "          [-0.1852,  0.0193, -0.0383,  ...,  0.0216, -0.0130, -0.0195]],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0145,  0.0092,  0.0042,  ...,  0.0094, -0.0373,  0.0051],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0076, -0.0003,  0.0775,  ..., -0.1881,  0.0065, -0.0908],\n",
       "          [-0.0559, -0.0302,  0.1331,  ...,  0.1298,  0.0183,  0.0213],\n",
       "          [-0.0762, -0.1171, -0.1747,  ...,  0.0252, -0.1886,  0.1580],\n",
       "          ...,\n",
       "          [-0.1333,  0.0166,  0.1896,  ...,  0.1099,  0.0659,  0.0786],\n",
       "          [-0.0412, -0.0079,  0.1274,  ..., -0.1427, -0.0462,  0.0216],\n",
       "          [-0.0119,  0.1202, -0.0410,  ...,  0.0241, -0.1501,  0.1804]],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0300,  0.1660,  0.0119,  ..., -0.0763,  0.1171,  0.0328],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0577,  0.1117, -0.1415,  ...,  0.0345, -0.0095,  0.2452],\n",
       "          [ 0.0255, -0.1527,  0.1183,  ..., -0.1090,  0.0807, -0.0778],\n",
       "          [-0.0228,  0.0558, -0.1194,  ...,  0.1692, -0.0282, -0.0130],\n",
       "          ...,\n",
       "          [ 0.0280, -0.0488, -0.0118,  ...,  0.0489, -0.0956, -0.1508],\n",
       "          [ 0.1035, -0.0107, -0.0432,  ..., -0.1107,  0.1229, -0.0069],\n",
       "          [ 0.0605,  0.0272, -0.1980,  ..., -0.0778, -0.0287,  0.0142]],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1232, -0.0339,  0.0020,  ..., -0.0893, -0.1041,  0.1194],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.8862, 0.9155, 0.9258,  ..., 0.9185, 0.8877, 0.8623],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0097, -0.0232,  0.0030,  ..., -0.0044, -0.0165, -0.0269],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0411,  0.0953, -0.0709,  ..., -0.0050, -0.0145, -0.1128],\n",
       "          [ 0.0305, -0.1115, -0.1231,  ..., -0.0417, -0.0576,  0.0279],\n",
       "          [ 0.0495,  0.1824, -0.0193,  ..., -0.0859,  0.0294,  0.0046],\n",
       "          ...,\n",
       "          [-0.0024, -0.0815,  0.1718,  ...,  0.0227,  0.0648, -0.1558],\n",
       "          [ 0.0027,  0.2993,  0.2402,  ..., -0.0151, -0.0411,  0.0803],\n",
       "          [ 0.0732,  0.0922,  0.1304,  ...,  0.1079, -0.0552,  0.0276]],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0345, -0.0578, -0.0439,  ..., -0.0657, -0.0631, -0.0366],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 4.1382e-02,  1.5671e-02, -6.7871e-02,  ..., -4.6692e-02,\n",
       "           -9.4788e-02, -2.2141e-02],\n",
       "          [ 1.3770e-01,  1.4355e-01,  1.6663e-02,  ..., -1.7593e-02,\n",
       "            3.3936e-02,  8.4167e-02],\n",
       "          [-1.8921e-01, -7.0984e-02,  1.0657e-01,  ...,  7.7393e-02,\n",
       "            7.6721e-02,  9.6375e-02],\n",
       "          ...,\n",
       "          [-1.8567e-01,  1.2863e-02, -6.8481e-02,  ...,  1.3623e-01,\n",
       "           -1.5222e-01,  1.4557e-02],\n",
       "          [ 2.9624e-05, -1.2488e-01,  2.2168e-01,  ..., -2.1881e-02,\n",
       "            1.4905e-01,  6.5491e-02],\n",
       "          [-5.2155e-02,  1.4984e-02, -2.9572e-02,  ...,  2.0691e-01,\n",
       "           -2.9495e-02, -1.4526e-01]], requires_grad=True)),\n",
       " ('layers.22.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1298, -0.0089,  0.0064,  ..., -0.1064, -0.1076,  0.1043],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0371, 1.0488, 1.0215,  ..., 1.0596, 0.9443, 1.0215],\n",
       "         requires_grad=True)),\n",
       " ('layers.22.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0297, -0.0789, -0.0095,  ..., -0.0169, -0.0400,  0.0676],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0244,  0.0905,  0.0466,  ...,  0.0132, -0.2854,  0.1000],\n",
       "          [-0.0192, -0.1252,  0.0872,  ..., -0.1322,  0.0483,  0.1270],\n",
       "          [-0.1235, -0.0578, -0.1261,  ..., -0.1208,  0.0320, -0.0220],\n",
       "          ...,\n",
       "          [-0.1917, -0.1307,  0.0406,  ..., -0.1105, -0.2844,  0.1534],\n",
       "          [-0.0889,  0.0809,  0.0149,  ..., -0.0723, -0.0069, -0.1622],\n",
       "          [ 0.1012, -0.0708,  0.1790,  ..., -0.2959,  0.1138, -0.1102]],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0408, -0.0424, -0.0419,  ..., -0.0094, -0.0020,  0.0291],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0344, -0.0652, -0.1247,  ...,  0.1532, -0.1488, -0.1523],\n",
       "          [ 0.1666,  0.1725, -0.0342,  ...,  0.0553, -0.0219,  0.0231],\n",
       "          [ 0.0259, -0.0908, -0.1721,  ...,  0.0738, -0.0539,  0.1494],\n",
       "          ...,\n",
       "          [-0.1143, -0.0539, -0.0065,  ...,  0.0745,  0.0646, -0.0626],\n",
       "          [-0.1229, -0.1004, -0.0226,  ..., -0.0478, -0.0560, -0.0562],\n",
       "          [-0.1553, -0.0524, -0.1222,  ..., -0.0910,  0.0108, -0.1299]],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0063,  0.0053, -0.0159,  ...,  0.0035, -0.0178, -0.0329],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0958,  0.0575,  0.2083,  ...,  0.0725,  0.0154,  0.1128],\n",
       "          [-0.1005, -0.1512,  0.1630,  ...,  0.0187,  0.0029, -0.0523],\n",
       "          [ 0.0952,  0.0171, -0.1443,  ...,  0.0344, -0.0341,  0.0953],\n",
       "          ...,\n",
       "          [ 0.0652,  0.0601,  0.0417,  ..., -0.1243,  0.0055,  0.1637],\n",
       "          [-0.0452,  0.0567, -0.0170,  ...,  0.0196, -0.0018, -0.0104],\n",
       "          [ 0.0370,  0.0205,  0.0566,  ..., -0.2363, -0.1125, -0.1577]],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0086, -0.3215, -0.0545,  ...,  0.2410, -0.0202,  0.0027],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0781, -0.1411,  0.0386,  ...,  0.0123,  0.2319, -0.0724],\n",
       "          [-0.1504,  0.0637, -0.1372,  ...,  0.0216,  0.0015,  0.0351],\n",
       "          [-0.1299, -0.1035,  0.0287,  ...,  0.0595,  0.0392,  0.1567],\n",
       "          ...,\n",
       "          [-0.0375,  0.0099,  0.1157,  ...,  0.0230, -0.0720, -0.0367],\n",
       "          [ 0.2520, -0.0224,  0.0671,  ..., -0.0411, -0.0530, -0.0309],\n",
       "          [ 0.0295, -0.0856, -0.1175,  ...,  0.0079, -0.0032, -0.0263]],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1305,  0.0041,  0.0228,  ..., -0.1047, -0.1014,  0.1078],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9302, 0.9067, 0.9346,  ..., 0.9204, 0.8550, 0.8428],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0196, -0.0246, -0.0033,  ..., -0.0129, -0.0096, -0.0247],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0584,  0.1185, -0.2021,  ..., -0.0045,  0.0485, -0.0397],\n",
       "          [-0.0111,  0.0641, -0.0354,  ...,  0.1337, -0.0674, -0.0706],\n",
       "          [ 0.0659, -0.0652, -0.0468,  ...,  0.1528, -0.2198,  0.0919],\n",
       "          ...,\n",
       "          [ 0.0814,  0.0851, -0.0760,  ...,  0.1301, -0.1980,  0.0941],\n",
       "          [-0.0167, -0.1442,  0.0216,  ...,  0.1392, -0.0424,  0.0473],\n",
       "          [-0.1443,  0.0392, -0.0634,  ..., -0.0312,  0.0619,  0.0523]],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0350, -0.0558,  0.0192,  ..., -0.0428, -0.0366, -0.0338],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 6.9380e-04,  5.7983e-03,  3.9581e-02,  ..., -9.2957e-02,\n",
       "           -1.4990e-01, -6.2744e-02],\n",
       "          [ 1.3831e-01,  2.4063e-02,  1.6418e-02,  ..., -1.0364e-01,\n",
       "            7.6965e-02, -2.4719e-02],\n",
       "          [ 9.5825e-02,  1.1987e-01, -4.5776e-02,  ..., -1.4404e-01,\n",
       "           -7.6843e-02,  4.9949e-05],\n",
       "          ...,\n",
       "          [ 2.5864e-02,  3.1891e-03,  7.5500e-02,  ...,  6.6895e-02,\n",
       "           -9.0088e-02,  1.6187e-01],\n",
       "          [ 3.4637e-02, -1.4795e-01,  2.3389e-01,  ...,  5.0659e-02,\n",
       "            6.0547e-02,  1.0364e-01],\n",
       "          [-2.1805e-02, -1.3196e-01,  1.6541e-02,  ...,  4.4891e-02,\n",
       "           -2.5391e-02, -3.5343e-03]], requires_grad=True)),\n",
       " ('layers.23.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1333,  0.0053,  0.0080,  ..., -0.1116, -0.1152,  0.0681],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0713, 1.0635, 1.0059,  ..., 1.0850, 0.9795, 1.0674],\n",
       "         requires_grad=True)),\n",
       " ('layers.23.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0025,  0.0004,  0.0142,  ...,  0.0026, -0.0127,  0.1202],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0055,  0.0377,  0.1171,  ..., -0.2007,  0.0722, -0.0895],\n",
       "          [ 0.0131,  0.1211, -0.0069,  ...,  0.0276, -0.0444, -0.0448],\n",
       "          [-0.1031,  0.0816, -0.0264,  ..., -0.0273, -0.0297,  0.1287],\n",
       "          ...,\n",
       "          [-0.0071,  0.0581, -0.0535,  ...,  0.1569,  0.1018,  0.0778],\n",
       "          [-0.0603, -0.1323, -0.2103,  ...,  0.0818,  0.0179, -0.0862],\n",
       "          [-0.0274,  0.0789,  0.1458,  ...,  0.0956,  0.0010,  0.2705]],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0451,  0.0305, -0.0167,  ...,  0.0002,  0.0558,  0.0279],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1583,  0.0461,  0.1475,  ..., -0.1671, -0.0841,  0.0513],\n",
       "          [-0.0331, -0.1259, -0.0030,  ..., -0.0049, -0.0069, -0.0206],\n",
       "          [ 0.0817, -0.0159,  0.0667,  ..., -0.0914, -0.0526, -0.0047],\n",
       "          ...,\n",
       "          [ 0.0104,  0.1073, -0.0368,  ..., -0.1647, -0.0037,  0.0549],\n",
       "          [-0.0257, -0.1019,  0.1462,  ...,  0.0046,  0.0432, -0.1725],\n",
       "          [ 0.0204,  0.0574,  0.0829,  ...,  0.0646, -0.0136, -0.1351]],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0059,  0.0001,  0.0457,  ..., -0.0108, -0.0059,  0.0078],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-1.5222e-01, -8.4106e-02,  9.4177e-02,  ..., -5.5847e-02,\n",
       "           -2.7161e-02,  7.7759e-02],\n",
       "          [ 8.4152e-03, -4.9011e-02,  2.2293e-02,  ...,  1.3550e-01,\n",
       "            8.1543e-02,  1.5186e-01],\n",
       "          [ 1.2833e-02, -9.4421e-02, -5.9845e-02,  ...,  1.8713e-01,\n",
       "           -1.0699e-04,  1.5540e-01],\n",
       "          ...,\n",
       "          [ 1.0400e-01,  6.0730e-02,  2.4097e-01,  ..., -6.0028e-02,\n",
       "            7.0457e-03, -1.4941e-01],\n",
       "          [-3.1555e-02, -1.0735e-02, -2.7161e-03,  ...,  1.5356e-01,\n",
       "           -1.1951e-01, -5.4779e-02],\n",
       "          [-6.6040e-02,  1.7651e-01,  7.8613e-02,  ..., -1.4832e-01,\n",
       "           -2.7908e-02, -2.7496e-02]], requires_grad=True)),\n",
       " ('layers.24.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1360,  0.0605,  0.1205,  ..., -0.0574,  0.1687,  0.1032],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0320,  0.0558,  0.0960,  ..., -0.0874, -0.0386, -0.0157],\n",
       "          [ 0.0377, -0.0251,  0.1610,  ..., -0.0332,  0.0388, -0.0820],\n",
       "          [ 0.0612, -0.0491, -0.0018,  ...,  0.0161, -0.0448,  0.0084],\n",
       "          ...,\n",
       "          [ 0.1569,  0.0829, -0.0148,  ...,  0.1848, -0.0044,  0.0997],\n",
       "          [ 0.0596,  0.0056,  0.0249,  ..., -0.0431, -0.0523,  0.1467],\n",
       "          [-0.0870,  0.0637,  0.0537,  ..., -0.0063,  0.0569,  0.0158]],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1284,  0.0089,  0.0220,  ..., -0.1165, -0.1125,  0.0923],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9189, 0.8911, 0.9341,  ..., 0.9019, 0.8501, 0.8848],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0052, -0.0275, -0.0193,  ..., -0.0178, -0.0234, -0.0194],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0781, -0.1893, -0.0711,  ..., -0.0853,  0.2117,  0.0366],\n",
       "          [-0.0542, -0.0403,  0.0912,  ..., -0.1205,  0.0958, -0.0406],\n",
       "          [ 0.0570,  0.1092, -0.0815,  ..., -0.1786,  0.0510,  0.1209],\n",
       "          ...,\n",
       "          [-0.0560, -0.1240,  0.1173,  ..., -0.0743, -0.1005, -0.0514],\n",
       "          [-0.1017,  0.0642,  0.1309,  ...,  0.0680, -0.0009, -0.0018],\n",
       "          [ 0.0031,  0.0488,  0.0751,  ...,  0.0359, -0.0922, -0.0611]],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0020, -0.0317, -0.0317,  ..., -0.0579, -0.0509, -0.0924],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0500, -0.0308, -0.0559,  ...,  0.1327,  0.2515, -0.0959],\n",
       "          [ 0.0282,  0.1367, -0.1705,  ..., -0.0391, -0.0525,  0.0469],\n",
       "          [-0.0938, -0.0806, -0.2369,  ...,  0.0732, -0.0958,  0.0771],\n",
       "          ...,\n",
       "          [ 0.0155,  0.0593, -0.0016,  ..., -0.0983, -0.0363,  0.0729],\n",
       "          [-0.1732,  0.1484, -0.2454,  ...,  0.1245,  0.0985,  0.0590],\n",
       "          [ 0.0834,  0.0342, -0.0972,  ...,  0.1405, -0.1174, -0.1115]],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1136,  0.0300,  0.0072,  ..., -0.1500, -0.1243,  0.0627],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0674, 1.0781, 1.0449,  ..., 1.0479, 1.0000, 1.0117],\n",
       "         requires_grad=True)),\n",
       " ('layers.24.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0654, -0.0153,  0.0221,  ...,  0.0362, -0.0461,  0.0677],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0470,  0.0223,  0.2389,  ..., -0.0435, -0.0714,  0.0110],\n",
       "          [ 0.0490,  0.0344, -0.1638,  ...,  0.0486, -0.0243,  0.1014],\n",
       "          [-0.0662, -0.0148,  0.0609,  ..., -0.0302, -0.0217, -0.0212],\n",
       "          ...,\n",
       "          [ 0.1300,  0.0671, -0.0623,  ...,  0.0740,  0.0274,  0.0058],\n",
       "          [-0.1142, -0.0924,  0.1104,  ..., -0.2380,  0.1066,  0.0252],\n",
       "          [ 0.0806, -0.0623,  0.1313,  ...,  0.1479, -0.1581,  0.0718]],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1914, -0.0783, -0.5347,  ..., -0.1525, -0.0257, -0.0913],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1177,  0.1045, -0.2217,  ..., -0.0647,  0.0102,  0.1545],\n",
       "          [-0.0261, -0.0023,  0.0882,  ..., -0.0257,  0.0178, -0.0947],\n",
       "          [-0.1888,  0.0849, -0.0870,  ...,  0.1409,  0.1013, -0.0308],\n",
       "          ...,\n",
       "          [-0.0599, -0.0765, -0.0607,  ...,  0.0203, -0.0491, -0.1176],\n",
       "          [ 0.0427, -0.1124,  0.0981,  ...,  0.0136,  0.1583,  0.0172],\n",
       "          [ 0.1052,  0.1179, -0.1135,  ..., -0.1131,  0.0089,  0.0668]],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0399, -0.0150, -0.0254,  ...,  0.0223, -0.0519, -0.0295],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1024,  0.0102,  0.1078,  ..., -0.0016,  0.0098, -0.0687],\n",
       "          [ 0.1094,  0.0302, -0.1265,  ...,  0.0952, -0.0014, -0.0964],\n",
       "          [-0.0089, -0.1081,  0.1199,  ..., -0.0925, -0.0413, -0.1257],\n",
       "          ...,\n",
       "          [ 0.2534,  0.0140,  0.0515,  ..., -0.1366, -0.1505,  0.0419],\n",
       "          [-0.0970, -0.0772,  0.0416,  ..., -0.1510,  0.0074, -0.2805],\n",
       "          [ 0.0051, -0.2289,  0.0467,  ...,  0.1355,  0.0876,  0.1681]],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1737, -0.1501, -0.3892,  ..., -0.1812, -0.0201, -0.0167],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0414, -0.0377, -0.0875,  ..., -0.0113,  0.0207,  0.0321],\n",
       "          [ 0.0677,  0.1941,  0.0437,  ...,  0.0779,  0.0257, -0.0308],\n",
       "          [-0.0471,  0.0813, -0.0424,  ..., -0.0906,  0.0645,  0.0475],\n",
       "          ...,\n",
       "          [ 0.1268, -0.0222,  0.0106,  ...,  0.0898, -0.0184,  0.1017],\n",
       "          [-0.0081, -0.0855, -0.0095,  ...,  0.0267, -0.0400, -0.0497],\n",
       "          [ 0.1094, -0.0567, -0.0303,  ...,  0.1879,  0.0123, -0.0747]],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1104,  0.0517,  0.0109,  ..., -0.1143, -0.1002,  0.0893],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9395, 0.8945, 0.9644,  ..., 0.9019, 0.9170, 0.8765],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0091, -0.0129, -0.0146,  ..., -0.0321,  0.0026, -0.0010],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0376,  0.2085,  0.0639,  ...,  0.0770,  0.0999, -0.2280],\n",
       "          [-0.1161, -0.0888,  0.1323,  ...,  0.0232, -0.0450, -0.0042],\n",
       "          [-0.1537,  0.1277,  0.0182,  ..., -0.0546,  0.0975,  0.0048],\n",
       "          ...,\n",
       "          [-0.2271,  0.0878, -0.0019,  ...,  0.0971,  0.2136, -0.0956],\n",
       "          [-0.0963, -0.1232, -0.0342,  ...,  0.0884, -0.0892,  0.0102],\n",
       "          [-0.2893,  0.2236,  0.1742,  ..., -0.1853, -0.0260, -0.0408]],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0212, -0.1016, -0.0493,  ..., -0.0604, -0.0321, -0.0707],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0038, -0.0073, -0.0602,  ..., -0.0485,  0.2236,  0.0549],\n",
       "          [-0.0466,  0.0877, -0.2174,  ..., -0.0297,  0.2130,  0.0446],\n",
       "          [-0.0451, -0.0422, -0.1415,  ..., -0.1669, -0.0719, -0.0060],\n",
       "          ...,\n",
       "          [ 0.0355,  0.0096, -0.0238,  ..., -0.0156, -0.0557, -0.0067],\n",
       "          [-0.1459,  0.1483, -0.0070,  ...,  0.0652,  0.0534,  0.0529],\n",
       "          [-0.0580, -0.0995,  0.1327,  ..., -0.0635,  0.2500, -0.0182]],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1102,  0.0740,  0.0125,  ..., -0.1191, -0.1036,  0.0776],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0635, 1.0557, 1.0420,  ..., 1.0439, 1.0303, 1.0166],\n",
       "         requires_grad=True)),\n",
       " ('layers.25.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0145, -0.0074,  0.0132,  ...,  0.0133, -0.0883,  0.0499],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 1.1890e-01, -5.6915e-02,  7.1838e-02,  ...,  1.1078e-01,\n",
       "           -8.2703e-03,  5.6915e-02],\n",
       "          [-7.3364e-02,  1.9580e-01, -6.0844e-04,  ..., -8.0811e-02,\n",
       "           -2.2202e-02, -5.0240e-03],\n",
       "          [ 4.7455e-02, -6.2622e-02,  7.3120e-02,  ..., -6.3538e-02,\n",
       "            3.2837e-02, -1.1823e-01],\n",
       "          ...,\n",
       "          [-1.3428e-01, -1.3489e-01,  1.8646e-02,  ...,  2.5375e-02,\n",
       "           -7.4646e-02,  2.1338e-01],\n",
       "          [ 3.3051e-02, -1.5454e-01,  4.6143e-02,  ...,  1.1659e-04,\n",
       "           -4.4800e-02,  9.7656e-02],\n",
       "          [-9.7473e-02, -2.1133e-02, -1.3257e-01,  ..., -3.4363e-02,\n",
       "            6.0760e-02,  1.8176e-01]], requires_grad=True)),\n",
       " ('layers.26.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0399,  0.0304,  0.0552,  ...,  0.2324, -0.1445,  0.1032],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0147, -0.2019, -0.0170,  ...,  0.2217, -0.1008,  0.0321],\n",
       "          [-0.0483, -0.0148, -0.1431,  ..., -0.0787, -0.1914, -0.0824],\n",
       "          [-0.0272, -0.0858, -0.0045,  ..., -0.0017, -0.0365, -0.0029],\n",
       "          ...,\n",
       "          [-0.1006, -0.0152,  0.1846,  ..., -0.1111,  0.0378,  0.0201],\n",
       "          [-0.1292, -0.1671,  0.1246,  ..., -0.0709, -0.1865, -0.0100],\n",
       "          [ 0.1240,  0.0966,  0.0313,  ...,  0.0966,  0.0942,  0.1031]],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0115, -0.0525,  0.0174,  ..., -0.0077, -0.0159, -0.0177],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0493,  0.1511,  0.1384,  ...,  0.1122,  0.1322, -0.0679],\n",
       "          [-0.0235,  0.1533,  0.0545,  ...,  0.1212,  0.0916, -0.0155],\n",
       "          [-0.0492, -0.0559,  0.1045,  ...,  0.1676,  0.0360,  0.0162],\n",
       "          ...,\n",
       "          [-0.1544, -0.0624, -0.1615,  ...,  0.0438, -0.1179,  0.0958],\n",
       "          [ 0.0311,  0.0204, -0.0338,  ..., -0.0033,  0.1323,  0.0514],\n",
       "          [-0.1698,  0.0273, -0.1774,  ...,  0.0353,  0.0276,  0.0734]],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0318, -0.0103,  0.2915,  ...,  0.1180,  0.0368,  0.1092],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0937,  0.1290,  0.0236,  ...,  0.0689,  0.0375, -0.0988],\n",
       "          [ 0.0835,  0.0204,  0.0468,  ...,  0.0009,  0.0693, -0.0516],\n",
       "          [ 0.0710,  0.1904,  0.1338,  ..., -0.0981,  0.1992, -0.1191],\n",
       "          ...,\n",
       "          [-0.1283,  0.1293,  0.0128,  ...,  0.0214,  0.1713,  0.0283],\n",
       "          [-0.2456,  0.1711, -0.0229,  ..., -0.0852,  0.2966, -0.0448],\n",
       "          [ 0.1217, -0.0779,  0.0068,  ...,  0.0323, -0.1062, -0.0581]],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1011,  0.0956, -0.0050,  ..., -0.1053, -0.0945,  0.1049],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9468, 0.9429, 0.9604,  ..., 0.9287, 0.9023, 0.8672],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0021, -0.0496, -0.0055,  ..., -0.0230, -0.0102, -0.0179],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1188,  0.0662, -0.1198,  ...,  0.0643, -0.0447, -0.0638],\n",
       "          [ 0.1155,  0.0641,  0.1436,  ...,  0.0679, -0.0441, -0.1698],\n",
       "          [ 0.0248, -0.1747, -0.1399,  ...,  0.0547,  0.1119, -0.1211],\n",
       "          ...,\n",
       "          [-0.1015,  0.1047, -0.0399,  ..., -0.0044,  0.0193,  0.1200],\n",
       "          [-0.0143, -0.1246,  0.2495,  ...,  0.0406, -0.1913,  0.1346],\n",
       "          [ 0.1539,  0.0303,  0.0961,  ..., -0.0693, -0.0842, -0.0542]],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0310, -0.0777,  0.0337,  ..., -0.0794, -0.0795, -0.0625],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1212,  0.0706, -0.1772,  ...,  0.1699,  0.0709, -0.3625],\n",
       "          [ 0.0549, -0.3081, -0.0285,  ..., -0.0778,  0.0747,  0.0791],\n",
       "          [-0.1049,  0.1466,  0.0682,  ..., -0.1381,  0.1422, -0.0230],\n",
       "          ...,\n",
       "          [-0.0855, -0.0845,  0.0908,  ...,  0.0125,  0.0445,  0.0364],\n",
       "          [ 0.0862,  0.0587, -0.2013,  ...,  0.3105,  0.0144,  0.0718],\n",
       "          [ 0.0124,  0.0137, -0.0878,  ...,  0.0125,  0.0922,  0.0346]],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0939,  0.1089, -0.0132,  ..., -0.1322, -0.1026,  0.1164],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.1240, 1.0908, 1.0615,  ..., 1.0400, 1.0273, 0.9839],\n",
       "         requires_grad=True)),\n",
       " ('layers.26.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0126,  0.0269,  0.0054,  ...,  0.0102, -0.0898,  0.0201],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1575, -0.1104, -0.1874,  ..., -0.0746,  0.0245, -0.1992],\n",
       "          [ 0.1055, -0.1040,  0.1549,  ..., -0.1091, -0.1680, -0.1313],\n",
       "          [ 0.0600, -0.0084,  0.0031,  ..., -0.0111,  0.1857, -0.1171],\n",
       "          ...,\n",
       "          [ 0.3181,  0.2312, -0.0137,  ..., -0.0195,  0.1255, -0.2244],\n",
       "          [-0.0945,  0.0637,  0.0731,  ...,  0.0743,  0.0200, -0.0234],\n",
       "          [ 0.0010, -0.0107, -0.1418,  ...,  0.1061,  0.0504,  0.0119]],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0207, -0.0083, -0.0012,  ..., -0.0381, -0.0181, -0.0848],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0966,  0.0532,  0.0250,  ...,  0.0117,  0.0979, -0.0149],\n",
       "          [ 0.0989,  0.2419, -0.0703,  ...,  0.0413,  0.1633, -0.0304],\n",
       "          [-0.0502, -0.0537, -0.0657,  ...,  0.0662, -0.0265,  0.0071],\n",
       "          ...,\n",
       "          [-0.0266,  0.1758, -0.1534,  ..., -0.0059,  0.0557,  0.0456],\n",
       "          [ 0.0950, -0.1204,  0.0302,  ...,  0.0038, -0.1443, -0.0461],\n",
       "          [-0.0589, -0.0012, -0.1860,  ...,  0.1803,  0.0086, -0.0228]],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0292,  0.0214, -0.0296,  ..., -0.0042,  0.0288,  0.0113],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0860,  0.0016, -0.1870,  ...,  0.0819, -0.0371,  0.1277],\n",
       "          [ 0.0605,  0.1351,  0.0929,  ..., -0.2039, -0.0386,  0.1537],\n",
       "          [-0.1409, -0.0925, -0.1622,  ...,  0.0264,  0.0461,  0.0371],\n",
       "          ...,\n",
       "          [ 0.0442, -0.0360,  0.1014,  ...,  0.1248,  0.0136, -0.0218],\n",
       "          [ 0.2622, -0.0540,  0.1702,  ...,  0.0321,  0.0985,  0.0258],\n",
       "          [ 0.0346, -0.0706, -0.1810,  ..., -0.0288,  0.0617, -0.0198]],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0972,  0.2013,  0.1819,  ...,  0.0972,  0.0638, -0.1040],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0940, -0.0369,  0.1225,  ..., -0.0395, -0.1218,  0.0268],\n",
       "          [-0.1927,  0.0283, -0.0739,  ..., -0.1316,  0.1178,  0.0461],\n",
       "          [-0.0201,  0.0452, -0.0785,  ...,  0.1229,  0.0353,  0.1212],\n",
       "          ...,\n",
       "          [-0.0916,  0.0668, -0.1664,  ..., -0.0674, -0.0241, -0.0463],\n",
       "          [-0.0384, -0.1620,  0.0228,  ..., -0.0482,  0.1201, -0.0321],\n",
       "          [ 0.0406,  0.0124,  0.0718,  ...,  0.0133,  0.0015,  0.1257]],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0897,  0.1365, -0.0196,  ..., -0.0906, -0.1115,  0.1456],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9487, 0.8975, 0.9478,  ..., 0.9609, 0.9126, 0.8174],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0048, -0.0504, -0.0072,  ..., -0.0449,  0.0045, -0.0166],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0317, -0.2054,  0.1180,  ..., -0.0408,  0.2070, -0.0583],\n",
       "          [-0.0664,  0.0434, -0.0224,  ..., -0.0699,  0.0036, -0.0782],\n",
       "          [-0.1212,  0.0953,  0.0388,  ...,  0.0737,  0.0923, -0.0790],\n",
       "          ...,\n",
       "          [-0.1854,  0.0759,  0.1333,  ...,  0.0891,  0.2131, -0.2463],\n",
       "          [ 0.0575,  0.0731,  0.0195,  ..., -0.2101, -0.0064,  0.0091],\n",
       "          [ 0.0225,  0.0400, -0.1019,  ..., -0.1522,  0.1920, -0.0478]],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0527, -0.0416, -0.0448,  ..., -0.0872, -0.0623, -0.0796],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0210, -0.0465, -0.0637,  ..., -0.1130,  0.0551, -0.1681],\n",
       "          [ 0.1036, -0.0023, -0.2791,  ...,  0.0762, -0.1809, -0.1760],\n",
       "          [ 0.0506, -0.0806,  0.1107,  ...,  0.1466,  0.1846,  0.0516],\n",
       "          ...,\n",
       "          [ 0.0186,  0.0428, -0.0231,  ...,  0.0546, -0.2333, -0.1433],\n",
       "          [-0.0503, -0.0838,  0.0524,  ..., -0.1908, -0.0196, -0.0880],\n",
       "          [ 0.1958,  0.0811, -0.0971,  ..., -0.0309, -0.2332,  0.0131]],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0704,  0.1165, -0.0263,  ..., -0.0865, -0.1239,  0.1439],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.1055, 1.1582, 1.0674,  ..., 1.0791, 1.0625, 0.9624],\n",
       "         requires_grad=True)),\n",
       " ('layers.27.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0186,  0.0343,  0.0082,  ..., -0.0156, -0.0963, -0.0186],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 1.1269e-02,  3.4851e-02, -5.6244e-02,  ...,  3.1082e-02,\n",
       "           -4.6326e-02, -5.8784e-03],\n",
       "          [ 2.3666e-02,  2.1194e-02,  5.1270e-02,  ...,  5.1697e-02,\n",
       "            7.9041e-02,  1.3635e-01],\n",
       "          [-1.1597e-01, -4.0741e-03, -7.8735e-02,  ..., -5.5511e-02,\n",
       "            1.6510e-02, -2.0239e-01],\n",
       "          ...,\n",
       "          [ 4.7028e-02,  1.2276e-02,  3.5431e-02,  ...,  2.7481e-02,\n",
       "           -7.9712e-02, -3.7903e-02],\n",
       "          [-1.7214e-03,  4.0344e-02, -9.6436e-02,  ..., -1.5063e-01,\n",
       "            3.0640e-01, -2.9039e-04],\n",
       "          [-6.4148e-02,  1.0260e-01, -3.2928e-02,  ..., -8.6823e-03,\n",
       "            8.5266e-02, -1.6309e-01]], requires_grad=True)),\n",
       " ('layers.28.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.1007,  0.0243, -0.0912,  ..., -0.0229,  0.0259,  0.0126],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0134,  0.0992,  0.1249,  ...,  0.1204,  0.0183, -0.0113],\n",
       "          [ 0.0745,  0.1118,  0.1675,  ..., -0.1221, -0.0062,  0.0747],\n",
       "          [ 0.0663,  0.0316, -0.0128,  ..., -0.0033,  0.0032, -0.0164],\n",
       "          ...,\n",
       "          [-0.0876, -0.0756,  0.1426,  ..., -0.0243, -0.0665,  0.0049],\n",
       "          [ 0.0432,  0.0861, -0.0515,  ..., -0.0271, -0.0506,  0.1285],\n",
       "          [-0.0212, -0.0989, -0.0852,  ...,  0.1012,  0.1058,  0.0440]],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0332,  0.0116, -0.0015,  ...,  0.0006, -0.0690, -0.0482],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1987,  0.0428, -0.1510,  ..., -0.0419, -0.1809, -0.0833],\n",
       "          [ 0.1699,  0.1472, -0.0743,  ..., -0.0601, -0.0619, -0.0986],\n",
       "          [-0.0278, -0.1048,  0.0129,  ..., -0.0343, -0.0541, -0.0886],\n",
       "          ...,\n",
       "          [ 0.0186,  0.0903,  0.0265,  ..., -0.0591, -0.0825,  0.0005],\n",
       "          [-0.1619,  0.0320, -0.1731,  ...,  0.0575, -0.0098,  0.1199],\n",
       "          [ 0.0469, -0.0992,  0.0958,  ...,  0.0591, -0.0251,  0.0226]],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0577,  0.0759, -0.0825,  ...,  0.0021, -0.0992, -0.0610],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0921, -0.0393,  0.0009,  ...,  0.2229, -0.0768,  0.1349],\n",
       "          [-0.1796,  0.0888,  0.1278,  ..., -0.0586, -0.0456, -0.1196],\n",
       "          [ 0.0007, -0.0543,  0.1125,  ..., -0.0089,  0.1009,  0.1321],\n",
       "          ...,\n",
       "          [-0.1022,  0.0446, -0.1200,  ...,  0.0130, -0.0554, -0.0598],\n",
       "          [ 0.0412, -0.0995,  0.1245,  ...,  0.0005,  0.1265,  0.0029],\n",
       "          [-0.0966,  0.0237, -0.0715,  ...,  0.0477, -0.0950, -0.0400]],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0328,  0.1339, -0.0332,  ..., -0.0541, -0.1379,  0.1292],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9458, 0.8652, 0.9438,  ..., 0.9326, 0.8994, 0.8145],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0180, -0.0327, -0.0150,  ..., -0.0282, -0.0259, -0.0081],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0754, -0.0651,  0.0657,  ..., -0.0220, -0.0044,  0.0089],\n",
       "          [ 0.1144,  0.1351,  0.1475,  ...,  0.0169,  0.2416, -0.1476],\n",
       "          [ 0.0818,  0.1782, -0.0594,  ..., -0.0430,  0.0293,  0.0571],\n",
       "          ...,\n",
       "          [-0.0746, -0.0013,  0.1179,  ..., -0.0110,  0.0154,  0.0306],\n",
       "          [-0.0907,  0.0655,  0.0992,  ...,  0.0890, -0.0102,  0.0253],\n",
       "          [ 0.0034, -0.0485,  0.0363,  ..., -0.1243,  0.0394, -0.0704]],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0347, -0.0748, -0.0641,  ..., -0.0410, -0.0645, -0.0100],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1498, -0.0316,  0.1482,  ...,  0.0479,  0.0526, -0.0008],\n",
       "          [ 0.0345,  0.3242, -0.1893,  ...,  0.1738, -0.0693, -0.0906],\n",
       "          [-0.2067,  0.0647,  0.0662,  ...,  0.0521, -0.0005,  0.0190],\n",
       "          ...,\n",
       "          [ 0.1835,  0.1620,  0.0534,  ..., -0.0376, -0.0142,  0.1343],\n",
       "          [-0.0169,  0.2512, -0.0499,  ..., -0.0107, -0.1004,  0.1527],\n",
       "          [ 0.0363, -0.1310, -0.0663,  ..., -0.0708,  0.1189,  0.0684]],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0329,  0.1437, -0.0253,  ..., -0.0527, -0.1475,  0.1203],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.1260, 1.1553, 1.1240,  ..., 1.0645, 1.0781, 0.9448],\n",
       "         requires_grad=True)),\n",
       " ('layers.28.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0290,  0.0026, -0.0347,  ..., -0.0372, -0.0996, -0.0299],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0112,  0.0325,  0.2041,  ...,  0.1279,  0.0256,  0.2235],\n",
       "          [ 0.0141,  0.1470, -0.0957,  ...,  0.1295, -0.1544, -0.0614],\n",
       "          [ 0.0934,  0.0242,  0.0289,  ..., -0.1183,  0.0079,  0.1393],\n",
       "          ...,\n",
       "          [-0.0253, -0.0793, -0.1580,  ...,  0.0579, -0.1782,  0.0790],\n",
       "          [-0.0664, -0.0172,  0.0293,  ...,  0.0122, -0.0309, -0.0349],\n",
       "          [-0.2452, -0.0679, -0.2281,  ...,  0.0273, -0.1043, -0.0056]],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0751,  0.1353,  0.1510,  ...,  0.0514, -0.0419,  0.0701],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0632,  0.1685,  0.1841,  ..., -0.2018,  0.0448, -0.0778],\n",
       "          [ 0.0335,  0.0042,  0.0219,  ...,  0.1934, -0.0278,  0.1694],\n",
       "          [ 0.0232, -0.1158,  0.1882,  ...,  0.1152,  0.0423, -0.1746],\n",
       "          ...,\n",
       "          [ 0.1417,  0.0376,  0.1300,  ..., -0.1876,  0.0793, -0.0341],\n",
       "          [-0.1016, -0.1254,  0.0066,  ...,  0.0114, -0.0622, -0.0328],\n",
       "          [-0.0610, -0.0952, -0.1637,  ...,  0.0482, -0.0500, -0.2021]],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0275,  0.0114, -0.0166,  ...,  0.0175,  0.0094, -0.0547],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1431, -0.0097,  0.0198,  ...,  0.1205,  0.0994,  0.0191],\n",
       "          [-0.1185,  0.0958, -0.0959,  ...,  0.0540,  0.0264, -0.0278],\n",
       "          [-0.0400, -0.0528, -0.0266,  ..., -0.0531, -0.0720,  0.0709],\n",
       "          ...,\n",
       "          [ 0.0635, -0.0037, -0.1267,  ...,  0.0679, -0.0266, -0.0437],\n",
       "          [-0.0213, -0.0823,  0.0098,  ...,  0.1639, -0.0545,  0.0799],\n",
       "          [-0.1909, -0.0341, -0.2074,  ...,  0.0558, -0.0466, -0.2021]],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1185,  0.1138,  0.3816,  ...,  0.1226, -0.0011,  0.0091],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0563,  0.0329, -0.1207,  ..., -0.0903, -0.0096,  0.0307],\n",
       "          [-0.2340, -0.0374,  0.0023,  ...,  0.0198,  0.0900,  0.1561],\n",
       "          [-0.0994, -0.1342,  0.0040,  ..., -0.1469, -0.0372,  0.0076],\n",
       "          ...,\n",
       "          [ 0.1371, -0.1151, -0.1126,  ...,  0.1318,  0.0148, -0.0327],\n",
       "          [-0.0132,  0.0523,  0.0190,  ..., -0.0514, -0.0375, -0.0236],\n",
       "          [ 0.0653, -0.1232,  0.1562,  ...,  0.0153,  0.0158,  0.1470]],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0271,  0.1548, -0.0192,  ..., -0.0254, -0.1486,  0.1385],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.9390, 0.9141, 0.9214,  ..., 0.9272, 0.8706, 0.8213],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0135, -0.0659,  0.0063,  ..., -0.0200,  0.0183, -0.0430],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0255, -0.0695, -0.1738,  ..., -0.0696,  0.0615, -0.0190],\n",
       "          [ 0.0908,  0.0273, -0.0190,  ...,  0.0197,  0.0845, -0.1069],\n",
       "          [ 0.1033,  0.0800,  0.0925,  ...,  0.0090,  0.1628, -0.1190],\n",
       "          ...,\n",
       "          [ 0.0116, -0.0123, -0.0417,  ..., -0.0428,  0.0755, -0.1969],\n",
       "          [ 0.0362,  0.0271, -0.0075,  ..., -0.1176, -0.1016, -0.1370],\n",
       "          [-0.0288,  0.0339,  0.1511,  ..., -0.1302,  0.1263,  0.1749]],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0467, -0.0727, -0.0016,  ..., -0.0873, -0.0254, -0.0141],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0550, -0.0299, -0.0160,  ..., -0.1260,  0.0645, -0.1074],\n",
       "          [ 0.2209, -0.1183, -0.0933,  ...,  0.0665,  0.0473,  0.1495],\n",
       "          [-0.1262,  0.1209,  0.0167,  ...,  0.0951,  0.0339, -0.1530],\n",
       "          ...,\n",
       "          [ 0.0890,  0.0327,  0.0987,  ...,  0.0184,  0.0781,  0.1310],\n",
       "          [-0.0659, -0.0157,  0.0623,  ...,  0.0587, -0.0497, -0.1516],\n",
       "          [ 0.1140, -0.1370, -0.0162,  ..., -0.2183,  0.0760, -0.2849]],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0126,  0.1469, -0.0103,  ..., -0.0443, -0.1464,  0.1322],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.1406, 1.1914, 1.1152,  ..., 1.1201, 1.1338, 0.9878],\n",
       "         requires_grad=True)),\n",
       " ('layers.29.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0407, -0.0097, -0.0382,  ..., -0.0295, -0.1177, -0.0489],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1412, -0.0122,  0.0271,  ...,  0.0044, -0.1004, -0.1208],\n",
       "          [ 0.1630, -0.0885, -0.0108,  ...,  0.0179,  0.1142,  0.0457],\n",
       "          [-0.1589, -0.0127, -0.0226,  ..., -0.2593, -0.0078, -0.0786],\n",
       "          ...,\n",
       "          [-0.0127, -0.0931,  0.0596,  ..., -0.0359,  0.0771, -0.0027],\n",
       "          [-0.0426,  0.0202,  0.1187,  ...,  0.1316,  0.0708,  0.0430],\n",
       "          [ 0.1475,  0.1522,  0.0456,  ...,  0.2534, -0.0676, -0.0604]],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0660,  0.2119, -0.0240,  ...,  0.1427, -0.0750, -0.0209],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0624,  0.0520,  0.0129,  ...,  0.1169, -0.0065,  0.0364],\n",
       "          [-0.0219, -0.0810,  0.0911,  ..., -0.0306, -0.0106, -0.1433],\n",
       "          [ 0.0695, -0.0540, -0.1353,  ..., -0.1118,  0.2358,  0.0125],\n",
       "          ...,\n",
       "          [ 0.1768, -0.0900,  0.0148,  ..., -0.0249,  0.1218, -0.0312],\n",
       "          [-0.0742, -0.1018, -0.0360,  ..., -0.1044, -0.0616,  0.1399],\n",
       "          [ 0.1707,  0.0189,  0.0876,  ..., -0.0452,  0.0814,  0.1061]],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0272, -0.0047,  0.0392,  ...,  0.0109,  0.0223,  0.0055],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0640,  0.1453,  0.1133,  ...,  0.0477,  0.0637, -0.0859],\n",
       "          [ 0.1237,  0.0394, -0.0850,  ..., -0.0446,  0.1187, -0.0256],\n",
       "          [-0.0942, -0.1431,  0.0436,  ..., -0.2100, -0.0831, -0.0184],\n",
       "          ...,\n",
       "          [ 0.0629,  0.0484,  0.0284,  ...,  0.0823, -0.0211,  0.0149],\n",
       "          [ 0.0243, -0.0321,  0.0593,  ...,  0.0155,  0.0316,  0.0727],\n",
       "          [ 0.0738,  0.0304,  0.0216,  ...,  0.0967, -0.0372,  0.0286]],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0264,  0.0971, -0.0595,  ..., -0.0450, -0.0136,  0.0586],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0441,  0.0092, -0.1110,  ...,  0.0688,  0.0546,  0.0042],\n",
       "          [-0.0684,  0.0180,  0.0690,  ...,  0.2443,  0.0202, -0.0627],\n",
       "          [ 0.0912, -0.0479, -0.0330,  ...,  0.0111, -0.0771, -0.0960],\n",
       "          ...,\n",
       "          [ 0.0602, -0.0146, -0.0159,  ...,  0.0425,  0.0533, -0.0646],\n",
       "          [-0.0404, -0.2294, -0.2766,  ...,  0.0268,  0.0840,  0.0742],\n",
       "          [ 0.0960,  0.0041, -0.0054,  ...,  0.0967, -0.0370,  0.0106]],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0413,  0.1848, -0.0377,  ..., -0.0240, -0.1490,  0.1270],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.8550, 0.8286, 0.8828,  ..., 0.8979, 0.8735, 0.8091],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0134, -0.0303,  0.0020,  ..., -0.0335,  0.0328, -0.0222],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1199,  0.1934, -0.0787,  ...,  0.0254, -0.1217,  0.1008],\n",
       "          [-0.1465, -0.1643,  0.1072,  ...,  0.0063,  0.0627, -0.0921],\n",
       "          [-0.1097, -0.0153, -0.0725,  ..., -0.0490, -0.0434,  0.0547],\n",
       "          ...,\n",
       "          [ 0.0851, -0.0063, -0.0645,  ...,  0.1656,  0.0230,  0.0529],\n",
       "          [-0.0009, -0.0448, -0.1152,  ..., -0.0718, -0.0380, -0.0205],\n",
       "          [-0.1019,  0.0640, -0.0545,  ..., -0.1324,  0.0875,  0.0856]],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0324, -0.0307, -0.0662,  ...,  0.0014, -0.0319, -0.1074],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0124, -0.1300,  0.0066,  ..., -0.1003, -0.0337,  0.0268],\n",
       "          [-0.1617, -0.1479, -0.0627,  ..., -0.0977, -0.0562, -0.0095],\n",
       "          [ 0.0430, -0.0410,  0.0880,  ..., -0.0426, -0.0540, -0.1053],\n",
       "          ...,\n",
       "          [ 0.1636,  0.1190, -0.1815,  ...,  0.2192,  0.0036, -0.0548],\n",
       "          [-0.0463,  0.0839, -0.0565,  ..., -0.2384, -0.0138,  0.0617],\n",
       "          [ 0.0554,  0.0430,  0.1722,  ...,  0.1475,  0.0819,  0.0040]],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0246,  0.1813,  0.0042,  ..., -0.0285, -0.1421,  0.0844],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.1914, 1.1855, 1.1797,  ..., 1.1709, 1.1865, 0.9868],\n",
       "         requires_grad=True)),\n",
       " ('layers.30.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0640, -0.0493, -0.0806,  ..., -0.0096, -0.1345, -0.0622],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0661, -0.0268,  0.0216,  ..., -0.0076, -0.1281, -0.0162],\n",
       "          [ 0.0848, -0.1052, -0.0245,  ..., -0.0171,  0.0239, -0.1093],\n",
       "          [ 0.0885,  0.0470,  0.0078,  ...,  0.1382, -0.0004,  0.0513],\n",
       "          ...,\n",
       "          [-0.0683, -0.0123,  0.0030,  ...,  0.0911, -0.0540, -0.0695],\n",
       "          [ 0.1235,  0.2207,  0.0655,  ..., -0.0029,  0.1165, -0.1069],\n",
       "          [ 0.0901,  0.1124, -0.1539,  ..., -0.0379, -0.1179,  0.1561]],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0045,  0.0058, -0.0602,  ...,  0.0869,  0.0500,  0.1616],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0066,  0.1588, -0.1260,  ...,  0.1087, -0.0338,  0.0443],\n",
       "          [ 0.2144,  0.0236,  0.0323,  ...,  0.0803, -0.2268,  0.1082],\n",
       "          [-0.3191, -0.1059, -0.0719,  ...,  0.1733,  0.0620,  0.1160],\n",
       "          ...,\n",
       "          [-0.0065, -0.0309,  0.0607,  ...,  0.1477, -0.0232,  0.0591],\n",
       "          [ 0.0940,  0.1194,  0.0393,  ...,  0.0389, -0.1018, -0.1171],\n",
       "          [ 0.0785, -0.2479, -0.0618,  ..., -0.0263,  0.1039,  0.2006]],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0172,  0.0790, -0.0355,  ...,  0.0088,  0.0445,  0.1006],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0834,  0.2255, -0.1630,  ..., -0.0923,  0.0221,  0.0774],\n",
       "          [-0.0202, -0.0829,  0.0490,  ..., -0.0097, -0.0707, -0.1731],\n",
       "          [ 0.1321,  0.0917,  0.0375,  ..., -0.0396, -0.0784,  0.0546],\n",
       "          ...,\n",
       "          [-0.0018, -0.0215,  0.0617,  ..., -0.0371, -0.2362,  0.0021],\n",
       "          [ 0.0249, -0.0454, -0.0872,  ...,  0.1205,  0.1892, -0.1146],\n",
       "          [ 0.0646,  0.1461,  0.0349,  ..., -0.1143, -0.1179, -0.0097]],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0867, -0.0652, -0.0506,  ..., -0.0456,  0.0408,  0.0269],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1454, -0.1901, -0.1332,  ...,  0.1648, -0.2141,  0.0064],\n",
       "          [-0.1154,  0.2311, -0.0703,  ...,  0.0027, -0.1194,  0.2035],\n",
       "          [-0.0786, -0.1045, -0.0600,  ..., -0.0854, -0.0424,  0.1272],\n",
       "          ...,\n",
       "          [ 0.0066, -0.0353,  0.1298,  ..., -0.1954, -0.0333,  0.0197],\n",
       "          [ 0.0514,  0.0524,  0.0967,  ..., -0.0076,  0.0764,  0.2186],\n",
       "          [-0.1497,  0.0567,  0.1337,  ...,  0.0442,  0.2076, -0.1359]],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0217,  0.2006, -0.0208,  ...,  0.0002, -0.1537,  0.0931],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.8608, 0.8354, 0.8389,  ..., 0.8574, 0.8149, 0.7808],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0352, -0.0232,  0.0105,  ...,  0.0111,  0.0416, -0.0195],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0386,  0.0311,  0.1101,  ..., -0.1094, -0.0123, -0.1699],\n",
       "          [-0.2839,  0.1292, -0.0452,  ...,  0.1644,  0.0512, -0.0262],\n",
       "          [ 0.0568, -0.0522, -0.0964,  ...,  0.1498, -0.0567, -0.0949],\n",
       "          ...,\n",
       "          [-0.0917,  0.0352, -0.0124,  ...,  0.0404, -0.1025,  0.0258],\n",
       "          [ 0.0410,  0.1028,  0.0094,  ..., -0.1083,  0.1016, -0.0865],\n",
       "          [ 0.0476, -0.1394,  0.1405,  ...,  0.0642,  0.0612, -0.1285]],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0036, -0.0587, -0.0442,  ..., -0.0138, -0.0261, -0.0357],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.2108, -0.1249,  0.2488,  ..., -0.0085,  0.0490,  0.2338],\n",
       "          [ 0.0551, -0.0583,  0.0397,  ..., -0.0078, -0.1317,  0.1371],\n",
       "          [ 0.1780,  0.4163, -0.0854,  ..., -0.0605,  0.0067,  0.2529],\n",
       "          ...,\n",
       "          [ 0.1732,  0.2368, -0.0458,  ...,  0.0019,  0.2159,  0.0547],\n",
       "          [-0.0956,  0.1805,  0.0765,  ..., -0.0509, -0.0291, -0.2487],\n",
       "          [ 0.0894, -0.0198, -0.1136,  ...,  0.0471,  0.1820,  0.0467]],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0011,  0.1979,  0.0535,  ...,  0.0070, -0.1411,  0.0127],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.2061, 1.2402, 1.1807,  ..., 1.1992, 1.2725, 1.0479],\n",
       "         requires_grad=True)),\n",
       " ('layers.31.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0309, -0.0011, -0.0916,  ..., -0.0663, -0.1016, -0.0908],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.self_attn.k_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0610, -0.0451,  0.1060,  ..., -0.0045,  0.0629,  0.0320],\n",
       "          [ 0.0581,  0.0090,  0.0688,  ...,  0.0133, -0.0659, -0.0453],\n",
       "          [-0.0830,  0.0250, -0.0495,  ...,  0.1100,  0.1473, -0.0746],\n",
       "          ...,\n",
       "          [-0.0801, -0.0298,  0.1118,  ..., -0.0787,  0.0659, -0.0304],\n",
       "          [-0.0071, -0.0019,  0.1458,  ...,  0.0620, -0.0609,  0.0068],\n",
       "          [ 0.0188,  0.0524, -0.1377,  ..., -0.1075,  0.1090,  0.1230]],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.self_attn.k_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0316, -0.0252,  0.0576,  ...,  0.0337, -0.1064,  0.0455],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.self_attn.v_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0228,  0.1142, -0.0090,  ...,  0.0679, -0.0930,  0.0531],\n",
       "          [-0.1482,  0.1616,  0.0205,  ...,  0.1394, -0.0128, -0.0377],\n",
       "          [-0.0665, -0.1737,  0.1567,  ...,  0.0522,  0.1008, -0.1288],\n",
       "          ...,\n",
       "          [ 0.1869, -0.0008,  0.0134,  ..., -0.0659,  0.1516,  0.0940],\n",
       "          [-0.0521, -0.0303, -0.1666,  ...,  0.0866, -0.0514, -0.0441],\n",
       "          [ 0.0416, -0.0211,  0.1401,  ..., -0.1459,  0.0310,  0.0162]],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.self_attn.v_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0276,  0.0199,  0.0064,  ..., -0.0132, -0.0022,  0.0396],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.self_attn.q_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1394, -0.1132,  0.0590,  ..., -0.0521, -0.1017, -0.1169],\n",
       "          [ 0.0390,  0.0511,  0.0609,  ..., -0.1354,  0.0784,  0.0695],\n",
       "          [ 0.0053,  0.0134, -0.0270,  ..., -0.0098, -0.0138, -0.0555],\n",
       "          ...,\n",
       "          [ 0.1302, -0.0844,  0.0656,  ...,  0.0075,  0.0995,  0.0273],\n",
       "          [ 0.0036, -0.0399,  0.0404,  ...,  0.0295,  0.0470,  0.0936],\n",
       "          [ 0.2103,  0.0097, -0.0331,  ..., -0.0596,  0.0621,  0.0209]],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.self_attn.q_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0064,  0.0604, -0.0404,  ...,  0.0138,  0.0124, -0.0477],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.self_attn.out_proj.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1560,  0.0572,  0.1282,  ..., -0.0339,  0.0875, -0.0750],\n",
       "          [ 0.2200, -0.0318, -0.0619,  ...,  0.0754, -0.1444,  0.0775],\n",
       "          [-0.2781,  0.0967, -0.0303,  ...,  0.0005,  0.1821, -0.1223],\n",
       "          ...,\n",
       "          [-0.0978, -0.2307, -0.1149,  ...,  0.0382, -0.0168, -0.0538],\n",
       "          [ 0.0169,  0.1229, -0.2114,  ...,  0.0367, -0.1127,  0.0725],\n",
       "          [ 0.0431,  0.1941, -0.1495,  ...,  0.0873,  0.1633,  0.0121]],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.self_attn.out_proj.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0704,  0.1678, -0.0396,  ..., -0.0092, -0.1167,  0.0154],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.self_attn_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.8057, 0.7578, 0.8169,  ..., 0.7720, 0.7974, 0.7378],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.self_attn_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0181, -0.0320,  0.0419,  ...,  0.0457,  0.0612, -0.0654],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.fc1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0927,  0.0623,  0.0677,  ...,  0.0003,  0.0930, -0.1919],\n",
       "          [-0.2076, -0.1877,  0.0922,  ...,  0.0100,  0.1102,  0.1086],\n",
       "          [-0.0817, -0.1860,  0.1298,  ..., -0.0883, -0.0630,  0.1697],\n",
       "          ...,\n",
       "          [-0.0519, -0.0696,  0.1295,  ..., -0.0881,  0.0611, -0.0076],\n",
       "          [-0.1251,  0.0482,  0.0823,  ..., -0.0919,  0.1378,  0.0243],\n",
       "          [ 0.0699, -0.1765,  0.0673,  ...,  0.1182,  0.0544,  0.1472]],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.fc1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0149, -0.0295,  0.0027,  ..., -0.0070, -0.0292, -0.0402],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.fc2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.2477, -0.0932,  0.2966,  ...,  0.3252,  0.1233,  0.2964],\n",
       "          [ 0.1342, -0.0402,  0.1313,  ..., -0.0264, -0.4353,  0.0124],\n",
       "          [ 0.0687,  0.1019, -0.0320,  ..., -0.0274,  0.4556, -0.0158],\n",
       "          ...,\n",
       "          [-0.2362,  0.1698,  0.0261,  ..., -0.1599,  0.3154, -0.1088],\n",
       "          [-0.1744,  0.0720, -0.1969,  ..., -0.0580,  0.2332,  0.0972],\n",
       "          [ 0.2322,  0.0957, -0.0220,  ..., -0.0375, -0.0983, -0.0659]],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.fc2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0251,  0.0538,  0.0545,  ...,  0.0009, -0.0087, -0.0869],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.final_layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1.0996, 1.1289, 1.0811,  ..., 1.0859, 1.1523, 0.9307],\n",
       "         requires_grad=True)),\n",
       " ('layers.32.final_layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0055,  0.0157, -0.1366,  ..., -0.0743, -0.1407, -0.2257],\n",
       "         requires_grad=True)),\n",
       " ('contact_head.regression.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00, -8.4375e-02,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00, -1.3016e-01,  0.0000e+00,  2.7553e-01,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  7.8669e-01,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  2.2357e-01, -5.3776e-01,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  2.0937e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00, -1.8993e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  2.2012e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5853e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.2130e-01,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.3205e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00, -3.1001e-01,  0.0000e+00,\n",
       "           -2.1857e-02,  0.0000e+00, -3.4266e-01,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.2701e-02,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00, -6.5575e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           -6.4770e-01, -1.1104e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00, -3.2084e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.5839e-02,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            5.8273e+00,  1.2352e+00,  0.0000e+00,  3.2299e-03, -1.1792e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00, -2.2520e-01,  0.0000e+00,\n",
       "           -4.5646e+00,  0.0000e+00, -3.1002e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  1.3811e+00,  0.0000e+00, -6.4799e-01,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  4.6967e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0584e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0087e-01,\n",
       "            0.0000e+00,  0.0000e+00,  7.5452e-01,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  2.3102e+00,  0.0000e+00,  0.0000e+00,  5.0777e-02,\n",
       "            0.0000e+00,  0.0000e+00,  2.2911e+00,  0.0000e+00,  9.3784e-01,\n",
       "            0.0000e+00,  0.0000e+00,  5.9839e-01, -5.3221e-01,  8.9329e-02,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00, -3.6761e-01,  5.4159e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.8738e+00,\n",
       "            1.1084e+00,  0.0000e+00,  2.9839e-01,  0.0000e+00,  6.6106e-01,\n",
       "           -3.2670e-01,  1.1696e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  8.2694e-01,  0.0000e+00,  1.8255e-02,\n",
       "            0.0000e+00,  7.1025e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9183e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  2.5513e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2022e-01,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.1021e-01,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  1.0690e-01,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00, -2.2160e-02,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  1.0254e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0425e+00,\n",
       "            0.0000e+00,  0.0000e+00, -3.0785e-03,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5599e-01,  0.0000e+00,\n",
       "           -4.3706e-01,  0.0000e+00, -8.5351e-02,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  2.8252e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            2.0869e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            2.6145e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            1.0303e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1310e+00,  0.0000e+00,\n",
       "            2.3840e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            5.3138e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.9836e-01,\n",
       "            1.0887e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0403e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1794e+00,  8.0344e-01,\n",
       "            0.0000e+00,  0.0000e+00,  3.2148e+00,  0.0000e+00,  1.6148e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5023e-01,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           -1.8556e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            2.8534e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.8542e+00,\n",
       "            1.4948e+00,  5.0054e+00,  0.0000e+00,  0.0000e+00,  2.5410e+00,\n",
       "            4.9989e-01,  3.1877e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  5.7436e+00,  1.1589e+00,\n",
       "            4.8430e-01,  0.0000e+00,  0.0000e+00,  1.1424e+00,  5.1037e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.6974e+00,\n",
       "            0.0000e+00,  0.0000e+00, -2.4388e+00,  0.0000e+00,  1.0836e+00,\n",
       "            8.7699e+00,  0.0000e+00,  7.9446e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.3902e+00,\n",
       "            1.6645e+00,  0.0000e+00,  2.6503e+00,  0.0000e+00,  2.9956e+00,\n",
       "            5.2431e+00,  0.0000e+00,  4.2836e+00,  0.0000e+00,  2.0569e+00]],\n",
       "         requires_grad=True)),\n",
       " ('contact_head.regression.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-4.8978], requires_grad=True)),\n",
       " ('embed_positions.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 3.3340e-03,  2.5024e-02,  1.2939e-02,  ..., -9.5596e-03,\n",
       "           -8.5831e-03, -1.5007e-02],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-2.1591e-02, -7.7087e-02,  1.1304e-01,  ...,  1.6101e-01,\n",
       "            1.5002e-01,  3.7378e-01],\n",
       "          ...,\n",
       "          [ 7.4463e-02, -5.9753e-02, -1.6693e-02,  ..., -5.1483e-02,\n",
       "            1.6236e-04,  1.3721e-01],\n",
       "          [-5.2681e-03, -1.2177e-01, -1.4905e-01,  ..., -2.0056e-01,\n",
       "           -7.1228e-02,  9.4421e-02],\n",
       "          [-3.7170e-02, -9.6436e-03,  9.0103e-03,  ..., -1.1032e-02,\n",
       "           -1.8311e-02,  9.2773e-03]], requires_grad=True)),\n",
       " ('emb_layer_norm_before.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.4233, 0.0712, 0.0764,  ..., 0.0758, 0.3706, 0.1709],\n",
       "         requires_grad=True)),\n",
       " ('emb_layer_norm_before.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0984,  0.0180,  0.0182,  ..., -0.0100,  0.0100,  0.0343],\n",
       "         requires_grad=True)),\n",
       " ('emb_layer_norm_after.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.7422, 0.7158, 0.7949,  ..., 0.7031, 0.7568, 0.8335],\n",
       "         requires_grad=True)),\n",
       " ('emb_layer_norm_after.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0012,  0.0410,  0.0330,  ...,  0.0036, -0.0171, -0.0696],\n",
       "         requires_grad=True)),\n",
       " ('lm_head.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-5.1719e+00, -3.9380e-01, -1.2314e+00, -3.8916e-01,  2.9800e-02,\n",
       "           2.1469e-02,  1.3336e-02,  1.5106e-02,  2.5223e-02, -2.1172e-03,\n",
       "           7.9880e-03,  1.4465e-02, -4.7836e-03, -8.2626e-03, -1.5259e-02,\n",
       "          -7.1716e-03, -1.2817e-03,  5.9242e-03, -6.7902e-03, -7.3471e-03,\n",
       "          -2.7954e-02, -1.2299e-02, -5.1605e-02, -6.2347e-02, -2.0004e-02,\n",
       "          -1.6602e+00, -4.2188e+00, -3.7246e+00, -1.4695e+01, -1.4727e+01,\n",
       "          -1.4633e+01, -1.4789e+01, -3.9917e-01], requires_grad=True)),\n",
       " ('lm_head.dense.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0269, -0.0480,  0.0645,  ..., -0.0146, -0.0748,  0.0294],\n",
       "          [-0.0073,  0.0069, -0.1222,  ...,  0.1351,  0.1310,  0.1515],\n",
       "          [-0.0077, -0.0139,  0.0428,  ..., -0.0116, -0.0047,  0.0184],\n",
       "          ...,\n",
       "          [-0.0823, -0.0014,  0.0497,  ...,  0.0228,  0.1249, -0.0616],\n",
       "          [ 0.0644,  0.1801,  0.0768,  ..., -0.0626,  0.0164,  0.0475],\n",
       "          [ 0.0304, -0.1836, -0.1935,  ...,  0.0567, -0.0906, -0.1216]],\n",
       "         requires_grad=True)),\n",
       " ('lm_head.dense.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0313, -0.0087, -0.0396,  ..., -0.0189, -0.0211, -0.0153],\n",
       "         requires_grad=True)),\n",
       " ('lm_head.layer_norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([0.6777, 1.1084, 0.6646,  ..., 0.5825, 0.9126, 0.6309],\n",
       "         requires_grad=True)),\n",
       " ('lm_head.layer_norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.0032, 0.0592, 0.0474,  ..., 0.0142, 0.0754, 0.0690],\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0528db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
